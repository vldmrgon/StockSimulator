[WARN] 2021-07-19 14:31:14,299 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:31:14,301 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 14:31:14,503 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 14:31:15,359 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 14:31:15,360 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 14:31:15,360 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 14:31:15,362 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 14:31:15,413 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 14:31:15,430 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 14:31:15,431 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 14:31:15,526 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 14:31:15,527 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 14:31:15,529 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 14:31:15,530 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 14:31:15,531 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[INFO] 2021-07-19 14:31:16,130 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35951.
[INFO] 2021-07-19 14:31:16,178 org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO] 2021-07-19 14:31:16,226 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 14:31:16,264 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 14:31:16,265 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 14:31:16,271 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 14:31:16,310 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f35db47d-2cf6-4a2c-aec8-894cc4e31380
[INFO] 2021-07-19 14:31:16,383 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 14:31:16,411 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO] 2021-07-19 14:31:16,816 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 14:31:16,928 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 14:31:17,185 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[INFO] 2021-07-19 14:31:17,219 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33109.
[INFO] 2021-07-19 14:31:17,220 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:33109
[INFO] 2021-07-19 14:31:17,222 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 14:31:17,230 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 33109, None)
[INFO] 2021-07-19 14:31:17,234 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:33109 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 33109, None)
[INFO] 2021-07-19 14:31:17,238 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 33109, None)
[INFO] 2021-07-19 14:31:17,239 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 33109, None)
[INFO] 2021-07-19 14:31:17,649 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 14:31:17,660 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 14:31:17,673 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 14:31:17,683 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 14:31:17,683 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 14:31:17,690 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 14:31:17,694 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 14:31:17,707 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 14:31:17,707 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 14:31:17,708 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a8c6732d-b8c4-46aa-997f-7ece6408b378
[WARN] 2021-07-19 14:34:12,318 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:34:12,321 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 14:34:12,543 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 14:34:13,241 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 14:34:13,242 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 14:34:13,243 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 14:34:13,245 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 14:34:13,285 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 14:34:13,296 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 14:34:13,297 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 14:34:13,395 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 14:34:13,396 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 14:34:13,397 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 14:34:13,399 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 14:34:13,400 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[INFO] 2021-07-19 14:34:13,984 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43601.
[INFO] 2021-07-19 14:34:14,054 org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO] 2021-07-19 14:34:14,102 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 14:34:14,132 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 14:34:14,134 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 14:34:14,141 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 14:34:14,179 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-29a9e413-1ce8-4c84-951b-9fdfe9e5ee4a
[INFO] 2021-07-19 14:34:14,235 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 14:34:14,272 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO] 2021-07-19 14:34:14,600 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 14:34:14,720 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 14:34:15,030 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[INFO] 2021-07-19 14:34:15,076 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36663.
[INFO] 2021-07-19 14:34:15,077 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:36663
[INFO] 2021-07-19 14:34:15,080 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 14:34:15,090 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 36663, None)
[INFO] 2021-07-19 14:34:15,096 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:36663 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 36663, None)
[INFO] 2021-07-19 14:34:15,100 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 36663, None)
[INFO] 2021-07-19 14:34:15,102 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 36663, None)
[INFO] 2021-07-19 14:34:15,639 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 14:34:15,652 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 14:34:15,672 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 14:34:15,697 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 14:34:15,698 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 14:34:15,705 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 14:34:15,710 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 14:34:15,719 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 14:34:15,720 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 14:34:15,721 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-cc45a3b2-4162-4ffe-8bbf-22572e63dce4
[WARN] 2021-07-19 14:34:39,957 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:34:39,959 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 14:38:01,313 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:38:01,314 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 14:40:35,647 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:40:35,649 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 14:43:03,342 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:43:03,345 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 14:43:41,349 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:43:41,351 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 14:50:49,684 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:50:49,687 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 14:51:31,885 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:51:31,886 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 14:52:57,137 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 14:52:57,139 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:01:19,687 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:01:19,696 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:02:04,735 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:02:04,736 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:22:50,401 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:22:50,413 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:26:59,068 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:26:59,070 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:27:28,606 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:27:28,609 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:28:03,028 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:28:03,057 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:28:28,217 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:28:28,241 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:28:47,257 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:28:47,258 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:29:20,387 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:29:20,394 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:29:50,958 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:29:50,962 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:30:13,824 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:30:13,833 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:30:34,938 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:30:34,941 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:34:30,978 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:34:30,981 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 16:36:44,912 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:36:44,914 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:36:45,242 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:36:46,014 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:36:46,020 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:36:46,022 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:36:46,023 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:36:46,087 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:36:46,113 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:36:46,114 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:36:46,241 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:36:46,243 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:36:46,245 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:36:46,251 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:36:46,253 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[INFO] 2021-07-19 16:36:47,089 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38623.
[INFO] 2021-07-19 16:36:47,139 org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO] 2021-07-19 16:36:47,209 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:36:47,278 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:36:47,280 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:36:47,287 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:36:47,361 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-4ff0d08e-c14b-4769-8583-af39e7596ea8
[INFO] 2021-07-19 16:36:47,481 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:36:47,513 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO] 2021-07-19 16:36:47,933 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:36:48,027 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:36:48,295 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[INFO] 2021-07-19 16:36:48,338 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34227.
[INFO] 2021-07-19 16:36:48,338 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:34227
[INFO] 2021-07-19 16:36:48,341 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:36:48,349 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 34227, None)
[INFO] 2021-07-19 16:36:48,360 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:34227 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 34227, None)
[INFO] 2021-07-19 16:36:48,363 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 34227, None)
[INFO] 2021-07-19 16:36:48,364 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 34227, None)
[INFO] 2021-07-19 16:36:48,932 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:36:48,933 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[INFO] 2021-07-19 16:36:49,974 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:36:49,985 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:36:50,001 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:36:50,018 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:36:50,019 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:36:50,035 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:36:50,051 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:36:50,059 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:36:50,059 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:36:50,060 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-2ea04e3d-7cb6-4186-a9c1-fb4eb25dac2d
[WARN] 2021-07-19 16:38:40,706 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:38:40,708 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:38:40,997 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:38:41,712 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:38:41,714 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:38:41,715 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:38:41,717 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:38:41,777 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:38:41,813 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:38:41,814 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:38:41,972 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:38:41,974 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:38:41,976 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:38:41,978 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:38:41,979 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[INFO] 2021-07-19 16:38:42,656 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37581.
[INFO] 2021-07-19 16:38:42,756 org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO] 2021-07-19 16:38:42,818 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:38:42,852 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:38:42,853 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:38:42,881 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:38:42,941 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3fa9b6ac-49bd-43bf-9094-ba250974c2f7
[INFO] 2021-07-19 16:38:43,010 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:38:43,043 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO] 2021-07-19 16:38:43,335 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:38:43,421 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:38:43,688 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[INFO] 2021-07-19 16:38:43,736 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34649.
[INFO] 2021-07-19 16:38:43,737 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:34649
[INFO] 2021-07-19 16:38:43,740 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:38:43,751 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 34649, None)
[INFO] 2021-07-19 16:38:43,757 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:34649 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 34649, None)
[INFO] 2021-07-19 16:38:43,761 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 34649, None)
[INFO] 2021-07-19 16:38:43,763 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 34649, None)
[INFO] 2021-07-19 16:38:44,267 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:38:44,268 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[INFO] 2021-07-19 16:38:45,227 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:38:45,239 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:38:45,258 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:38:45,268 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:38:45,277 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:38:45,287 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:38:45,292 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:38:45,305 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:38:45,305 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:38:45,306 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-dfcf74ba-b9d8-4f88-873e-ac8f67a3dc24
[WARN] 2021-07-19 16:41:46,901 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:41:46,905 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:41:47,234 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:41:47,936 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:41:47,938 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:41:47,939 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:41:47,941 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:41:47,984 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:41:48,002 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:41:48,005 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:41:48,105 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:41:48,107 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:41:48,108 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:41:48,109 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:41:48,110 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[INFO] 2021-07-19 16:41:48,671 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37449.
[INFO] 2021-07-19 16:41:48,730 org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO] 2021-07-19 16:41:48,801 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:41:48,837 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:41:48,838 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:41:48,848 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:41:48,901 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ba29a24e-28e0-492c-9264-a914a1caf5ec
[INFO] 2021-07-19 16:41:48,970 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:41:49,010 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO] 2021-07-19 16:41:49,354 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:41:49,484 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:41:49,729 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[INFO] 2021-07-19 16:41:49,761 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43619.
[INFO] 2021-07-19 16:41:49,762 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:43619
[INFO] 2021-07-19 16:41:49,764 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:41:49,772 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 43619, None)
[INFO] 2021-07-19 16:41:49,777 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:43619 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 43619, None)
[INFO] 2021-07-19 16:41:49,780 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 43619, None)
[INFO] 2021-07-19 16:41:49,782 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 43619, None)
[INFO] 2021-07-19 16:41:50,273 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:41:50,274 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[INFO] 2021-07-19 16:41:51,173 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:41:51,193 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:41:51,211 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:41:51,232 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:41:51,232 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:41:51,242 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:41:51,246 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:41:51,258 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:41:51,259 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:41:51,260 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-28a7364f-bf3f-4a77-97ea-4546d8c9ba2f
[WARN] 2021-07-19 16:43:24,108 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:43:24,110 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:43:24,364 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:43:24,938 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:43:24,939 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:43:24,940 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:43:24,942 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:43:24,979 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:43:24,996 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:43:24,997 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:43:25,101 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:43:25,102 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:43:25,104 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:43:25,107 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:43:25,108 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[INFO] 2021-07-19 16:43:25,819 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39813.
[INFO] 2021-07-19 16:43:25,902 org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO] 2021-07-19 16:43:25,962 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:43:25,999 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:43:26,001 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:43:26,012 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:43:26,072 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-eb71bcad-ec23-4f9c-a56a-395f192e331e
[INFO] 2021-07-19 16:43:26,173 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:43:26,214 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO] 2021-07-19 16:43:26,549 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:43:26,644 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:43:26,874 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[INFO] 2021-07-19 16:43:26,907 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41845.
[INFO] 2021-07-19 16:43:26,907 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:41845
[INFO] 2021-07-19 16:43:26,909 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:43:26,917 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 41845, None)
[INFO] 2021-07-19 16:43:26,921 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:41845 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 41845, None)
[INFO] 2021-07-19 16:43:26,924 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 41845, None)
[INFO] 2021-07-19 16:43:26,926 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 41845, None)
[INFO] 2021-07-19 16:43:27,391 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:43:27,392 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[INFO] 2021-07-19 16:43:28,776 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:43:28,786 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:43:28,802 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:43:28,813 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:43:28,814 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:43:28,822 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:43:28,828 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:43:28,837 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:43:28,838 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:43:28,840 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e62cab61-8dc5-4060-8aba-201e19e47e04
[WARN] 2021-07-19 16:46:24,438 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:46:24,442 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:46:24,653 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:46:25,188 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:46:25,189 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:46:25,190 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:46:25,195 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:46:25,239 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:46:25,255 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:46:25,256 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:46:25,345 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:46:25,347 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:46:25,348 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:46:25,349 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:46:25,350 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[INFO] 2021-07-19 16:46:25,975 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39597.
[INFO] 2021-07-19 16:46:26,021 org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO] 2021-07-19 16:46:26,078 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:46:26,102 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:46:26,103 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:46:26,111 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:46:26,163 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-439f294a-8f1d-4e8d-81de-55eb64ff1478
[INFO] 2021-07-19 16:46:26,241 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:46:26,287 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO] 2021-07-19 16:46:26,733 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:46:26,815 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:46:27,049 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[INFO] 2021-07-19 16:46:27,083 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39599.
[INFO] 2021-07-19 16:46:27,084 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:39599
[INFO] 2021-07-19 16:46:27,086 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:46:27,094 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 39599, None)
[INFO] 2021-07-19 16:46:27,099 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:39599 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 39599, None)
[INFO] 2021-07-19 16:46:27,102 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 39599, None)
[INFO] 2021-07-19 16:46:27,103 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 39599, None)
[INFO] 2021-07-19 16:46:27,567 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:46:27,567 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[INFO] 2021-07-19 16:46:28,644 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:46:28,654 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:46:28,669 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:46:28,685 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:46:28,686 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:46:28,694 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:46:28,699 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:46:28,710 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:46:28,710 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:46:28,711 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c10cbaab-d1b3-490f-83ee-248ce0f52feb
[WARN] 2021-07-19 16:46:44,392 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:46:44,394 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:46:44,649 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:46:45,431 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:46:45,432 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:46:45,433 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:46:45,435 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:46:45,478 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:46:45,496 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:46:45,497 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:46:45,601 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:46:45,602 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:46:45,603 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:46:45,604 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:46:45,605 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[INFO] 2021-07-19 16:46:46,272 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 42877.
[INFO] 2021-07-19 16:46:46,324 org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO] 2021-07-19 16:46:46,373 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:46:46,402 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:46:46,403 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:46:46,409 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:46:46,440 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d7894887-ef28-4387-993b-ff4bdee45234
[INFO] 2021-07-19 16:46:46,524 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:46:46,553 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO] 2021-07-19 16:46:46,854 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:46:46,943 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:46:47,203 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[INFO] 2021-07-19 16:46:47,243 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42673.
[INFO] 2021-07-19 16:46:47,243 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:42673
[INFO] 2021-07-19 16:46:47,246 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:46:47,255 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 42673, None)
[INFO] 2021-07-19 16:46:47,259 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:42673 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 42673, None)
[INFO] 2021-07-19 16:46:47,263 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 42673, None)
[INFO] 2021-07-19 16:46:47,264 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 42673, None)
[INFO] 2021-07-19 16:46:47,721 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:46:47,722 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[INFO] 2021-07-19 16:46:48,655 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 68 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 16:46:48,748 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 16:46:51,116 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:46:51,120 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
[INFO] 2021-07-19 16:46:51,124 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 16:46:51,892 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 268.696679 ms
[INFO] 2021-07-19 16:46:51,972 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 174.2 KiB, free 995.8 MiB)
[INFO] 2021-07-19 16:46:52,042 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.8 MiB)
[INFO] 2021-07-19 16:46:52,046 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.29:42673 (size: 27.5 KiB, free: 996.0 MiB)
[INFO] 2021-07-19 16:46:52,050 org.apache.spark.SparkContext - Created broadcast 0 from csv at Simulator.scala:21
[INFO] 2021-07-19 16:46:52,059 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO] 2021-07-19 16:46:52,222 org.apache.spark.SparkContext - Starting job: csv at Simulator.scala:21
[INFO] 2021-07-19 16:46:52,243 org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at Simulator.scala:21) with 1 output partitions
[INFO] 2021-07-19 16:46:52,244 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at Simulator.scala:21)
[INFO] 2021-07-19 16:46:52,245 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 16:46:52,246 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO] 2021-07-19 16:46:52,251 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:21), which has no missing parents
[INFO] 2021-07-19 16:46:52,373 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 995.8 MiB)
[INFO] 2021-07-19 16:46:52,376 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 995.8 MiB)
[INFO] 2021-07-19 16:46:52,377 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.29:42673 (size: 5.4 KiB, free: 996.0 MiB)
[INFO] 2021-07-19 16:46:52,378 org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:46:52,396 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:21) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:46:52,397 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
[INFO] 2021-07-19 16:46:52,461 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4886 bytes) taskResourceAssignments Map()
[INFO] 2021-07-19 16:46:52,484 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO] 2021-07-19 16:46:52,622 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data/aapl.us.txt, range: 0-428286, partition values: [empty row]
[INFO] 2021-07-19 16:46:52,648 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.527286 ms
[INFO] 2021-07-19 16:46:52,695 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1596 bytes result sent to driver
[INFO] 2021-07-19 16:46:52,705 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 259 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:46:52,707 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:46:52,713 org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at Simulator.scala:21) finished in 0.445 s
[INFO] 2021-07-19 16:46:52,718 org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:46:52,719 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
[INFO] 2021-07-19 16:46:52,721 org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at Simulator.scala:21, took 0.499316 s
[INFO] 2021-07-19 16:46:52,754 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.571549 ms
[INFO] 2021-07-19 16:46:52,809 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:46:52,810 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:46:52,810 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 16:46:52,821 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 174.2 KiB, free 995.6 MiB)
[INFO] 2021-07-19 16:46:52,853 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.6 MiB)
[INFO] 2021-07-19 16:46:52,866 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.29:42673 (size: 27.5 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 16:46:52,867 org.apache.spark.SparkContext - Created broadcast 2 from csv at Simulator.scala:21
[INFO] 2021-07-19 16:46:52,871 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO] 2021-07-19 16:46:52,878 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.1.29:42673 in memory (size: 5.4 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 16:46:53,160 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:46:53,160 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:46:53,160 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<Date: string, Open: string, High: string, Low: string, Close: string ... 1 more field>
[INFO] 2021-07-19 16:46:53,217 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO] 2021-07-19 16:46:53,284 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.104773 ms
[INFO] 2021-07-19 16:46:53,288 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 174.1 KiB, free 995.4 MiB)
[INFO] 2021-07-19 16:46:53,300 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.4 MiB)
[INFO] 2021-07-19 16:46:53,301 org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.29:42673 (size: 27.5 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 16:46:53,302 org.apache.spark.SparkContext - Created broadcast 3 from save at Simulator.scala:30
[INFO] 2021-07-19 16:46:53,306 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[INFO] 2021-07-19 16:46:53,352 org.apache.spark.SparkContext - Starting job: save at Simulator.scala:30
[INFO] 2021-07-19 16:46:53,356 org.apache.spark.scheduler.DAGScheduler - Registering RDD 13 (save at Simulator.scala:30) as input to shuffle 0
[INFO] 2021-07-19 16:46:53,358 org.apache.spark.scheduler.DAGScheduler - Got job 1 (save at Simulator.scala:30) with 1 output partitions
[INFO] 2021-07-19 16:46:53,358 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (save at Simulator.scala:30)
[INFO] 2021-07-19 16:46:53,359 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
[INFO] 2021-07-19 16:46:53,360 org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
[INFO] 2021-07-19 16:46:53,364 org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at save at Simulator.scala:30), which has no missing parents
[INFO] 2021-07-19 16:46:53,405 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 16.9 KiB, free 995.4 MiB)
[INFO] 2021-07-19 16:46:53,424 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 995.4 MiB)
[INFO] 2021-07-19 16:46:53,425 org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.29:42673 (size: 8.0 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 16:46:53,426 org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:46:53,429 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at save at Simulator.scala:30) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:46:53,429 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
[INFO] 2021-07-19 16:46:53,434 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4875 bytes) taskResourceAssignments Map()
[INFO] 2021-07-19 16:46:53,435 org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO] 2021-07-19 16:46:53,507 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data/aapl.us.txt, range: 0-428286, partition values: [empty row]
[INFO] 2021-07-19 16:46:53,545 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.786606 ms
[INFO] 2021-07-19 16:46:53,841 org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1818 bytes result sent to driver
[INFO] 2021-07-19 16:46:53,845 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 413 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:46:53,846 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:46:53,849 org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (save at Simulator.scala:30) finished in 0.476 s
[INFO] 2021-07-19 16:46:53,850 org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO] 2021-07-19 16:46:53,850 org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO] 2021-07-19 16:46:53,851 org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO] 2021-07-19 16:46:53,852 org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO] 2021-07-19 16:46:53,854 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (ShuffledRowRDD[14] at save at Simulator.scala:30), which has no missing parents
[INFO] 2021-07-19 16:46:53,886 org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 169.4 KiB, free 995.2 MiB)
[INFO] 2021-07-19 16:46:53,898 org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 60.8 KiB, free 995.2 MiB)
[INFO] 2021-07-19 16:46:53,899 org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.1.29:42673 (size: 60.8 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 16:46:53,900 org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:46:53,901 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (ShuffledRowRDD[14] at save at Simulator.scala:30) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:46:53,902 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0
[INFO] 2021-07-19 16:46:53,902 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.1.29:42673 in memory (size: 27.5 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 16:46:53,909 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.29, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[INFO] 2021-07-19 16:46:53,910 org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO] 2021-07-19 16:46:53,999 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.1.29:42673 in memory (size: 27.5 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 16:46:54,009 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (343.8 KiB) non-empty blocks including 1 (343.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[INFO] 2021-07-19 16:46:54,011 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
[INFO] 2021-07-19 16:46:54,018 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO] 2021-07-19 16:46:54,217 org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202107191646533281443855240775792_0002_m_000000_2: Committed
[INFO] 2021-07-19 16:46:54,230 org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3407 bytes result sent to driver
[INFO] 2021-07-19 16:46:54,233 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 325 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:46:54,233 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:46:54,234 org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (save at Simulator.scala:30) finished in 0.373 s
[INFO] 2021-07-19 16:46:54,235 org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:46:54,235 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
[INFO] 2021-07-19 16:46:54,236 org.apache.spark.scheduler.DAGScheduler - Job 1 finished: save at Simulator.scala:30, took 0.883477 s
[INFO] 2021-07-19 16:46:54,256 org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job e082e09a-727e-4c01-92d2-bfe94e42a829 committed.
[INFO] 2021-07-19 16:46:54,261 org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job e082e09a-727e-4c01-92d2-bfe94e42a829.
[INFO] 2021-07-19 16:46:54,275 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:46:54,288 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:46:54,303 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:46:54,315 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:46:54,316 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:46:54,320 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:46:54,323 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:46:54,328 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:46:54,329 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:46:54,330 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-3b918c37-49e7-4987-bc04-0eeed27baf21
[WARN] 2021-07-19 16:53:57,132 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:53:57,141 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:53:57,471 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:53:58,554 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:53:58,570 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:53:58,575 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:53:58,579 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:53:58,645 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:53:58,665 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:53:58,667 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:53:58,871 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:53:58,872 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:53:58,874 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:53:58,886 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:53:58,888 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[DEBUG] 2021-07-19 16:53:59,776 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 45083
[INFO] 2021-07-19 16:53:59,804 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45083.
[DEBUG] 2021-07-19 16:53:59,806 org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
[INFO] 2021-07-19 16:53:59,858 org.apache.spark.SparkEnv - Registering MapOutputTracker
[DEBUG] 2021-07-19 16:53:59,859 org.apache.spark.MapOutputTrackerMasterEndpoint - init
[INFO] 2021-07-19 16:53:59,928 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:54:00,007 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:54:00,029 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:54:00,058 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:54:00,162 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-acc8e69d-266d-4d0a-9972-3178ef2936e9
[DEBUG] 2021-07-19 16:54:00,164 org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
[DEBUG] 2021-07-19 16:54:00,167 org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
[INFO] 2021-07-19 16:54:00,218 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:54:00,263 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[DEBUG] 2021-07-19 16:54:00,265 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
[DEBUG] 2021-07-19 16:54:00,296 org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[DEBUG] 2021-07-19 16:54:00,632 org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
[INFO] 2021-07-19 16:54:00,679 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:54:00,800 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:54:01,219 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[DEBUG] 2021-07-19 16:54:01,271 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 45123
[INFO] 2021-07-19 16:54:01,271 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45123.
[INFO] 2021-07-19 16:54:01,271 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:45123
[INFO] 2021-07-19 16:54:01,275 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:54:01,287 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 45123, None)
[DEBUG] 2021-07-19 16:54:01,290 org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.1.29
[INFO] 2021-07-19 16:54:01,291 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:45123 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:01,295 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:01,297 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 45123, None)
[DEBUG] 2021-07-19 16:54:01,695 org.apache.spark.SparkContext - Adding shutdown hook
[INFO] 2021-07-19 16:54:01,959 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:54:01,960 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[DEBUG] 2021-07-19 16:54:01,963 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> test
[DEBUG] 2021-07-19 16:54:01,963 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local[*]
[DEBUG] 2021-07-19 16:54:03,239 org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:
  
[INFO] 2021-07-19 16:54:03,318 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 59 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 16:54:03,427 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 3 ms to list leaf files for 1 paths.
[DEBUG] 2021-07-19 16:54:05,754 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#0
[DEBUG] 2021-07-19 16:54:06,014 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#6
[INFO] 2021-07-19 16:54:06,789 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:54:06,793 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
[INFO] 2021-07-19 16:54:06,798 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[DEBUG] 2021-07-19 16:54:07,500 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[DEBUG] 2021-07-19 16:54:07,542 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[INFO] 2021-07-19 16:54:07,874 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 372.825558 ms
[INFO] 2021-07-19 16:54:08,005 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 174.2 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:54:08,007 org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 104 ms
[DEBUG] 2021-07-19 16:54:08,010 org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 105 ms
[INFO] 2021-07-19 16:54:08,112 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:54:08,114 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:08,116 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.29:45123 (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:54:08,118 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 16:54:08,119 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 16:54:08,121 org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 12 ms
[DEBUG] 2021-07-19 16:54:08,121 org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 12 ms
[INFO] 2021-07-19 16:54:08,124 org.apache.spark.SparkContext - Created broadcast 0 from csv at Simulator.scala:21
[INFO] 2021-07-19 16:54:08,142 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:54:08,232 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:54:08,259 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:54:08,316 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
[DEBUG] 2021-07-19 16:54:08,321 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
[DEBUG] 2021-07-19 16:54:08,324 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 16:54:08,344 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 16:54:08,347 org.apache.spark.SparkContext - Starting job: csv at Simulator.scala:21
[DEBUG] 2021-07-19 16:54:08,354 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 16:54:08,371 org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at Simulator.scala:21) with 1 output partitions
[INFO] 2021-07-19 16:54:08,371 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at Simulator.scala:21)
[INFO] 2021-07-19 16:54:08,372 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 16:54:08,375 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 16:54:08,397 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=csv at Simulator.scala:21;jobs=0))
[DEBUG] 2021-07-19 16:54:08,399 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:54:08,402 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:21), which has no missing parents
[DEBUG] 2021-07-19 16:54:08,403 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
[INFO] 2021-07-19 16:54:08,662 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:54:08,663 org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 3 ms
[DEBUG] 2021-07-19 16:54:08,664 org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 4 ms
[INFO] 2021-07-19 16:54:08,668 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:54:08,669 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:08,669 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.29:45123 (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:54:08,670 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 16:54:08,670 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 16:54:08,671 org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 3 ms
[DEBUG] 2021-07-19 16:54:08,671 org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 3 ms
[INFO] 2021-07-19 16:54:08,671 org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:54:08,695 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:21) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:54:08,696 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:54:08,729 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
[DEBUG] 2021-07-19 16:54:08,735 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 3 ms
[DEBUG] 2021-07-19 16:54:08,736 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[DEBUG] 2021-07-19 16:54:08,749 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
[DEBUG] 2021-07-19 16:54:08,750 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[INFO] 2021-07-19 16:54:08,780 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4886 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:54:08,785 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 16:54:08,805 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[DEBUG] 2021-07-19 16:54:08,818 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
[DEBUG] 2021-07-19 16:54:08,850 org.apache.spark.storage.BlockManager - Getting local block broadcast_1
[DEBUG] 2021-07-19 16:54:08,851 org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:54:09,029 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data/aapl.us.txt, range: 0-428286, partition values: [empty row]
[DEBUG] 2021-07-19 16:54:09,043 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[DEBUG] 2021-07-19 16:54:09,054 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[INFO] 2021-07-19 16:54:09,078 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.482527 ms
[DEBUG] 2021-07-19 16:54:09,080 org.apache.spark.storage.BlockManager - Getting local block broadcast_0
[DEBUG] 2021-07-19 16:54:09,080 org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:54:09,141 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1596 bytes result sent to driver
[DEBUG] 2021-07-19 16:54:09,144 org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
[INFO] 2021-07-19 16:54:09,153 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 390 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:54:09,168 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:54:09,181 org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at Simulator.scala:21) finished in 0.735 s
[DEBUG] 2021-07-19 16:54:09,190 org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
[INFO] 2021-07-19 16:54:09,190 org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:54:09,192 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
[INFO] 2021-07-19 16:54:09,199 org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at Simulator.scala:21, took 0.852101 s
[DEBUG] 2021-07-19 16:54:09,229 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[DEBUG] 2021-07-19 16:54:09,240 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[INFO] 2021-07-19 16:54:09,335 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 104.795329 ms
[DEBUG] 2021-07-19 16:54:09,421 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(34)
[DEBUG] 2021-07-19 16:54:09,428 org.apache.spark.ContextCleaner - Cleaning accumulator 34
[DEBUG] 2021-07-19 16:54:09,434 org.apache.spark.ContextCleaner - Cleaned accumulator 34
[DEBUG] 2021-07-19 16:54:09,434 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(38)
[DEBUG] 2021-07-19 16:54:09,435 org.apache.spark.ContextCleaner - Cleaning accumulator 38
[DEBUG] 2021-07-19 16:54:09,435 org.apache.spark.ContextCleaner - Cleaned accumulator 38
[DEBUG] 2021-07-19 16:54:09,435 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(35)
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Cleaning accumulator 35
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Cleaned accumulator 35
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(37)
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Cleaning accumulator 37
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Cleaned accumulator 37
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(24)
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Cleaning accumulator 24
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Cleaned accumulator 24
[DEBUG] 2021-07-19 16:54:09,437 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(27)
[DEBUG] 2021-07-19 16:54:09,438 org.apache.spark.ContextCleaner - Cleaning accumulator 27
[DEBUG] 2021-07-19 16:54:09,438 org.apache.spark.ContextCleaner - Cleaned accumulator 27
[DEBUG] 2021-07-19 16:54:09,438 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(36)
[DEBUG] 2021-07-19 16:54:09,438 org.apache.spark.ContextCleaner - Cleaning accumulator 36
[DEBUG] 2021-07-19 16:54:09,438 org.apache.spark.ContextCleaner - Cleaned accumulator 36
[DEBUG] 2021-07-19 16:54:09,439 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(33)
[DEBUG] 2021-07-19 16:54:09,439 org.apache.spark.ContextCleaner - Cleaning accumulator 33
[DEBUG] 2021-07-19 16:54:09,439 org.apache.spark.ContextCleaner - Cleaned accumulator 33
[DEBUG] 2021-07-19 16:54:09,439 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(25)
[DEBUG] 2021-07-19 16:54:09,440 org.apache.spark.ContextCleaner - Cleaning accumulator 25
[DEBUG] 2021-07-19 16:54:09,441 org.apache.spark.ContextCleaner - Cleaned accumulator 25
[DEBUG] 2021-07-19 16:54:09,441 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(32)
[DEBUG] 2021-07-19 16:54:09,441 org.apache.spark.ContextCleaner - Cleaning accumulator 32
[DEBUG] 2021-07-19 16:54:09,441 org.apache.spark.ContextCleaner - Cleaned accumulator 32
[DEBUG] 2021-07-19 16:54:09,441 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(1)
[DEBUG] 2021-07-19 16:54:09,451 org.apache.spark.ContextCleaner - Cleaning broadcast 1
[DEBUG] 2021-07-19 16:54:09,454 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
[DEBUG] 2021-07-19 16:54:09,489 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
[DEBUG] 2021-07-19 16:54:09,490 org.apache.spark.storage.BlockManager - Removing broadcast 1
[DEBUG] 2021-07-19 16:54:09,501 org.apache.spark.storage.BlockManager - Removing block broadcast_1
[DEBUG] 2021-07-19 16:54:09,502 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 11088 dropped from memory (free 1044169692)
[DEBUG] 2021-07-19 16:54:09,504 org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
[DEBUG] 2021-07-19 16:54:09,505 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 5515 dropped from memory (free 1044175207)
[DEBUG] 2021-07-19 16:54:09,508 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:09,513 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.1.29:45123 in memory (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:54:09,536 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 16:54:09,536 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 16:54:09,544 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
[DEBUG] 2021-07-19 16:54:09,546 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:45083
[DEBUG] 2021-07-19 16:54:09,551 org.apache.spark.ContextCleaner - Cleaned broadcast 1
[DEBUG] 2021-07-19 16:54:09,551 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(17)
[DEBUG] 2021-07-19 16:54:09,552 org.apache.spark.ContextCleaner - Cleaning accumulator 17
[DEBUG] 2021-07-19 16:54:09,552 org.apache.spark.ContextCleaner - Cleaned accumulator 17
[DEBUG] 2021-07-19 16:54:09,552 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(21)
[DEBUG] 2021-07-19 16:54:09,552 org.apache.spark.ContextCleaner - Cleaning accumulator 21
[DEBUG] 2021-07-19 16:54:09,553 org.apache.spark.ContextCleaner - Cleaned accumulator 21
[DEBUG] 2021-07-19 16:54:09,553 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(30)
[DEBUG] 2021-07-19 16:54:09,553 org.apache.spark.ContextCleaner - Cleaning accumulator 30
[DEBUG] 2021-07-19 16:54:09,554 org.apache.spark.ContextCleaner - Cleaned accumulator 30
[DEBUG] 2021-07-19 16:54:09,564 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(31)
[DEBUG] 2021-07-19 16:54:09,564 org.apache.spark.ContextCleaner - Cleaning accumulator 31
[DEBUG] 2021-07-19 16:54:09,574 org.apache.spark.ContextCleaner - Cleaned accumulator 31
[DEBUG] 2021-07-19 16:54:09,574 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(28)
[DEBUG] 2021-07-19 16:54:09,574 org.apache.spark.ContextCleaner - Cleaning accumulator 28
[DEBUG] 2021-07-19 16:54:09,574 org.apache.spark.ContextCleaner - Cleaned accumulator 28
[DEBUG] 2021-07-19 16:54:09,575 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(23)
[DEBUG] 2021-07-19 16:54:09,578 org.apache.spark.ContextCleaner - Cleaning accumulator 23
[DEBUG] 2021-07-19 16:54:09,579 org.apache.spark.ContextCleaner - Cleaned accumulator 23
[DEBUG] 2021-07-19 16:54:09,579 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(26)
[DEBUG] 2021-07-19 16:54:09,579 org.apache.spark.ContextCleaner - Cleaning accumulator 26
[DEBUG] 2021-07-19 16:54:09,581 org.apache.spark.ContextCleaner - Cleaned accumulator 26
[DEBUG] 2021-07-19 16:54:09,582 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(40)
[DEBUG] 2021-07-19 16:54:09,583 org.apache.spark.ContextCleaner - Cleaning accumulator 40
[DEBUG] 2021-07-19 16:54:09,583 org.apache.spark.ContextCleaner - Cleaned accumulator 40
[DEBUG] 2021-07-19 16:54:09,584 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(16)
[DEBUG] 2021-07-19 16:54:09,584 org.apache.spark.ContextCleaner - Cleaning accumulator 16
[DEBUG] 2021-07-19 16:54:09,585 org.apache.spark.ContextCleaner - Cleaned accumulator 16
[DEBUG] 2021-07-19 16:54:09,586 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(19)
[DEBUG] 2021-07-19 16:54:09,586 org.apache.spark.ContextCleaner - Cleaning accumulator 19
[DEBUG] 2021-07-19 16:54:09,587 org.apache.spark.ContextCleaner - Cleaned accumulator 19
[DEBUG] 2021-07-19 16:54:09,587 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(18)
[DEBUG] 2021-07-19 16:54:09,587 org.apache.spark.ContextCleaner - Cleaning accumulator 18
[DEBUG] 2021-07-19 16:54:09,587 org.apache.spark.ContextCleaner - Cleaned accumulator 18
[DEBUG] 2021-07-19 16:54:09,587 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(29)
[DEBUG] 2021-07-19 16:54:09,588 org.apache.spark.ContextCleaner - Cleaning accumulator 29
[DEBUG] 2021-07-19 16:54:09,588 org.apache.spark.ContextCleaner - Cleaned accumulator 29
[DEBUG] 2021-07-19 16:54:09,588 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(22)
[DEBUG] 2021-07-19 16:54:09,588 org.apache.spark.ContextCleaner - Cleaning accumulator 22
[DEBUG] 2021-07-19 16:54:09,589 org.apache.spark.ContextCleaner - Cleaned accumulator 22
[DEBUG] 2021-07-19 16:54:09,589 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(39)
[DEBUG] 2021-07-19 16:54:09,589 org.apache.spark.ContextCleaner - Cleaning accumulator 39
[DEBUG] 2021-07-19 16:54:09,589 org.apache.spark.ContextCleaner - Cleaned accumulator 39
[DEBUG] 2021-07-19 16:54:09,590 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(20)
[DEBUG] 2021-07-19 16:54:09,590 org.apache.spark.ContextCleaner - Cleaning accumulator 20
[DEBUG] 2021-07-19 16:54:09,590 org.apache.spark.ContextCleaner - Cleaned accumulator 20
[INFO] 2021-07-19 16:54:09,655 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:54:09,655 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:54:09,665 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 16:54:09,775 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 174.2 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:54:09,776 org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 98 ms
[DEBUG] 2021-07-19 16:54:09,776 org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 98 ms
[INFO] 2021-07-19 16:54:09,824 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:54:09,825 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:09,826 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.29:45123 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:54:09,826 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 16:54:09,827 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[DEBUG] 2021-07-19 16:54:09,827 org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 3 ms
[DEBUG] 2021-07-19 16:54:09,827 org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 3 ms
[INFO] 2021-07-19 16:54:09,829 org.apache.spark.SparkContext - Created broadcast 2 from csv at Simulator.scala:21
[INFO] 2021-07-19 16:54:09,830 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:54:09,852 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 16:54:09,875 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 16:54:09,940 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inferFromDataset$2
[DEBUG] 2021-07-19 16:54:09,942 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
[INFO] 2021-07-19 16:54:10,284 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:54:10,284 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:54:10,285 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<Date: string, Open: string, High: string, Low: string, Close: string ... 1 more field>
[DEBUG] 2021-07-19 16:54:10,332 org.apache.spark.internal.io.FileCommitProtocol - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job 63e58546-d885-4d99-91e6-5965316ecdf3; output=file:/home/dev/IdeaProjects/StockSimulator/data1/aapl.txt; dynamic=false
[DEBUG] 2021-07-19 16:54:10,337 org.apache.spark.internal.io.FileCommitProtocol - Using (String, String, Boolean) constructor
[INFO] 2021-07-19 16:54:10,368 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[DEBUG] 2021-07-19 16:54:10,425 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       // common sub-expressions
/* 029 */
/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(0));
/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 035 */       null : (inputadapter_row_0.getUTF8String(1));
/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 037 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 038 */       null : (inputadapter_row_0.getUTF8String(2));
/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 041 */       null : (inputadapter_row_0.getUTF8String(3));
/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 044 */       null : (inputadapter_row_0.getUTF8String(4));
/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 046 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 047 */       null : (inputadapter_row_0.getUTF8String(5));
/* 048 */       project_mutableStateArray_0[0].reset();
/* 049 */
/* 050 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 051 */
/* 052 */       if (inputadapter_isNull_0) {
/* 053 */         project_mutableStateArray_0[0].setNullAt(0);
/* 054 */       } else {
/* 055 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 056 */       }
/* 057 */
/* 058 */       if (inputadapter_isNull_1) {
/* 059 */         project_mutableStateArray_0[0].setNullAt(1);
/* 060 */       } else {
/* 061 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 062 */       }
/* 063 */
/* 064 */       if (inputadapter_isNull_2) {
/* 065 */         project_mutableStateArray_0[0].setNullAt(2);
/* 066 */       } else {
/* 067 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);
/* 068 */       }
/* 069 */
/* 070 */       if (inputadapter_isNull_3) {
/* 071 */         project_mutableStateArray_0[0].setNullAt(3);
/* 072 */       } else {
/* 073 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);
/* 074 */       }
/* 075 */
/* 076 */       if (inputadapter_isNull_4) {
/* 077 */         project_mutableStateArray_0[0].setNullAt(4);
/* 078 */       } else {
/* 079 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);
/* 080 */       }
/* 081 */
/* 082 */       if (inputadapter_isNull_5) {
/* 083 */         project_mutableStateArray_0[0].setNullAt(5);
/* 084 */       } else {
/* 085 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);
/* 086 */       }
/* 087 */
/* 088 */       project_mutableStateArray_0[0].write(6, ((UTF8String) references[0] /* literal */));
/* 089 */       append((project_mutableStateArray_0[0].getRow()));
/* 090 */       if (shouldStop()) return;
/* 091 */     }
/* 092 */   }
/* 093 */
/* 094 */ }

[DEBUG] 2021-07-19 16:54:10,434 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       // common sub-expressions
/* 029 */
/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(0));
/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 035 */       null : (inputadapter_row_0.getUTF8String(1));
/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 037 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 038 */       null : (inputadapter_row_0.getUTF8String(2));
/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 041 */       null : (inputadapter_row_0.getUTF8String(3));
/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 044 */       null : (inputadapter_row_0.getUTF8String(4));
/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 046 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 047 */       null : (inputadapter_row_0.getUTF8String(5));
/* 048 */       project_mutableStateArray_0[0].reset();
/* 049 */
/* 050 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 051 */
/* 052 */       if (inputadapter_isNull_0) {
/* 053 */         project_mutableStateArray_0[0].setNullAt(0);
/* 054 */       } else {
/* 055 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 056 */       }
/* 057 */
/* 058 */       if (inputadapter_isNull_1) {
/* 059 */         project_mutableStateArray_0[0].setNullAt(1);
/* 060 */       } else {
/* 061 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 062 */       }
/* 063 */
/* 064 */       if (inputadapter_isNull_2) {
/* 065 */         project_mutableStateArray_0[0].setNullAt(2);
/* 066 */       } else {
/* 067 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);
/* 068 */       }
/* 069 */
/* 070 */       if (inputadapter_isNull_3) {
/* 071 */         project_mutableStateArray_0[0].setNullAt(3);
/* 072 */       } else {
/* 073 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);
/* 074 */       }
/* 075 */
/* 076 */       if (inputadapter_isNull_4) {
/* 077 */         project_mutableStateArray_0[0].setNullAt(4);
/* 078 */       } else {
/* 079 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);
/* 080 */       }
/* 081 */
/* 082 */       if (inputadapter_isNull_5) {
/* 083 */         project_mutableStateArray_0[0].setNullAt(5);
/* 084 */       } else {
/* 085 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);
/* 086 */       }
/* 087 */
/* 088 */       project_mutableStateArray_0[0].write(6, ((UTF8String) references[0] /* literal */));
/* 089 */       append((project_mutableStateArray_0[0].getRow()));
/* 090 */       if (shouldStop()) return;
/* 091 */     }
/* 092 */   }
/* 093 */
/* 094 */ }

[INFO] 2021-07-19 16:54:10,475 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 49.489566 ms
[INFO] 2021-07-19 16:54:10,496 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 174.1 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:54:10,496 org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 18 ms
[DEBUG] 2021-07-19 16:54:10,498 org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 20 ms
[DEBUG] 2021-07-19 16:54:10,545 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(52)
[DEBUG] 2021-07-19 16:54:10,547 org.apache.spark.ContextCleaner - Cleaning accumulator 52
[DEBUG] 2021-07-19 16:54:10,547 org.apache.spark.ContextCleaner - Cleaned accumulator 52
[DEBUG] 2021-07-19 16:54:10,547 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(54)
[DEBUG] 2021-07-19 16:54:10,547 org.apache.spark.ContextCleaner - Cleaning accumulator 54
[DEBUG] 2021-07-19 16:54:10,547 org.apache.spark.ContextCleaner - Cleaned accumulator 54
[DEBUG] 2021-07-19 16:54:10,547 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(70)
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaning accumulator 70
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaned accumulator 70
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(47)
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaning accumulator 47
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaned accumulator 47
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(68)
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaning accumulator 68
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaned accumulator 68
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(76)
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaning accumulator 76
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaned accumulator 76
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(71)
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaning accumulator 71
[DEBUG] 2021-07-19 16:54:10,548 org.apache.spark.ContextCleaner - Cleaned accumulator 71
[DEBUG] 2021-07-19 16:54:10,549 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(50)
[DEBUG] 2021-07-19 16:54:10,551 org.apache.spark.ContextCleaner - Cleaning accumulator 50
[DEBUG] 2021-07-19 16:54:10,552 org.apache.spark.ContextCleaner - Cleaned accumulator 50
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(55)
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaning accumulator 55
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaned accumulator 55
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(53)
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaning accumulator 53
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaned accumulator 53
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(72)
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaning accumulator 72
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaned accumulator 72
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(67)
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaning accumulator 67
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaned accumulator 67
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(77)
[DEBUG] 2021-07-19 16:54:10,555 org.apache.spark.ContextCleaner - Cleaning accumulator 77
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaned accumulator 77
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(75)
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaning accumulator 75
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaned accumulator 75
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(51)
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaning accumulator 51
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaned accumulator 51
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(74)
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaning accumulator 74
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaned accumulator 74
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(46)
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaning accumulator 46
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaned accumulator 46
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(45)
[DEBUG] 2021-07-19 16:54:10,556 org.apache.spark.ContextCleaner - Cleaning accumulator 45
[DEBUG] 2021-07-19 16:54:10,561 org.apache.spark.ContextCleaner - Cleaned accumulator 45
[DEBUG] 2021-07-19 16:54:10,561 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(48)
[DEBUG] 2021-07-19 16:54:10,564 org.apache.spark.ContextCleaner - Cleaning accumulator 48
[DEBUG] 2021-07-19 16:54:10,564 org.apache.spark.ContextCleaner - Cleaned accumulator 48
[DEBUG] 2021-07-19 16:54:10,565 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(69)
[DEBUG] 2021-07-19 16:54:10,565 org.apache.spark.ContextCleaner - Cleaning accumulator 69
[DEBUG] 2021-07-19 16:54:10,566 org.apache.spark.ContextCleaner - Cleaned accumulator 69
[DEBUG] 2021-07-19 16:54:10,566 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(73)
[DEBUG] 2021-07-19 16:54:10,567 org.apache.spark.ContextCleaner - Cleaning accumulator 73
[DEBUG] 2021-07-19 16:54:10,567 org.apache.spark.ContextCleaner - Cleaned accumulator 73
[DEBUG] 2021-07-19 16:54:10,567 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(49)
[DEBUG] 2021-07-19 16:54:10,568 org.apache.spark.ContextCleaner - Cleaning accumulator 49
[DEBUG] 2021-07-19 16:54:10,568 org.apache.spark.ContextCleaner - Cleaned accumulator 49
[INFO] 2021-07-19 16:54:10,571 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:54:10,572 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:10,573 org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.29:45123 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:54:10,575 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
[DEBUG] 2021-07-19 16:54:10,575 org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
[DEBUG] 2021-07-19 16:54:10,576 org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 5 ms
[DEBUG] 2021-07-19 16:54:10,576 org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 5 ms
[INFO] 2021-07-19 16:54:10,577 org.apache.spark.SparkContext - Created broadcast 3 from save at Simulator.scala:30
[INFO] 2021-07-19 16:54:10,582 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:54:10,591 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:54:10,595 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:54:10,621 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$write$15
[DEBUG] 2021-07-19 16:54:10,633 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$write$15) is now cleaned +++
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(14)
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Cleaning accumulator 14
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Cleaned accumulator 14
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(2)
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Cleaning accumulator 2
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Cleaned accumulator 2
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(4)
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Cleaning accumulator 4
[DEBUG] 2021-07-19 16:54:10,657 org.apache.spark.ContextCleaner - Cleaned accumulator 4
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(13)
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaning accumulator 13
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaned accumulator 13
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(10)
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaning accumulator 10
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaned accumulator 10
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(6)
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaning accumulator 6
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaned accumulator 6
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(11)
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaning accumulator 11
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaned accumulator 11
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(15)
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaning accumulator 15
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaned accumulator 15
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(12)
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaning accumulator 12
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Cleaned accumulator 12
[DEBUG] 2021-07-19 16:54:10,658 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(3)
[DEBUG] 2021-07-19 16:54:10,660 org.apache.spark.ContextCleaner - Cleaning accumulator 3
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Cleaned accumulator 3
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(7)
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Cleaning accumulator 7
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Cleaned accumulator 7
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(9)
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Cleaning accumulator 9
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Cleaned accumulator 9
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(1)
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Cleaning accumulator 1
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Cleaned accumulator 1
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(5)
[DEBUG] 2021-07-19 16:54:10,661 org.apache.spark.ContextCleaner - Cleaning accumulator 5
[DEBUG] 2021-07-19 16:54:10,662 org.apache.spark.ContextCleaner - Cleaned accumulator 5
[DEBUG] 2021-07-19 16:54:10,662 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(8)
[DEBUG] 2021-07-19 16:54:10,662 org.apache.spark.ContextCleaner - Cleaning accumulator 8
[DEBUG] 2021-07-19 16:54:10,662 org.apache.spark.ContextCleaner - Cleaned accumulator 8
[DEBUG] 2021-07-19 16:54:10,662 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(0)
[DEBUG] 2021-07-19 16:54:10,662 org.apache.spark.ContextCleaner - Cleaning accumulator 0
[DEBUG] 2021-07-19 16:54:10,662 org.apache.spark.ContextCleaner - Cleaned accumulator 0
[INFO] 2021-07-19 16:54:10,741 org.apache.spark.SparkContext - Starting job: save at Simulator.scala:30
[DEBUG] 2021-07-19 16:54:10,742 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[DEBUG] 2021-07-19 16:54:10,746 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 16:54:10,751 org.apache.spark.scheduler.DAGScheduler - Registering RDD 13 (save at Simulator.scala:30) as input to shuffle 0
[INFO] 2021-07-19 16:54:10,755 org.apache.spark.scheduler.DAGScheduler - Got job 1 (save at Simulator.scala:30) with 1 output partitions
[INFO] 2021-07-19 16:54:10,755 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (save at Simulator.scala:30)
[INFO] 2021-07-19 16:54:10,755 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
[INFO] 2021-07-19 16:54:10,758 org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
[DEBUG] 2021-07-19 16:54:10,763 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 2 (name=save at Simulator.scala:30;jobs=1))
[DEBUG] 2021-07-19 16:54:10,768 org.apache.spark.scheduler.DAGScheduler - missing: List(ShuffleMapStage 1)
[DEBUG] 2021-07-19 16:54:10,769 org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 1 (name=save at Simulator.scala:30;jobs=1))
[DEBUG] 2021-07-19 16:54:10,769 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:54:10,770 org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at save at Simulator.scala:30), which has no missing parents
[DEBUG] 2021-07-19 16:54:10,770 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 1)
[INFO] 2021-07-19 16:54:10,840 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 16.9 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:54:10,840 org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 1 ms
[DEBUG] 2021-07-19 16:54:10,841 org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 2 ms
[DEBUG] 2021-07-19 16:54:10,854 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(2)
[DEBUG] 2021-07-19 16:54:10,854 org.apache.spark.ContextCleaner - Cleaning broadcast 2
[DEBUG] 2021-07-19 16:54:10,854 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
[INFO] 2021-07-19 16:54:10,856 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:54:10,857 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 2
[DEBUG] 2021-07-19 16:54:10,857 org.apache.spark.storage.BlockManager - Removing broadcast 2
[DEBUG] 2021-07-19 16:54:10,857 org.apache.spark.storage.BlockManager - Removing block broadcast_2
[DEBUG] 2021-07-19 16:54:10,857 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[DEBUG] 2021-07-19 16:54:10,857 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 178344 dropped from memory (free 1043915088)
[DEBUG] 2021-07-19 16:54:10,858 org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
[DEBUG] 2021-07-19 16:54:10,858 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 28145 dropped from memory (free 1043943233)
[INFO] 2021-07-19 16:54:10,858 org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.29:45123 (size: 8.1 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:54:10,858 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
[DEBUG] 2021-07-19 16:54:10,858 org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
[DEBUG] 2021-07-19 16:54:10,858 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[DEBUG] 2021-07-19 16:54:10,859 org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 3 ms
[DEBUG] 2021-07-19 16:54:10,859 org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 4 ms
[INFO] 2021-07-19 16:54:10,859 org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:54:10,859 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.1.29:45123 in memory (size: 27.5 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 16:54:10,862 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at save at Simulator.scala:30) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:54:10,862 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:54:10,862 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
[DEBUG] 2021-07-19 16:54:10,863 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
[DEBUG] 2021-07-19 16:54:10,863 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
[DEBUG] 2021-07-19 16:54:10,864 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
[INFO] 2021-07-19 16:54:10,866 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4875 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:54:10,866 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[DEBUG] 2021-07-19 16:54:10,867 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 16:54:10,867 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[INFO] 2021-07-19 16:54:10,867 org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[DEBUG] 2021-07-19 16:54:10,870 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
[DEBUG] 2021-07-19 16:54:10,875 org.apache.spark.storage.BlockManager - Getting local block broadcast_4
[DEBUG] 2021-07-19 16:54:10,875 org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 16:54:10,887 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 2, response is 0
[DEBUG] 2021-07-19 16:54:10,896 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:45083
[DEBUG] 2021-07-19 16:54:10,905 org.apache.spark.ContextCleaner - Cleaned broadcast 2
[DEBUG] 2021-07-19 16:54:10,905 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(43)
[DEBUG] 2021-07-19 16:54:10,905 org.apache.spark.ContextCleaner - Cleaning accumulator 43
[DEBUG] 2021-07-19 16:54:10,905 org.apache.spark.ContextCleaner - Cleaned accumulator 43
[DEBUG] 2021-07-19 16:54:10,905 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(41)
[DEBUG] 2021-07-19 16:54:10,905 org.apache.spark.ContextCleaner - Cleaning accumulator 41
[DEBUG] 2021-07-19 16:54:10,905 org.apache.spark.ContextCleaner - Cleaned accumulator 41
[DEBUG] 2021-07-19 16:54:10,906 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(42)
[DEBUG] 2021-07-19 16:54:10,906 org.apache.spark.ContextCleaner - Cleaning accumulator 42
[DEBUG] 2021-07-19 16:54:10,906 org.apache.spark.ContextCleaner - Cleaned accumulator 42
[DEBUG] 2021-07-19 16:54:10,906 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(44)
[DEBUG] 2021-07-19 16:54:10,906 org.apache.spark.ContextCleaner - Cleaning accumulator 44
[DEBUG] 2021-07-19 16:54:10,906 org.apache.spark.ContextCleaner - Cleaned accumulator 44
[INFO] 2021-07-19 16:54:10,931 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data/aapl.us.txt, range: 0-428286, partition values: [empty row]
[DEBUG] 2021-07-19 16:54:10,946 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */   private void writeFields_0_0(InternalRow i) {
/* 051 */
/* 052 */     boolean isNull_0 = i.isNullAt(0);
/* 053 */     UTF8String value_0 = isNull_0 ?
/* 054 */     null : (i.getUTF8String(0));
/* 055 */     if (isNull_0) {
/* 056 */       mutableStateArray_0[0].setNullAt(0);
/* 057 */     } else {
/* 058 */       mutableStateArray_0[0].write(0, value_0);
/* 059 */     }
/* 060 */
/* 061 */     boolean isNull_1 = i.isNullAt(1);
/* 062 */     UTF8String value_1 = isNull_1 ?
/* 063 */     null : (i.getUTF8String(1));
/* 064 */     if (isNull_1) {
/* 065 */       mutableStateArray_0[0].setNullAt(1);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(1, value_1);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_2 = i.isNullAt(2);
/* 071 */     UTF8String value_2 = isNull_2 ?
/* 072 */     null : (i.getUTF8String(2));
/* 073 */     if (isNull_2) {
/* 074 */       mutableStateArray_0[0].setNullAt(2);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(2, value_2);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_3 = i.isNullAt(3);
/* 080 */     UTF8String value_3 = isNull_3 ?
/* 081 */     null : (i.getUTF8String(3));
/* 082 */     if (isNull_3) {
/* 083 */       mutableStateArray_0[0].setNullAt(3);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(3, value_3);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_4 = i.isNullAt(4);
/* 089 */     UTF8String value_4 = isNull_4 ?
/* 090 */     null : (i.getUTF8String(4));
/* 091 */     if (isNull_4) {
/* 092 */       mutableStateArray_0[0].setNullAt(4);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(4, value_4);
/* 095 */     }
/* 096 */
/* 097 */   }
/* 098 */
/* 099 */ }

[DEBUG] 2021-07-19 16:54:10,982 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */   private void writeFields_0_0(InternalRow i) {
/* 051 */
/* 052 */     boolean isNull_0 = i.isNullAt(0);
/* 053 */     UTF8String value_0 = isNull_0 ?
/* 054 */     null : (i.getUTF8String(0));
/* 055 */     if (isNull_0) {
/* 056 */       mutableStateArray_0[0].setNullAt(0);
/* 057 */     } else {
/* 058 */       mutableStateArray_0[0].write(0, value_0);
/* 059 */     }
/* 060 */
/* 061 */     boolean isNull_1 = i.isNullAt(1);
/* 062 */     UTF8String value_1 = isNull_1 ?
/* 063 */     null : (i.getUTF8String(1));
/* 064 */     if (isNull_1) {
/* 065 */       mutableStateArray_0[0].setNullAt(1);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(1, value_1);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_2 = i.isNullAt(2);
/* 071 */     UTF8String value_2 = isNull_2 ?
/* 072 */     null : (i.getUTF8String(2));
/* 073 */     if (isNull_2) {
/* 074 */       mutableStateArray_0[0].setNullAt(2);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(2, value_2);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_3 = i.isNullAt(3);
/* 080 */     UTF8String value_3 = isNull_3 ?
/* 081 */     null : (i.getUTF8String(3));
/* 082 */     if (isNull_3) {
/* 083 */       mutableStateArray_0[0].setNullAt(3);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(3, value_3);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_4 = i.isNullAt(4);
/* 089 */     UTF8String value_4 = isNull_4 ?
/* 090 */     null : (i.getUTF8String(4));
/* 091 */     if (isNull_4) {
/* 092 */       mutableStateArray_0[0].setNullAt(4);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(4, value_4);
/* 095 */     }
/* 096 */
/* 097 */   }
/* 098 */
/* 099 */ }

[INFO] 2021-07-19 16:54:11,076 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 129.268721 ms
[DEBUG] 2021-07-19 16:54:11,082 org.apache.spark.storage.BlockManager - Getting local block broadcast_3
[DEBUG] 2021-07-19 16:54:11,082 org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 16:54:11,343 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(0)
[DEBUG] 2021-07-19 16:54:11,344 org.apache.spark.ContextCleaner - Cleaning broadcast 0
[DEBUG] 2021-07-19 16:54:11,344 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 0
[DEBUG] 2021-07-19 16:54:11,368 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 0
[DEBUG] 2021-07-19 16:54:11,369 org.apache.spark.storage.BlockManager - Removing broadcast 0
[DEBUG] 2021-07-19 16:54:11,369 org.apache.spark.storage.BlockManager - Removing block broadcast_0_piece0
[DEBUG] 2021-07-19 16:54:11,369 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 of size 28145 dropped from memory (free 1043971378)
[DEBUG] 2021-07-19 16:54:11,370 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:11,371 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.1.29:45123 in memory (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:54:11,373 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 16:54:11,373 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 16:54:11,373 org.apache.spark.storage.BlockManager - Removing block broadcast_0
[DEBUG] 2021-07-19 16:54:11,373 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 of size 178344 dropped from memory (free 1044149722)
[DEBUG] 2021-07-19 16:54:11,393 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 0, response is 0
[DEBUG] 2021-07-19 16:54:11,401 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:45083
[DEBUG] 2021-07-19 16:54:11,413 org.apache.spark.ContextCleaner - Cleaned broadcast 0
[DEBUG] 2021-07-19 16:54:11,704 org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 1 with length 1
[DEBUG] 2021-07-19 16:54:11,708 org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 1: [329444]
[INFO] 2021-07-19 16:54:11,729 org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1861 bytes result sent to driver
[DEBUG] 2021-07-19 16:54:11,730 org.apache.spark.executor.ExecutorMetricsPoller - removing (1, 0) from stageTCMP
[INFO] 2021-07-19 16:54:11,736 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 872 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:54:11,736 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[DEBUG] 2021-07-19 16:54:11,737 org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
[INFO] 2021-07-19 16:54:11,739 org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (save at Simulator.scala:30) finished in 0.951 s
[INFO] 2021-07-19 16:54:11,740 org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO] 2021-07-19 16:54:11,741 org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO] 2021-07-19 16:54:11,741 org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO] 2021-07-19 16:54:11,742 org.apache.spark.scheduler.DAGScheduler - failed: Set()
[DEBUG] 2021-07-19 16:54:11,742 org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 1
[DEBUG] 2021-07-19 16:54:11,746 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 2 (name=save at Simulator.scala:30;jobs=1))
[DEBUG] 2021-07-19 16:54:11,746 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:54:11,746 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (ShuffledRowRDD[14] at save at Simulator.scala:30), which has no missing parents
[DEBUG] 2021-07-19 16:54:11,746 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 2)
[INFO] 2021-07-19 16:54:11,812 org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 169.4 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:54:11,824 org.apache.spark.storage.BlockManager - Put block broadcast_5 locally took 15 ms
[DEBUG] 2021-07-19 16:54:11,825 org.apache.spark.storage.BlockManager - Putting block broadcast_5 without replication took 15 ms
[INFO] 2021-07-19 16:54:11,829 org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 60.8 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:54:11,835 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_5_piece0 for BlockManagerId(driver, 192.168.1.29, 45123, None)
[INFO] 2021-07-19 16:54:11,835 org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.1.29:45123 (size: 60.8 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:54:11,838 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
[DEBUG] 2021-07-19 16:54:11,838 org.apache.spark.storage.BlockManager - Told master about block broadcast_5_piece0
[DEBUG] 2021-07-19 16:54:11,838 org.apache.spark.storage.BlockManager - Put block broadcast_5_piece0 locally took 10 ms
[DEBUG] 2021-07-19 16:54:11,838 org.apache.spark.storage.BlockManager - Putting block broadcast_5_piece0 without replication took 10 ms
[INFO] 2021-07-19 16:54:11,839 org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:54:11,846 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (ShuffledRowRDD[14] at save at Simulator.scala:30) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:54:11,846 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:54:11,850 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 2.0: 1
[DEBUG] 2021-07-19 16:54:11,852 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
[DEBUG] 2021-07-19 16:54:11,853 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 2.0: NODE_LOCAL, ANY
[DEBUG] 2021-07-19 16:54:11,854 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
[INFO] 2021-07-19 16:54:11,858 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.29, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:54:11,858 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
[INFO] 2021-07-19 16:54:11,860 org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[DEBUG] 2021-07-19 16:54:11,865 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
[DEBUG] 2021-07-19 16:54:11,871 org.apache.spark.storage.BlockManager - Getting local block broadcast_5
[DEBUG] 2021-07-19 16:54:11,871 org.apache.spark.storage.BlockManager - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 16:54:12,011 org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 0
[DEBUG] 2021-07-19 16:54:12,022 org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 0, mappers 0-1, partitions 0-1
[DEBUG] 2021-07-19 16:54:12,051 org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
[INFO] 2021-07-19 16:54:12,060 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (343.8 KiB) non-empty blocks including 1 (343.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
[INFO] 2021-07-19 16:54:12,062 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[DEBUG] 2021-07-19 16:54:12,062 org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_0_1_0,0)
[DEBUG] 2021-07-19 16:54:12,063 org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_1_0
[DEBUG] 2021-07-19 16:54:12,066 org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 16 ms
[INFO] 2021-07-19 16:54:12,071 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[DEBUG] 2021-07-19 16:54:12,348 org.apache.spark.scheduler.OutputCommitCoordinator - Commit allowed for stage=2.0, partition=0, task attempt 0
[INFO] 2021-07-19 16:54:12,353 org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202107191654104444431371696892767_0002_m_000000_2: Committed
[INFO] 2021-07-19 16:54:12,396 org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3407 bytes result sent to driver
[DEBUG] 2021-07-19 16:54:12,397 org.apache.spark.executor.ExecutorMetricsPoller - removing (2, 0) from stageTCMP
[INFO] 2021-07-19 16:54:12,400 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 545 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:54:12,400 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:54:12,401 org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (save at Simulator.scala:30) finished in 0.643 s
[DEBUG] 2021-07-19 16:54:12,402 org.apache.spark.scheduler.DAGScheduler - After removal of stage 2, remaining stages = 1
[DEBUG] 2021-07-19 16:54:12,402 org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
[INFO] 2021-07-19 16:54:12,402 org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:54:12,402 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
[DEBUG] 2021-07-19 16:54:12,403 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@4e3bbb)
[INFO] 2021-07-19 16:54:12,404 org.apache.spark.scheduler.DAGScheduler - Job 1 finished: save at Simulator.scala:30, took 1.661353 s
[DEBUG] 2021-07-19 16:54:12,436 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Committing files staged for absolute locations Map()
[DEBUG] 2021-07-19 16:54:12,438 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Create absolute parent directories: Set()
[INFO] 2021-07-19 16:54:12,440 org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 8366d6c0-837f-41eb-b75c-027d6176684a committed.
[INFO] 2021-07-19 16:54:12,446 org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 8366d6c0-837f-41eb-b75c-027d6176684a.
[INFO] 2021-07-19 16:54:12,480 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:54:12,495 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:54:12,509 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:54:12,573 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:54:12,574 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:54:12,578 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:54:12,583 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:54:12,673 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:54:12,674 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:54:12,675 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-3e77abfa-b344-421a-9b49-6a315641dbb3
[WARN] 2021-07-19 16:55:21,749 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:55:21,750 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:55:22,115 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:55:22,975 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:55:22,976 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:55:22,978 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:55:22,979 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:55:23,032 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:55:23,054 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:55:23,056 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:55:23,163 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:55:23,164 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:55:23,166 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:55:23,169 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:55:23,170 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[DEBUG] 2021-07-19 16:55:23,968 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 37295
[INFO] 2021-07-19 16:55:24,014 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37295.
[DEBUG] 2021-07-19 16:55:24,017 org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
[INFO] 2021-07-19 16:55:24,095 org.apache.spark.SparkEnv - Registering MapOutputTracker
[DEBUG] 2021-07-19 16:55:24,096 org.apache.spark.MapOutputTrackerMasterEndpoint - init
[INFO] 2021-07-19 16:55:24,157 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:55:24,215 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:55:24,217 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:55:24,228 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:55:24,298 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a03e3491-0f1a-431b-9536-b50931b39ffa
[DEBUG] 2021-07-19 16:55:24,303 org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
[DEBUG] 2021-07-19 16:55:24,305 org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
[INFO] 2021-07-19 16:55:24,374 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:55:24,402 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[DEBUG] 2021-07-19 16:55:24,403 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
[DEBUG] 2021-07-19 16:55:24,422 org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[DEBUG] 2021-07-19 16:55:24,733 org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
[INFO] 2021-07-19 16:55:24,787 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:55:24,995 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:55:25,476 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[DEBUG] 2021-07-19 16:55:25,532 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 34509
[INFO] 2021-07-19 16:55:25,533 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34509.
[INFO] 2021-07-19 16:55:25,533 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:34509
[INFO] 2021-07-19 16:55:25,536 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:55:25,545 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 34509, None)
[DEBUG] 2021-07-19 16:55:25,548 org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.1.29
[INFO] 2021-07-19 16:55:25,550 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:34509 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:25,553 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:25,555 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 34509, None)
[DEBUG] 2021-07-19 16:55:25,867 org.apache.spark.SparkContext - Adding shutdown hook
[INFO] 2021-07-19 16:55:26,094 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:55:26,095 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[DEBUG] 2021-07-19 16:55:26,097 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> test
[DEBUG] 2021-07-19 16:55:26,098 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local[*]
[DEBUG] 2021-07-19 16:55:27,005 org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:
  
[INFO] 2021-07-19 16:55:27,080 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 60 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 16:55:27,238 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 25 ms to list leaf files for 1 paths.
[DEBUG] 2021-07-19 16:55:30,009 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#0
[DEBUG] 2021-07-19 16:55:30,393 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#6
[INFO] 2021-07-19 16:55:31,093 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:55:31,098 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
[INFO] 2021-07-19 16:55:31,112 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[DEBUG] 2021-07-19 16:55:31,992 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[DEBUG] 2021-07-19 16:55:32,082 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[INFO] 2021-07-19 16:55:32,599 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 604.228097 ms
[INFO] 2021-07-19 16:55:32,733 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 174.2 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:55:32,735 org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 97 ms
[DEBUG] 2021-07-19 16:55:32,737 org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 99 ms
[INFO] 2021-07-19 16:55:32,838 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:55:32,840 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:32,843 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.29:34509 (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:55:32,853 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 16:55:32,853 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 16:55:32,854 org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 20 ms
[DEBUG] 2021-07-19 16:55:32,854 org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 20 ms
[INFO] 2021-07-19 16:55:32,857 org.apache.spark.SparkContext - Created broadcast 0 from csv at Simulator.scala:21
[INFO] 2021-07-19 16:55:32,871 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:55:32,953 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:55:32,986 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:55:33,072 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
[DEBUG] 2021-07-19 16:55:33,076 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
[DEBUG] 2021-07-19 16:55:33,082 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 16:55:33,102 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 16:55:33,104 org.apache.spark.SparkContext - Starting job: csv at Simulator.scala:21
[DEBUG] 2021-07-19 16:55:33,111 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 16:55:33,128 org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at Simulator.scala:21) with 1 output partitions
[INFO] 2021-07-19 16:55:33,129 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at Simulator.scala:21)
[INFO] 2021-07-19 16:55:33,130 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 16:55:33,134 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 16:55:33,149 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=csv at Simulator.scala:21;jobs=0))
[DEBUG] 2021-07-19 16:55:33,153 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:55:33,157 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:21), which has no missing parents
[DEBUG] 2021-07-19 16:55:33,158 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
[INFO] 2021-07-19 16:55:33,287 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:55:33,288 org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 3 ms
[DEBUG] 2021-07-19 16:55:33,288 org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 3 ms
[INFO] 2021-07-19 16:55:33,291 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:55:33,292 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:33,293 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.29:34509 (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:55:33,294 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 16:55:33,294 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 16:55:33,295 org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 5 ms
[DEBUG] 2021-07-19 16:55:33,295 org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 5 ms
[INFO] 2021-07-19 16:55:33,295 org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:55:33,319 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:21) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:55:33,321 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:55:33,363 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
[DEBUG] 2021-07-19 16:55:33,371 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 6 ms
[DEBUG] 2021-07-19 16:55:33,379 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[DEBUG] 2021-07-19 16:55:33,404 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
[DEBUG] 2021-07-19 16:55:33,406 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[INFO] 2021-07-19 16:55:33,439 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4886 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:55:33,459 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 16:55:33,476 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[DEBUG] 2021-07-19 16:55:33,490 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
[DEBUG] 2021-07-19 16:55:33,539 org.apache.spark.storage.BlockManager - Getting local block broadcast_1
[DEBUG] 2021-07-19 16:55:33,541 org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:55:33,749 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data/aapl.us.txt, range: 0-428286, partition values: [empty row]
[DEBUG] 2021-07-19 16:55:33,766 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[DEBUG] 2021-07-19 16:55:33,785 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[INFO] 2021-07-19 16:55:33,807 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.192482 ms
[DEBUG] 2021-07-19 16:55:33,809 org.apache.spark.storage.BlockManager - Getting local block broadcast_0
[DEBUG] 2021-07-19 16:55:33,809 org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:55:33,870 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1596 bytes result sent to driver
[DEBUG] 2021-07-19 16:55:33,872 org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
[INFO] 2021-07-19 16:55:33,880 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 463 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:55:33,882 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:55:33,913 org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at Simulator.scala:21) finished in 0.720 s
[DEBUG] 2021-07-19 16:55:33,919 org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
[INFO] 2021-07-19 16:55:33,920 org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:55:33,921 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
[INFO] 2021-07-19 16:55:33,926 org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at Simulator.scala:21, took 0.820898 s
[DEBUG] 2021-07-19 16:55:33,958 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[DEBUG] 2021-07-19 16:55:33,993 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[INFO] 2021-07-19 16:55:34,036 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 77.871088 ms
[INFO] 2021-07-19 16:55:34,147 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:55:34,148 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:55:34,148 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 16:55:34,164 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 174.2 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:55:34,165 org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 10 ms
[DEBUG] 2021-07-19 16:55:34,169 org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 14 ms
[INFO] 2021-07-19 16:55:34,207 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:55:34,207 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:34,209 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.29:34509 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:55:34,210 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 16:55:34,210 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[DEBUG] 2021-07-19 16:55:34,210 org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 4 ms
[DEBUG] 2021-07-19 16:55:34,211 org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 4 ms
[INFO] 2021-07-19 16:55:34,212 org.apache.spark.SparkContext - Created broadcast 2 from csv at Simulator.scala:21
[INFO] 2021-07-19 16:55:34,213 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:55:34,226 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 16:55:34,239 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 16:55:34,301 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inferFromDataset$2
[DEBUG] 2021-07-19 16:55:34,304 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
[INFO] 2021-07-19 16:55:34,553 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:55:34,553 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:55:34,554 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<Date: string, Open: string, High: string, Low: string, Close: string ... 1 more field>
[DEBUG] 2021-07-19 16:55:34,606 org.apache.spark.internal.io.FileCommitProtocol - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job 4f441212-b8c4-4edf-b2fe-f60ce24a157f; output=file:/home/dev/IdeaProjects/StockSimulator/data1/aapl.txt; dynamic=false
[DEBUG] 2021-07-19 16:55:34,614 org.apache.spark.internal.io.FileCommitProtocol - Using (String, String, Boolean) constructor
[INFO] 2021-07-19 16:55:34,642 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[DEBUG] 2021-07-19 16:55:34,690 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       // common sub-expressions
/* 029 */
/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(0));
/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 035 */       null : (inputadapter_row_0.getUTF8String(1));
/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 037 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 038 */       null : (inputadapter_row_0.getUTF8String(2));
/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 041 */       null : (inputadapter_row_0.getUTF8String(3));
/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 044 */       null : (inputadapter_row_0.getUTF8String(4));
/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 046 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 047 */       null : (inputadapter_row_0.getUTF8String(5));
/* 048 */       project_mutableStateArray_0[0].reset();
/* 049 */
/* 050 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 051 */
/* 052 */       if (inputadapter_isNull_0) {
/* 053 */         project_mutableStateArray_0[0].setNullAt(0);
/* 054 */       } else {
/* 055 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 056 */       }
/* 057 */
/* 058 */       if (inputadapter_isNull_1) {
/* 059 */         project_mutableStateArray_0[0].setNullAt(1);
/* 060 */       } else {
/* 061 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 062 */       }
/* 063 */
/* 064 */       if (inputadapter_isNull_2) {
/* 065 */         project_mutableStateArray_0[0].setNullAt(2);
/* 066 */       } else {
/* 067 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);
/* 068 */       }
/* 069 */
/* 070 */       if (inputadapter_isNull_3) {
/* 071 */         project_mutableStateArray_0[0].setNullAt(3);
/* 072 */       } else {
/* 073 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);
/* 074 */       }
/* 075 */
/* 076 */       if (inputadapter_isNull_4) {
/* 077 */         project_mutableStateArray_0[0].setNullAt(4);
/* 078 */       } else {
/* 079 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);
/* 080 */       }
/* 081 */
/* 082 */       if (inputadapter_isNull_5) {
/* 083 */         project_mutableStateArray_0[0].setNullAt(5);
/* 084 */       } else {
/* 085 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);
/* 086 */       }
/* 087 */
/* 088 */       project_mutableStateArray_0[0].write(6, ((UTF8String) references[0] /* literal */));
/* 089 */       append((project_mutableStateArray_0[0].getRow()));
/* 090 */       if (shouldStop()) return;
/* 091 */     }
/* 092 */   }
/* 093 */
/* 094 */ }

[DEBUG] 2021-07-19 16:55:34,696 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       // common sub-expressions
/* 029 */
/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(0));
/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 035 */       null : (inputadapter_row_0.getUTF8String(1));
/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 037 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 038 */       null : (inputadapter_row_0.getUTF8String(2));
/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 041 */       null : (inputadapter_row_0.getUTF8String(3));
/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 044 */       null : (inputadapter_row_0.getUTF8String(4));
/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 046 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 047 */       null : (inputadapter_row_0.getUTF8String(5));
/* 048 */       project_mutableStateArray_0[0].reset();
/* 049 */
/* 050 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 051 */
/* 052 */       if (inputadapter_isNull_0) {
/* 053 */         project_mutableStateArray_0[0].setNullAt(0);
/* 054 */       } else {
/* 055 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 056 */       }
/* 057 */
/* 058 */       if (inputadapter_isNull_1) {
/* 059 */         project_mutableStateArray_0[0].setNullAt(1);
/* 060 */       } else {
/* 061 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 062 */       }
/* 063 */
/* 064 */       if (inputadapter_isNull_2) {
/* 065 */         project_mutableStateArray_0[0].setNullAt(2);
/* 066 */       } else {
/* 067 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);
/* 068 */       }
/* 069 */
/* 070 */       if (inputadapter_isNull_3) {
/* 071 */         project_mutableStateArray_0[0].setNullAt(3);
/* 072 */       } else {
/* 073 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);
/* 074 */       }
/* 075 */
/* 076 */       if (inputadapter_isNull_4) {
/* 077 */         project_mutableStateArray_0[0].setNullAt(4);
/* 078 */       } else {
/* 079 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);
/* 080 */       }
/* 081 */
/* 082 */       if (inputadapter_isNull_5) {
/* 083 */         project_mutableStateArray_0[0].setNullAt(5);
/* 084 */       } else {
/* 085 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);
/* 086 */       }
/* 087 */
/* 088 */       project_mutableStateArray_0[0].write(6, ((UTF8String) references[0] /* literal */));
/* 089 */       append((project_mutableStateArray_0[0].getRow()));
/* 090 */       if (shouldStop()) return;
/* 091 */     }
/* 092 */   }
/* 093 */
/* 094 */ }

[INFO] 2021-07-19 16:55:34,718 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.547376 ms
[INFO] 2021-07-19 16:55:34,723 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 174.1 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:55:34,724 org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 4 ms
[DEBUG] 2021-07-19 16:55:34,725 org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 5 ms
[DEBUG] 2021-07-19 16:55:34,745 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(28)
[DEBUG] 2021-07-19 16:55:34,747 org.apache.spark.ContextCleaner - Cleaning accumulator 28
[DEBUG] 2021-07-19 16:55:34,748 org.apache.spark.ContextCleaner - Cleaned accumulator 28
[DEBUG] 2021-07-19 16:55:34,748 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(16)
[DEBUG] 2021-07-19 16:55:34,748 org.apache.spark.ContextCleaner - Cleaning accumulator 16
[DEBUG] 2021-07-19 16:55:34,748 org.apache.spark.ContextCleaner - Cleaned accumulator 16
[DEBUG] 2021-07-19 16:55:34,748 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(33)
[DEBUG] 2021-07-19 16:55:34,749 org.apache.spark.ContextCleaner - Cleaning accumulator 33
[DEBUG] 2021-07-19 16:55:34,749 org.apache.spark.ContextCleaner - Cleaned accumulator 33
[DEBUG] 2021-07-19 16:55:34,749 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(30)
[DEBUG] 2021-07-19 16:55:34,749 org.apache.spark.ContextCleaner - Cleaning accumulator 30
[DEBUG] 2021-07-19 16:55:34,749 org.apache.spark.ContextCleaner - Cleaned accumulator 30
[DEBUG] 2021-07-19 16:55:34,749 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(38)
[DEBUG] 2021-07-19 16:55:34,749 org.apache.spark.ContextCleaner - Cleaning accumulator 38
[DEBUG] 2021-07-19 16:55:34,749 org.apache.spark.ContextCleaner - Cleaned accumulator 38
[DEBUG] 2021-07-19 16:55:34,750 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(18)
[DEBUG] 2021-07-19 16:55:34,750 org.apache.spark.ContextCleaner - Cleaning accumulator 18
[DEBUG] 2021-07-19 16:55:34,750 org.apache.spark.ContextCleaner - Cleaned accumulator 18
[DEBUG] 2021-07-19 16:55:34,750 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(31)
[DEBUG] 2021-07-19 16:55:34,750 org.apache.spark.ContextCleaner - Cleaning accumulator 31
[DEBUG] 2021-07-19 16:55:34,750 org.apache.spark.ContextCleaner - Cleaned accumulator 31
[DEBUG] 2021-07-19 16:55:34,750 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(40)
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Cleaning accumulator 40
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Cleaned accumulator 40
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(27)
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Cleaning accumulator 27
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Cleaned accumulator 27
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(32)
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Cleaning accumulator 32
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Cleaned accumulator 32
[DEBUG] 2021-07-19 16:55:34,751 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(26)
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Cleaning accumulator 26
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Cleaned accumulator 26
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(35)
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Cleaning accumulator 35
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Cleaned accumulator 35
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(22)
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Cleaning accumulator 22
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Cleaned accumulator 22
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(39)
[DEBUG] 2021-07-19 16:55:34,752 org.apache.spark.ContextCleaner - Cleaning accumulator 39
[DEBUG] 2021-07-19 16:55:34,753 org.apache.spark.ContextCleaner - Cleaned accumulator 39
[DEBUG] 2021-07-19 16:55:34,753 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(24)
[DEBUG] 2021-07-19 16:55:34,753 org.apache.spark.ContextCleaner - Cleaning accumulator 24
[DEBUG] 2021-07-19 16:55:34,753 org.apache.spark.ContextCleaner - Cleaned accumulator 24
[DEBUG] 2021-07-19 16:55:34,753 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(34)
[DEBUG] 2021-07-19 16:55:34,753 org.apache.spark.ContextCleaner - Cleaning accumulator 34
[DEBUG] 2021-07-19 16:55:34,753 org.apache.spark.ContextCleaner - Cleaned accumulator 34
[DEBUG] 2021-07-19 16:55:34,753 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(1)
[DEBUG] 2021-07-19 16:55:34,754 org.apache.spark.ContextCleaner - Cleaning broadcast 1
[DEBUG] 2021-07-19 16:55:34,754 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
[INFO] 2021-07-19 16:55:34,768 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:55:34,786 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:34,790 org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.29:34509 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:55:34,792 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
[DEBUG] 2021-07-19 16:55:34,792 org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
[DEBUG] 2021-07-19 16:55:34,793 org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 25 ms
[DEBUG] 2021-07-19 16:55:34,793 org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 25 ms
[INFO] 2021-07-19 16:55:34,794 org.apache.spark.SparkContext - Created broadcast 3 from save at Simulator.scala:30
[INFO] 2021-07-19 16:55:34,799 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:55:34,803 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
[DEBUG] 2021-07-19 16:55:34,806 org.apache.spark.storage.BlockManager - Removing broadcast 1
[DEBUG] 2021-07-19 16:55:34,810 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:55:34,812 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:55:34,813 org.apache.spark.storage.BlockManager - Removing block broadcast_1
[DEBUG] 2021-07-19 16:55:34,814 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 11088 dropped from memory (free 1043756753)
[DEBUG] 2021-07-19 16:55:34,816 org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
[DEBUG] 2021-07-19 16:55:34,817 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 5515 dropped from memory (free 1043762268)
[DEBUG] 2021-07-19 16:55:34,818 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:34,820 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.1.29:34509 in memory (size: 5.4 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:55:34,820 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 16:55:34,821 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 16:55:34,822 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
[DEBUG] 2021-07-19 16:55:34,823 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:37295
[DEBUG] 2021-07-19 16:55:34,852 org.apache.spark.ContextCleaner - Cleaned broadcast 1
[DEBUG] 2021-07-19 16:55:34,853 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(23)
[DEBUG] 2021-07-19 16:55:34,853 org.apache.spark.ContextCleaner - Cleaning accumulator 23
[DEBUG] 2021-07-19 16:55:34,853 org.apache.spark.ContextCleaner - Cleaned accumulator 23
[DEBUG] 2021-07-19 16:55:34,853 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(36)
[DEBUG] 2021-07-19 16:55:34,853 org.apache.spark.ContextCleaner - Cleaning accumulator 36
[DEBUG] 2021-07-19 16:55:34,853 org.apache.spark.ContextCleaner - Cleaned accumulator 36
[DEBUG] 2021-07-19 16:55:34,853 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(29)
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Cleaning accumulator 29
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Cleaned accumulator 29
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(21)
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Cleaning accumulator 21
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Cleaned accumulator 21
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(17)
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Cleaning accumulator 17
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Cleaned accumulator 17
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(20)
[DEBUG] 2021-07-19 16:55:34,854 org.apache.spark.ContextCleaner - Cleaning accumulator 20
[DEBUG] 2021-07-19 16:55:34,856 org.apache.spark.ContextCleaner - Cleaned accumulator 20
[DEBUG] 2021-07-19 16:55:34,856 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(19)
[DEBUG] 2021-07-19 16:55:34,857 org.apache.spark.ContextCleaner - Cleaning accumulator 19
[DEBUG] 2021-07-19 16:55:34,857 org.apache.spark.ContextCleaner - Cleaned accumulator 19
[DEBUG] 2021-07-19 16:55:34,857 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(25)
[DEBUG] 2021-07-19 16:55:34,857 org.apache.spark.ContextCleaner - Cleaning accumulator 25
[DEBUG] 2021-07-19 16:55:34,857 org.apache.spark.ContextCleaner - Cleaned accumulator 25
[DEBUG] 2021-07-19 16:55:34,857 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(37)
[DEBUG] 2021-07-19 16:55:34,857 org.apache.spark.ContextCleaner - Cleaning accumulator 37
[DEBUG] 2021-07-19 16:55:34,857 org.apache.spark.ContextCleaner - Cleaned accumulator 37
[DEBUG] 2021-07-19 16:55:34,865 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       // common sub-expressions
/* 029 */
/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(0));
/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 035 */       null : (inputadapter_row_0.getUTF8String(1));
/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 037 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 038 */       null : (inputadapter_row_0.getUTF8String(2));
/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 041 */       null : (inputadapter_row_0.getUTF8String(3));
/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 044 */       null : (inputadapter_row_0.getUTF8String(4));
/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 046 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 047 */       null : (inputadapter_row_0.getUTF8String(5));
/* 048 */       project_mutableStateArray_0[0].reset();
/* 049 */
/* 050 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 051 */
/* 052 */       if (inputadapter_isNull_0) {
/* 053 */         project_mutableStateArray_0[0].setNullAt(0);
/* 054 */       } else {
/* 055 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 056 */       }
/* 057 */
/* 058 */       if (inputadapter_isNull_1) {
/* 059 */         project_mutableStateArray_0[0].setNullAt(1);
/* 060 */       } else {
/* 061 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 062 */       }
/* 063 */
/* 064 */       if (inputadapter_isNull_2) {
/* 065 */         project_mutableStateArray_0[0].setNullAt(2);
/* 066 */       } else {
/* 067 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);
/* 068 */       }
/* 069 */
/* 070 */       if (inputadapter_isNull_3) {
/* 071 */         project_mutableStateArray_0[0].setNullAt(3);
/* 072 */       } else {
/* 073 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);
/* 074 */       }
/* 075 */
/* 076 */       if (inputadapter_isNull_4) {
/* 077 */         project_mutableStateArray_0[0].setNullAt(4);
/* 078 */       } else {
/* 079 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);
/* 080 */       }
/* 081 */
/* 082 */       if (inputadapter_isNull_5) {
/* 083 */         project_mutableStateArray_0[0].setNullAt(5);
/* 084 */       } else {
/* 085 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);
/* 086 */       }
/* 087 */
/* 088 */       project_mutableStateArray_0[0].write(6, ((UTF8String) references[0] /* literal */));
/* 089 */       append((project_mutableStateArray_0[0].getRow()));
/* 090 */       if (shouldStop()) return;
/* 091 */     }
/* 092 */   }
/* 093 */
/* 094 */ }

[DEBUG] 2021-07-19 16:55:34,868 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:55:34,870 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:55:34,906 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$write$15
[DEBUG] 2021-07-19 16:55:34,918 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$write$15) is now cleaned +++
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(13)
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Cleaning accumulator 13
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Cleaned accumulator 13
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(9)
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Cleaning accumulator 9
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Cleaned accumulator 9
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(1)
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Cleaning accumulator 1
[DEBUG] 2021-07-19 16:55:34,982 org.apache.spark.ContextCleaner - Cleaned accumulator 1
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(3)
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaning accumulator 3
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaned accumulator 3
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(12)
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaning accumulator 12
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaned accumulator 12
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(15)
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaning accumulator 15
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaned accumulator 15
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(5)
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaning accumulator 5
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaned accumulator 5
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(10)
[DEBUG] 2021-07-19 16:55:34,983 org.apache.spark.ContextCleaner - Cleaning accumulator 10
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Cleaned accumulator 10
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(14)
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Cleaning accumulator 14
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Cleaned accumulator 14
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(11)
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Cleaning accumulator 11
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Cleaned accumulator 11
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(0)
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.ContextCleaner - Cleaning broadcast 0
[DEBUG] 2021-07-19 16:55:34,984 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 0
[DEBUG] 2021-07-19 16:55:34,991 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 0
[DEBUG] 2021-07-19 16:55:34,992 org.apache.spark.storage.BlockManager - Removing broadcast 0
[DEBUG] 2021-07-19 16:55:34,992 org.apache.spark.storage.BlockManager - Removing block broadcast_0_piece0
[DEBUG] 2021-07-19 16:55:34,993 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 of size 28144 dropped from memory (free 1043790412)
[DEBUG] 2021-07-19 16:55:34,993 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:34,994 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.1.29:34509 in memory (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:55:34,995 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 16:55:34,995 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 16:55:34,995 org.apache.spark.storage.BlockManager - Removing block broadcast_0
[DEBUG] 2021-07-19 16:55:34,995 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 of size 178344 dropped from memory (free 1043968756)
[DEBUG] 2021-07-19 16:55:35,015 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 0, response is 0
[DEBUG] 2021-07-19 16:55:35,023 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:37295
[INFO] 2021-07-19 16:55:35,027 org.apache.spark.SparkContext - Starting job: save at Simulator.scala:30
[DEBUG] 2021-07-19 16:55:35,028 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 16:55:35,030 org.apache.spark.scheduler.DAGScheduler - Got job 1 (save at Simulator.scala:30) with 1 output partitions
[INFO] 2021-07-19 16:55:35,030 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (save at Simulator.scala:30)
[INFO] 2021-07-19 16:55:35,030 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 16:55:35,031 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 16:55:35,032 org.apache.spark.ContextCleaner - Cleaned broadcast 0
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(2)
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Cleaning accumulator 2
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Cleaned accumulator 2
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(8)
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Cleaning accumulator 8
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Cleaned accumulator 8
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(0)
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Cleaning accumulator 0
[DEBUG] 2021-07-19 16:55:35,033 org.apache.spark.ContextCleaner - Cleaned accumulator 0
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(4)
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Cleaning accumulator 4
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Cleaned accumulator 4
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(7)
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Cleaning accumulator 7
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Cleaned accumulator 7
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(6)
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Cleaning accumulator 6
[DEBUG] 2021-07-19 16:55:35,034 org.apache.spark.ContextCleaner - Cleaned accumulator 6
[DEBUG] 2021-07-19 16:55:35,039 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 1 (name=save at Simulator.scala:30;jobs=1))
[DEBUG] 2021-07-19 16:55:35,040 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:55:35,040 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (CoalescedRDD[15] at save at Simulator.scala:30), which has no missing parents
[DEBUG] 2021-07-19 16:55:35,040 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 1)
[INFO] 2021-07-19 16:55:35,113 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 176.1 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:55:35,114 org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 1 ms
[DEBUG] 2021-07-19 16:55:35,114 org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 2 ms
[INFO] 2021-07-19 16:55:35,128 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 63.1 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:55:35,128 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.1.29, 34509, None)
[DEBUG] 2021-07-19 16:55:35,129 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(2)
[DEBUG] 2021-07-19 16:55:35,129 org.apache.spark.ContextCleaner - Cleaning broadcast 2
[DEBUG] 2021-07-19 16:55:35,129 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
[INFO] 2021-07-19 16:55:35,129 org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.29:34509 (size: 63.1 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:55:35,133 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
[DEBUG] 2021-07-19 16:55:35,133 org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
[DEBUG] 2021-07-19 16:55:35,133 org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 6 ms
[DEBUG] 2021-07-19 16:55:35,133 org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 6 ms
[INFO] 2021-07-19 16:55:35,134 org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:55:35,135 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[15] at save at Simulator.scala:30) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:55:35,138 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:55:35,138 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
[DEBUG] 2021-07-19 16:55:35,139 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
[DEBUG] 2021-07-19 16:55:35,139 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
[DEBUG] 2021-07-19 16:55:35,140 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
[DEBUG] 2021-07-19 16:55:35,143 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 2
[DEBUG] 2021-07-19 16:55:35,143 org.apache.spark.storage.BlockManager - Removing broadcast 2
[DEBUG] 2021-07-19 16:55:35,143 org.apache.spark.storage.BlockManager - Removing block broadcast_2
[DEBUG] 2021-07-19 16:55:35,144 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 178344 dropped from memory (free 1043902150)
[DEBUG] 2021-07-19 16:55:35,145 org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
[DEBUG] 2021-07-19 16:55:35,145 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 28144 dropped from memory (free 1043930294)
[DEBUG] 2021-07-19 16:55:35,153 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 34509, None)
[INFO] 2021-07-19 16:55:35,153 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.1.29:34509 in memory (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:55:35,154 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 16:55:35,154 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[INFO] 2021-07-19 16:55:35,159 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 5115 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:55:35,163 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 16:55:35,164 org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[DEBUG] 2021-07-19 16:55:35,177 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
[DEBUG] 2021-07-19 16:55:35,181 org.apache.spark.storage.BlockManager - Getting local block broadcast_4
[DEBUG] 2021-07-19 16:55:35,181 org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 16:55:35,190 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 2, response is 0
[DEBUG] 2021-07-19 16:55:35,190 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:37295
[DEBUG] 2021-07-19 16:55:35,191 org.apache.spark.ContextCleaner - Cleaned broadcast 2
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(42)
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Cleaning accumulator 42
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Cleaned accumulator 42
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(44)
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Cleaning accumulator 44
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Cleaned accumulator 44
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(41)
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Cleaning accumulator 41
[DEBUG] 2021-07-19 16:55:35,192 org.apache.spark.ContextCleaner - Cleaned accumulator 41
[DEBUG] 2021-07-19 16:55:35,193 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(43)
[DEBUG] 2021-07-19 16:55:35,193 org.apache.spark.ContextCleaner - Cleaning accumulator 43
[DEBUG] 2021-07-19 16:55:35,193 org.apache.spark.ContextCleaner - Cleaned accumulator 43
[INFO] 2021-07-19 16:55:35,359 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO] 2021-07-19 16:55:35,418 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data/aapl.us.txt, range: 0-428286, partition values: [empty row]
[DEBUG] 2021-07-19 16:55:35,443 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */   private void writeFields_0_0(InternalRow i) {
/* 051 */
/* 052 */     boolean isNull_0 = i.isNullAt(0);
/* 053 */     UTF8String value_0 = isNull_0 ?
/* 054 */     null : (i.getUTF8String(0));
/* 055 */     if (isNull_0) {
/* 056 */       mutableStateArray_0[0].setNullAt(0);
/* 057 */     } else {
/* 058 */       mutableStateArray_0[0].write(0, value_0);
/* 059 */     }
/* 060 */
/* 061 */     boolean isNull_1 = i.isNullAt(1);
/* 062 */     UTF8String value_1 = isNull_1 ?
/* 063 */     null : (i.getUTF8String(1));
/* 064 */     if (isNull_1) {
/* 065 */       mutableStateArray_0[0].setNullAt(1);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(1, value_1);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_2 = i.isNullAt(2);
/* 071 */     UTF8String value_2 = isNull_2 ?
/* 072 */     null : (i.getUTF8String(2));
/* 073 */     if (isNull_2) {
/* 074 */       mutableStateArray_0[0].setNullAt(2);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(2, value_2);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_3 = i.isNullAt(3);
/* 080 */     UTF8String value_3 = isNull_3 ?
/* 081 */     null : (i.getUTF8String(3));
/* 082 */     if (isNull_3) {
/* 083 */       mutableStateArray_0[0].setNullAt(3);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(3, value_3);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_4 = i.isNullAt(4);
/* 089 */     UTF8String value_4 = isNull_4 ?
/* 090 */     null : (i.getUTF8String(4));
/* 091 */     if (isNull_4) {
/* 092 */       mutableStateArray_0[0].setNullAt(4);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(4, value_4);
/* 095 */     }
/* 096 */
/* 097 */   }
/* 098 */
/* 099 */ }

[DEBUG] 2021-07-19 16:55:35,450 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */   private void writeFields_0_0(InternalRow i) {
/* 051 */
/* 052 */     boolean isNull_0 = i.isNullAt(0);
/* 053 */     UTF8String value_0 = isNull_0 ?
/* 054 */     null : (i.getUTF8String(0));
/* 055 */     if (isNull_0) {
/* 056 */       mutableStateArray_0[0].setNullAt(0);
/* 057 */     } else {
/* 058 */       mutableStateArray_0[0].write(0, value_0);
/* 059 */     }
/* 060 */
/* 061 */     boolean isNull_1 = i.isNullAt(1);
/* 062 */     UTF8String value_1 = isNull_1 ?
/* 063 */     null : (i.getUTF8String(1));
/* 064 */     if (isNull_1) {
/* 065 */       mutableStateArray_0[0].setNullAt(1);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(1, value_1);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_2 = i.isNullAt(2);
/* 071 */     UTF8String value_2 = isNull_2 ?
/* 072 */     null : (i.getUTF8String(2));
/* 073 */     if (isNull_2) {
/* 074 */       mutableStateArray_0[0].setNullAt(2);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(2, value_2);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_3 = i.isNullAt(3);
/* 080 */     UTF8String value_3 = isNull_3 ?
/* 081 */     null : (i.getUTF8String(3));
/* 082 */     if (isNull_3) {
/* 083 */       mutableStateArray_0[0].setNullAt(3);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(3, value_3);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_4 = i.isNullAt(4);
/* 089 */     UTF8String value_4 = isNull_4 ?
/* 090 */     null : (i.getUTF8String(4));
/* 091 */     if (isNull_4) {
/* 092 */       mutableStateArray_0[0].setNullAt(4);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(4, value_4);
/* 095 */     }
/* 096 */
/* 097 */   }
/* 098 */
/* 099 */ }

[INFO] 2021-07-19 16:55:35,480 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 36.426727 ms
[DEBUG] 2021-07-19 16:55:35,481 org.apache.spark.storage.BlockManager - Getting local block broadcast_3
[DEBUG] 2021-07-19 16:55:35,481 org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 16:55:36,045 org.apache.spark.scheduler.OutputCommitCoordinator - Commit allowed for stage=1.0, partition=0, task attempt 0
[INFO] 2021-07-19 16:55:36,047 org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202107191655341398148485967936821_0001_m_000000_1: Committed
[INFO] 2021-07-19 16:55:36,060 org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2525 bytes result sent to driver
[DEBUG] 2021-07-19 16:55:36,061 org.apache.spark.executor.ExecutorMetricsPoller - removing (1, 0) from stageTCMP
[INFO] 2021-07-19 16:55:36,063 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 923 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:55:36,063 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:55:36,064 org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (save at Simulator.scala:30) finished in 1.020 s
[DEBUG] 2021-07-19 16:55:36,064 org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
[INFO] 2021-07-19 16:55:36,065 org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:55:36,065 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
[DEBUG] 2021-07-19 16:55:36,065 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@7a905ff9)
[INFO] 2021-07-19 16:55:36,066 org.apache.spark.scheduler.DAGScheduler - Job 1 finished: save at Simulator.scala:30, took 1.038285 s
[DEBUG] 2021-07-19 16:55:36,079 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Committing files staged for absolute locations Map()
[DEBUG] 2021-07-19 16:55:36,080 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Create absolute parent directories: Set()
[INFO] 2021-07-19 16:55:36,082 org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job ea5eb090-cdcb-4533-a40d-7e8c2de4270c committed.
[INFO] 2021-07-19 16:55:36,086 org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job ea5eb090-cdcb-4533-a40d-7e8c2de4270c.
[INFO] 2021-07-19 16:55:36,097 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:55:36,111 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:55:36,125 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:55:36,142 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:55:36,142 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:55:36,146 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:55:36,148 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:55:36,156 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:55:36,156 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:55:36,157 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-db29207f-949b-4689-9029-8472d217bf41
[WARN] 2021-07-19 16:57:47,777 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:57:47,779 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:57:48,068 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:57:48,655 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:57:48,656 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:57:48,669 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:57:48,671 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:57:48,736 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:57:48,754 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:57:48,756 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:57:48,873 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:57:48,875 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:57:48,876 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:57:48,878 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:57:48,879 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[DEBUG] 2021-07-19 16:57:49,507 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 40453
[INFO] 2021-07-19 16:57:49,519 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40453.
[DEBUG] 2021-07-19 16:57:49,521 org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
[INFO] 2021-07-19 16:57:49,567 org.apache.spark.SparkEnv - Registering MapOutputTracker
[DEBUG] 2021-07-19 16:57:49,569 org.apache.spark.MapOutputTrackerMasterEndpoint - init
[INFO] 2021-07-19 16:57:49,628 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:57:49,666 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:57:49,668 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:57:49,675 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:57:49,769 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-1872c773-c133-4137-a6bc-f11470217893
[DEBUG] 2021-07-19 16:57:49,773 org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
[DEBUG] 2021-07-19 16:57:49,776 org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
[INFO] 2021-07-19 16:57:49,906 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:57:49,945 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[DEBUG] 2021-07-19 16:57:49,947 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
[DEBUG] 2021-07-19 16:57:49,968 org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[DEBUG] 2021-07-19 16:57:50,288 org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
[INFO] 2021-07-19 16:57:50,329 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:57:50,416 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:57:50,668 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[DEBUG] 2021-07-19 16:57:50,702 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 45627
[INFO] 2021-07-19 16:57:50,702 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45627.
[INFO] 2021-07-19 16:57:50,703 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:45627
[INFO] 2021-07-19 16:57:50,705 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:57:50,714 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 45627, None)
[DEBUG] 2021-07-19 16:57:50,717 org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.1.29
[INFO] 2021-07-19 16:57:50,719 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:45627 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 45627, None)
[INFO] 2021-07-19 16:57:50,721 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 45627, None)
[INFO] 2021-07-19 16:57:50,723 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 45627, None)
[DEBUG] 2021-07-19 16:57:51,020 org.apache.spark.SparkContext - Adding shutdown hook
[INFO] 2021-07-19 16:57:51,257 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:57:51,257 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[DEBUG] 2021-07-19 16:57:51,261 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> test
[DEBUG] 2021-07-19 16:57:51,261 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local[*]
[DEBUG] 2021-07-19 16:57:52,262 org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:
  
[INFO] 2021-07-19 16:57:52,332 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 54 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 16:57:52,436 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.
[DEBUG] 2021-07-19 16:57:54,468 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#0
[DEBUG] 2021-07-19 16:57:54,834 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#6
[INFO] 2021-07-19 16:57:55,406 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:57:55,411 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
[INFO] 2021-07-19 16:57:55,418 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[DEBUG] 2021-07-19 16:57:56,210 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[DEBUG] 2021-07-19 16:57:56,261 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[INFO] 2021-07-19 16:57:56,579 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 367.543222 ms
[INFO] 2021-07-19 16:57:56,642 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 174.2 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:57:56,644 org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 45 ms
[DEBUG] 2021-07-19 16:57:56,647 org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 47 ms
[INFO] 2021-07-19 16:57:56,760 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:57:56,762 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 45627, None)
[INFO] 2021-07-19 16:57:56,764 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.29:45627 (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:57:56,766 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 16:57:56,766 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 16:57:56,767 org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 9 ms
[DEBUG] 2021-07-19 16:57:56,768 org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 10 ms
[INFO] 2021-07-19 16:57:56,770 org.apache.spark.SparkContext - Created broadcast 0 from csv at Simulator.scala:34
[INFO] 2021-07-19 16:57:56,792 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:57:56,881 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:57:56,904 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:57:56,955 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
[DEBUG] 2021-07-19 16:57:56,960 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
[DEBUG] 2021-07-19 16:57:56,963 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 16:57:56,981 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 16:57:56,983 org.apache.spark.SparkContext - Starting job: csv at Simulator.scala:34
[DEBUG] 2021-07-19 16:57:56,990 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 16:57:57,008 org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at Simulator.scala:34) with 1 output partitions
[INFO] 2021-07-19 16:57:57,009 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at Simulator.scala:34)
[INFO] 2021-07-19 16:57:57,009 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 16:57:57,011 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 16:57:57,028 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=csv at Simulator.scala:34;jobs=0))
[DEBUG] 2021-07-19 16:57:57,029 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:57:57,040 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:34), which has no missing parents
[DEBUG] 2021-07-19 16:57:57,041 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
[INFO] 2021-07-19 16:57:57,147 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:57:57,148 org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 2 ms
[DEBUG] 2021-07-19 16:57:57,148 org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 2 ms
[INFO] 2021-07-19 16:57:57,150 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:57:57,150 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 45627, None)
[INFO] 2021-07-19 16:57:57,151 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.29:45627 (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:57:57,151 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 16:57:57,152 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 16:57:57,152 org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 2 ms
[DEBUG] 2021-07-19 16:57:57,152 org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 3 ms
[INFO] 2021-07-19 16:57:57,153 org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:57:57,170 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:34) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:57:57,171 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:57:57,192 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
[DEBUG] 2021-07-19 16:57:57,197 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 3 ms
[DEBUG] 2021-07-19 16:57:57,198 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[DEBUG] 2021-07-19 16:57:57,215 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
[DEBUG] 2021-07-19 16:57:57,217 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[INFO] 2021-07-19 16:57:57,252 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4941 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:57:57,266 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 16:57:57,279 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[DEBUG] 2021-07-19 16:57:57,294 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
[DEBUG] 2021-07-19 16:57:57,324 org.apache.spark.storage.BlockManager - Getting local block broadcast_1
[DEBUG] 2021-07-19 16:57:57,325 org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:57:57,429 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data1/aapl.txt/part-00000-4f441212-b8c4-4edf-b2fe-f60ce24a157f-c000.csv, range: 0-445012, partition values: [empty row]
[DEBUG] 2021-07-19 16:57:57,463 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[DEBUG] 2021-07-19 16:57:57,482 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[INFO] 2021-07-19 16:57:57,519 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 52.262366 ms
[DEBUG] 2021-07-19 16:57:57,521 org.apache.spark.storage.BlockManager - Getting local block broadcast_0
[DEBUG] 2021-07-19 16:57:57,521 org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:57:57,603 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1599 bytes result sent to driver
[DEBUG] 2021-07-19 16:57:57,612 org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
[INFO] 2021-07-19 16:57:57,668 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 424 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:57:57,708 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:57:57,721 org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at Simulator.scala:34) finished in 0.654 s
[DEBUG] 2021-07-19 16:57:57,737 org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
[INFO] 2021-07-19 16:57:57,742 org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:57:57,742 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
[INFO] 2021-07-19 16:57:57,745 org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at Simulator.scala:34, took 0.761380 s
[DEBUG] 2021-07-19 16:57:57,804 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[DEBUG] 2021-07-19 16:57:57,845 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[INFO] 2021-07-19 16:57:57,913 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 108.559248 ms
[INFO] 2021-07-19 16:57:58,047 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:57:58,048 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:57:58,049 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 16:57:58,073 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 174.2 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:57:58,074 org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 12 ms
[DEBUG] 2021-07-19 16:57:58,074 org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 13 ms
[INFO] 2021-07-19 16:57:58,110 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:57:58,110 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 45627, None)
[INFO] 2021-07-19 16:57:58,111 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.29:45627 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:57:58,112 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 16:57:58,112 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[DEBUG] 2021-07-19 16:57:58,117 org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 7 ms
[DEBUG] 2021-07-19 16:57:58,118 org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 8 ms
[INFO] 2021-07-19 16:57:58,119 org.apache.spark.SparkContext - Created broadcast 2 from csv at Simulator.scala:34
[INFO] 2021-07-19 16:57:58,123 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:57:58,150 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 16:57:58,170 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 16:57:58,218 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inferFromDataset$2
[DEBUG] 2021-07-19 16:57:58,221 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
[DEBUG] 2021-07-19 16:57:58,383 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:57:58,388 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:57:58,389 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:57:58,391 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to ticket#22
[INFO] 2021-07-19 16:57:58,477 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:57:58,490 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:57:58,504 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:57:58,517 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:57:58,517 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:57:58,527 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:57:58,531 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:57:58,547 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:57:58,548 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:57:58,549 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-b171e318-db2f-4f3d-8ef0-459364ac7ccc
[WARN] 2021-07-19 16:58:37,655 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:58:37,657 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:58:37,918 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:58:38,645 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:58:38,646 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:58:38,647 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:58:38,648 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:58:38,691 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:58:38,714 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:58:38,716 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:58:38,841 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:58:38,842 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:58:38,843 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:58:38,855 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:58:38,861 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[DEBUG] 2021-07-19 16:58:39,855 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 34931
[INFO] 2021-07-19 16:58:39,877 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34931.
[DEBUG] 2021-07-19 16:58:39,884 org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
[INFO] 2021-07-19 16:58:39,965 org.apache.spark.SparkEnv - Registering MapOutputTracker
[DEBUG] 2021-07-19 16:58:39,967 org.apache.spark.MapOutputTrackerMasterEndpoint - init
[INFO] 2021-07-19 16:58:40,042 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:58:40,083 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:58:40,085 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:58:40,096 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:58:40,161 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-22326102-4b86-4338-9f69-f32ea0be4782
[DEBUG] 2021-07-19 16:58:40,166 org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
[DEBUG] 2021-07-19 16:58:40,169 org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
[INFO] 2021-07-19 16:58:40,257 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:58:40,285 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[DEBUG] 2021-07-19 16:58:40,286 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
[DEBUG] 2021-07-19 16:58:40,305 org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[DEBUG] 2021-07-19 16:58:40,635 org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
[INFO] 2021-07-19 16:58:40,677 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:58:40,764 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:58:41,048 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[DEBUG] 2021-07-19 16:58:41,095 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 38547
[INFO] 2021-07-19 16:58:41,095 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38547.
[INFO] 2021-07-19 16:58:41,095 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:38547
[INFO] 2021-07-19 16:58:41,098 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:58:41,108 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 38547, None)
[DEBUG] 2021-07-19 16:58:41,111 org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.1.29
[INFO] 2021-07-19 16:58:41,114 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:38547 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 38547, None)
[INFO] 2021-07-19 16:58:41,117 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 38547, None)
[INFO] 2021-07-19 16:58:41,118 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 38547, None)
[DEBUG] 2021-07-19 16:58:41,464 org.apache.spark.SparkContext - Adding shutdown hook
[INFO] 2021-07-19 16:58:41,982 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:58:41,983 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[DEBUG] 2021-07-19 16:58:41,991 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> test
[DEBUG] 2021-07-19 16:58:41,992 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local[*]
[DEBUG] 2021-07-19 16:58:42,968 org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:
  
[INFO] 2021-07-19 16:58:43,026 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 47 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 16:58:43,136 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 3 ms to list leaf files for 1 paths.
[DEBUG] 2021-07-19 16:58:45,570 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#0
[DEBUG] 2021-07-19 16:58:46,035 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#6
[INFO] 2021-07-19 16:58:46,518 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:58:46,521 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
[INFO] 2021-07-19 16:58:46,526 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[DEBUG] 2021-07-19 16:58:47,110 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[DEBUG] 2021-07-19 16:58:47,142 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[INFO] 2021-07-19 16:58:47,520 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 409.107705 ms
[INFO] 2021-07-19 16:58:47,621 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 174.2 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:58:47,624 org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 77 ms
[DEBUG] 2021-07-19 16:58:47,626 org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 79 ms
[INFO] 2021-07-19 16:58:47,707 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:58:47,709 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 38547, None)
[INFO] 2021-07-19 16:58:47,711 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.29:38547 (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:58:47,712 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 16:58:47,713 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 16:58:47,714 org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 10 ms
[DEBUG] 2021-07-19 16:58:47,714 org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 10 ms
[INFO] 2021-07-19 16:58:47,716 org.apache.spark.SparkContext - Created broadcast 0 from csv at Simulator.scala:34
[INFO] 2021-07-19 16:58:47,729 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:58:47,813 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:58:47,837 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:58:47,900 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
[DEBUG] 2021-07-19 16:58:47,904 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
[DEBUG] 2021-07-19 16:58:47,907 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 16:58:47,925 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 16:58:47,926 org.apache.spark.SparkContext - Starting job: csv at Simulator.scala:34
[DEBUG] 2021-07-19 16:58:47,933 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 16:58:47,949 org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at Simulator.scala:34) with 1 output partitions
[INFO] 2021-07-19 16:58:47,950 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at Simulator.scala:34)
[INFO] 2021-07-19 16:58:47,951 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 16:58:47,953 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 16:58:47,958 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=csv at Simulator.scala:34;jobs=0))
[DEBUG] 2021-07-19 16:58:47,960 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:58:47,962 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:34), which has no missing parents
[DEBUG] 2021-07-19 16:58:47,962 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
[INFO] 2021-07-19 16:58:48,085 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:58:48,086 org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 1 ms
[DEBUG] 2021-07-19 16:58:48,086 org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 2 ms
[INFO] 2021-07-19 16:58:48,088 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:58:48,088 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 38547, None)
[INFO] 2021-07-19 16:58:48,089 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.29:38547 (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:58:48,090 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 16:58:48,090 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 16:58:48,090 org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 3 ms
[DEBUG] 2021-07-19 16:58:48,091 org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 3 ms
[INFO] 2021-07-19 16:58:48,091 org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:58:48,109 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:34) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:58:48,110 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:58:48,134 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
[DEBUG] 2021-07-19 16:58:48,140 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 4 ms
[DEBUG] 2021-07-19 16:58:48,142 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[DEBUG] 2021-07-19 16:58:48,160 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
[DEBUG] 2021-07-19 16:58:48,161 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[INFO] 2021-07-19 16:58:48,195 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4941 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:58:48,202 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 16:58:48,224 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[DEBUG] 2021-07-19 16:58:48,242 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
[DEBUG] 2021-07-19 16:58:48,280 org.apache.spark.storage.BlockManager - Getting local block broadcast_1
[DEBUG] 2021-07-19 16:58:48,281 org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:58:48,408 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data1/aapl.txt/part-00000-4f441212-b8c4-4edf-b2fe-f60ce24a157f-c000.csv, range: 0-445012, partition values: [empty row]
[DEBUG] 2021-07-19 16:58:48,424 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[DEBUG] 2021-07-19 16:58:48,431 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[INFO] 2021-07-19 16:58:48,451 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.428319 ms
[DEBUG] 2021-07-19 16:58:48,454 org.apache.spark.storage.BlockManager - Getting local block broadcast_0
[DEBUG] 2021-07-19 16:58:48,461 org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:58:48,519 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1642 bytes result sent to driver
[DEBUG] 2021-07-19 16:58:48,520 org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
[INFO] 2021-07-19 16:58:48,530 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 353 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:58:48,533 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:58:48,540 org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at Simulator.scala:34) finished in 0.556 s
[DEBUG] 2021-07-19 16:58:48,544 org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
[INFO] 2021-07-19 16:58:48,545 org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:58:48,545 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
[INFO] 2021-07-19 16:58:48,548 org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at Simulator.scala:34, took 0.620979 s
[DEBUG] 2021-07-19 16:58:48,574 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[DEBUG] 2021-07-19 16:58:48,583 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[INFO] 2021-07-19 16:58:48,599 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.917259 ms
[INFO] 2021-07-19 16:58:48,708 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:58:48,708 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:58:48,709 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 16:58:48,736 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 174.2 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:58:48,737 org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 18 ms
[DEBUG] 2021-07-19 16:58:48,737 org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 19 ms
[INFO] 2021-07-19 16:58:48,755 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:58:48,756 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 38547, None)
[INFO] 2021-07-19 16:58:48,757 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.29:38547 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:58:48,758 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 16:58:48,758 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[DEBUG] 2021-07-19 16:58:48,758 org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 4 ms
[DEBUG] 2021-07-19 16:58:48,759 org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 4 ms
[INFO] 2021-07-19 16:58:48,759 org.apache.spark.SparkContext - Created broadcast 2 from csv at Simulator.scala:34
[INFO] 2021-07-19 16:58:48,761 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:58:48,772 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 16:58:48,784 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 16:58:48,816 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inferFromDataset$2
[DEBUG] 2021-07-19 16:58:48,818 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
[DEBUG] 2021-07-19 16:58:48,963 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:58:48,967 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:58:48,968 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:58:48,969 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to ticket#22
[INFO] 2021-07-19 16:58:49,018 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:58:49,030 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:58:49,041 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:58:49,062 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:58:49,063 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:58:49,071 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:58:49,073 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:58:49,079 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:58:49,080 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:58:49,081 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-d1ec11ef-ec1e-40ba-88ed-4bf9ed1e057a
[WARN] 2021-07-19 16:59:18,289 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 16:59:18,290 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 16:59:18,531 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 16:59:19,376 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:59:19,377 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 16:59:19,381 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 16:59:19,383 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 16:59:19,437 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 16:59:19,464 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 16:59:19,466 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 16:59:19,659 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 16:59:19,661 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 16:59:19,662 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 16:59:19,664 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 16:59:19,667 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[DEBUG] 2021-07-19 16:59:20,662 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 36715
[INFO] 2021-07-19 16:59:20,681 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36715.
[DEBUG] 2021-07-19 16:59:20,684 org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
[INFO] 2021-07-19 16:59:20,719 org.apache.spark.SparkEnv - Registering MapOutputTracker
[DEBUG] 2021-07-19 16:59:20,720 org.apache.spark.MapOutputTrackerMasterEndpoint - init
[INFO] 2021-07-19 16:59:20,759 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 16:59:20,789 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 16:59:20,790 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 16:59:20,797 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 16:59:20,863 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f991211b-248c-4413-a97b-4e89c5eaab66
[DEBUG] 2021-07-19 16:59:20,893 org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
[DEBUG] 2021-07-19 16:59:20,896 org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
[INFO] 2021-07-19 16:59:20,998 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 16:59:21,046 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[DEBUG] 2021-07-19 16:59:21,050 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
[DEBUG] 2021-07-19 16:59:21,072 org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[DEBUG] 2021-07-19 16:59:21,438 org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
[INFO] 2021-07-19 16:59:21,495 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 16:59:21,692 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 16:59:22,030 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[DEBUG] 2021-07-19 16:59:22,069 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 35439
[INFO] 2021-07-19 16:59:22,069 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35439.
[INFO] 2021-07-19 16:59:22,069 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:35439
[INFO] 2021-07-19 16:59:22,071 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 16:59:22,078 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 35439, None)
[DEBUG] 2021-07-19 16:59:22,081 org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.1.29
[INFO] 2021-07-19 16:59:22,084 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:35439 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 35439, None)
[INFO] 2021-07-19 16:59:22,093 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 35439, None)
[INFO] 2021-07-19 16:59:22,105 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 35439, None)
[DEBUG] 2021-07-19 16:59:22,413 org.apache.spark.SparkContext - Adding shutdown hook
[INFO] 2021-07-19 16:59:22,656 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 16:59:22,656 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[DEBUG] 2021-07-19 16:59:22,659 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> test
[DEBUG] 2021-07-19 16:59:22,660 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local[*]
[DEBUG] 2021-07-19 16:59:23,722 org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:
  
[INFO] 2021-07-19 16:59:23,794 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 53 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 16:59:23,893 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.
[DEBUG] 2021-07-19 16:59:25,965 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#0
[DEBUG] 2021-07-19 16:59:26,234 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#6
[INFO] 2021-07-19 16:59:26,714 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:59:26,717 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
[INFO] 2021-07-19 16:59:26,721 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[DEBUG] 2021-07-19 16:59:27,298 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[DEBUG] 2021-07-19 16:59:27,336 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[INFO] 2021-07-19 16:59:27,664 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 359.358173 ms
[INFO] 2021-07-19 16:59:27,769 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 174.2 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:59:27,771 org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 74 ms
[DEBUG] 2021-07-19 16:59:27,773 org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 76 ms
[INFO] 2021-07-19 16:59:27,849 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:59:27,852 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 35439, None)
[INFO] 2021-07-19 16:59:27,855 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.29:35439 (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:59:27,863 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 16:59:27,864 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 16:59:27,865 org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 18 ms
[DEBUG] 2021-07-19 16:59:27,865 org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 18 ms
[INFO] 2021-07-19 16:59:27,867 org.apache.spark.SparkContext - Created broadcast 0 from csv at Simulator.scala:34
[INFO] 2021-07-19 16:59:27,885 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:59:27,977 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:59:28,000 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:59:28,058 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
[DEBUG] 2021-07-19 16:59:28,063 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
[DEBUG] 2021-07-19 16:59:28,068 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 16:59:28,083 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 16:59:28,085 org.apache.spark.SparkContext - Starting job: csv at Simulator.scala:34
[DEBUG] 2021-07-19 16:59:28,092 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 16:59:28,109 org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at Simulator.scala:34) with 1 output partitions
[INFO] 2021-07-19 16:59:28,109 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at Simulator.scala:34)
[INFO] 2021-07-19 16:59:28,110 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 16:59:28,112 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 16:59:28,114 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=csv at Simulator.scala:34;jobs=0))
[DEBUG] 2021-07-19 16:59:28,116 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:59:28,118 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:34), which has no missing parents
[DEBUG] 2021-07-19 16:59:28,119 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
[INFO] 2021-07-19 16:59:28,262 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:59:28,263 org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 1 ms
[DEBUG] 2021-07-19 16:59:28,263 org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 2 ms
[INFO] 2021-07-19 16:59:28,265 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 16:59:28,265 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 35439, None)
[INFO] 2021-07-19 16:59:28,266 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.29:35439 (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 16:59:28,270 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 16:59:28,270 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 16:59:28,270 org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 6 ms
[DEBUG] 2021-07-19 16:59:28,271 org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 6 ms
[INFO] 2021-07-19 16:59:28,272 org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:59:28,293 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:34) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:59:28,294 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:59:28,319 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
[DEBUG] 2021-07-19 16:59:28,324 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 3 ms
[DEBUG] 2021-07-19 16:59:28,327 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[DEBUG] 2021-07-19 16:59:28,350 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
[DEBUG] 2021-07-19 16:59:28,350 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[INFO] 2021-07-19 16:59:28,383 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4941 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:59:28,389 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 16:59:28,406 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[DEBUG] 2021-07-19 16:59:28,417 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
[DEBUG] 2021-07-19 16:59:28,447 org.apache.spark.storage.BlockManager - Getting local block broadcast_1
[DEBUG] 2021-07-19 16:59:28,448 org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:59:28,621 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data1/aapl.txt/part-00000-4f441212-b8c4-4edf-b2fe-f60ce24a157f-c000.csv, range: 0-445012, partition values: [empty row]
[DEBUG] 2021-07-19 16:59:28,632 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[DEBUG] 2021-07-19 16:59:28,635 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[INFO] 2021-07-19 16:59:28,665 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 33.668648 ms
[DEBUG] 2021-07-19 16:59:28,667 org.apache.spark.storage.BlockManager - Getting local block broadcast_0
[DEBUG] 2021-07-19 16:59:28,667 org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 16:59:28,711 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1642 bytes result sent to driver
[DEBUG] 2021-07-19 16:59:28,713 org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
[INFO] 2021-07-19 16:59:28,721 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 358 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 16:59:28,723 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 16:59:28,732 org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at Simulator.scala:34) finished in 0.582 s
[DEBUG] 2021-07-19 16:59:28,736 org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
[INFO] 2021-07-19 16:59:28,737 org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 16:59:28,737 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
[INFO] 2021-07-19 16:59:28,740 org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at Simulator.scala:34, took 0.654031 s
[DEBUG] 2021-07-19 16:59:28,761 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[DEBUG] 2021-07-19 16:59:28,768 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[INFO] 2021-07-19 16:59:28,785 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.121653 ms
[INFO] 2021-07-19 16:59:28,893 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 16:59:28,894 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 16:59:28,894 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 16:59:28,908 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 174.2 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:59:28,909 org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 9 ms
[DEBUG] 2021-07-19 16:59:28,909 org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 10 ms
[INFO] 2021-07-19 16:59:28,926 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 16:59:28,927 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 35439, None)
[INFO] 2021-07-19 16:59:28,927 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.29:35439 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:59:28,928 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 16:59:28,928 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[DEBUG] 2021-07-19 16:59:28,928 org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 2 ms
[DEBUG] 2021-07-19 16:59:28,928 org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 2 ms
[INFO] 2021-07-19 16:59:28,930 org.apache.spark.SparkContext - Created broadcast 2 from csv at Simulator.scala:34
[INFO] 2021-07-19 16:59:28,931 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:59:28,942 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 16:59:28,949 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 16:59:28,990 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inferFromDataset$2
[DEBUG] 2021-07-19 16:59:28,993 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
[DEBUG] 2021-07-19 16:59:29,079 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:59:29,082 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:59:29,083 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to 'ticket
[DEBUG] 2021-07-19 16:59:29,084 org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1 - Resolving 'ticket to ticket#22
[INFO] 2021-07-19 16:59:29,214 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(Ticket),EqualTo(Ticket,aapl)
[INFO] 2021-07-19 16:59:29,215 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(ticket#22),(ticket#22 = aapl)
[INFO] 2021-07-19 16:59:29,216 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<date: string, open: string, high: string, low: string, close: string ... 5 more fields>
[DEBUG] 2021-07-19 16:59:29,272 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while ( inputadapter_input_0.hasNext()) {
/* 027 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 028 */
/* 029 */       do {
/* 030 */         boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);
/* 031 */         UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?
/* 032 */         null : (inputadapter_row_0.getUTF8String(6));
/* 033 */
/* 034 */         boolean filter_value_2 = !inputadapter_isNull_6;
/* 035 */         if (!filter_value_2) continue;
/* 036 */
/* 037 */         boolean filter_value_3 = false;
/* 038 */         filter_value_3 = inputadapter_value_6.equals(((UTF8String) references[1] /* literal */));
/* 039 */         if (!filter_value_3) continue;
/* 040 */
/* 041 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 042 */
/* 043 */         // common sub-expressions
/* 044 */
/* 045 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 046 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 047 */         null : (inputadapter_row_0.getUTF8String(0));
/* 048 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 049 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 050 */         null : (inputadapter_row_0.getUTF8String(1));
/* 051 */         boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 052 */         UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 053 */         null : (inputadapter_row_0.getUTF8String(2));
/* 054 */         boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 055 */         UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 056 */         null : (inputadapter_row_0.getUTF8String(3));
/* 057 */         boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 058 */         UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 059 */         null : (inputadapter_row_0.getUTF8String(4));
/* 060 */         boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 061 */         UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 062 */         null : (inputadapter_row_0.getUTF8String(5));
/* 063 */         filter_mutableStateArray_0[1].reset();
/* 064 */
/* 065 */         filter_mutableStateArray_0[1].zeroOutNullBytes();
/* 066 */
/* 067 */         if (inputadapter_isNull_0) {
/* 068 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 069 */         } else {
/* 070 */           filter_mutableStateArray_0[1].write(0, inputadapter_value_0);
/* 071 */         }
/* 072 */
/* 073 */         if (inputadapter_isNull_1) {
/* 074 */           filter_mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           filter_mutableStateArray_0[1].write(1, inputadapter_value_1);
/* 077 */         }
/* 078 */
/* 079 */         if (inputadapter_isNull_2) {
/* 080 */           filter_mutableStateArray_0[1].setNullAt(2);
/* 081 */         } else {
/* 082 */           filter_mutableStateArray_0[1].write(2, inputadapter_value_2);
/* 083 */         }
/* 084 */
/* 085 */         if (inputadapter_isNull_3) {
/* 086 */           filter_mutableStateArray_0[1].setNullAt(3);
/* 087 */         } else {
/* 088 */           filter_mutableStateArray_0[1].write(3, inputadapter_value_3);
/* 089 */         }
/* 090 */
/* 091 */         if (inputadapter_isNull_4) {
/* 092 */           filter_mutableStateArray_0[1].setNullAt(4);
/* 093 */         } else {
/* 094 */           filter_mutableStateArray_0[1].write(4, inputadapter_value_4);
/* 095 */         }
/* 096 */
/* 097 */         if (inputadapter_isNull_5) {
/* 098 */           filter_mutableStateArray_0[1].setNullAt(5);
/* 099 */         } else {
/* 100 */           filter_mutableStateArray_0[1].write(5, inputadapter_value_5);
/* 101 */         }
/* 102 */         append((filter_mutableStateArray_0[1].getRow()));
/* 103 */
/* 104 */       } while(false);
/* 105 */       if (shouldStop()) return;
/* 106 */     }
/* 107 */   }
/* 108 */
/* 109 */ }

[DEBUG] 2021-07-19 16:59:29,281 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while ( inputadapter_input_0.hasNext()) {
/* 027 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 028 */
/* 029 */       do {
/* 030 */         boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);
/* 031 */         UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?
/* 032 */         null : (inputadapter_row_0.getUTF8String(6));
/* 033 */
/* 034 */         boolean filter_value_2 = !inputadapter_isNull_6;
/* 035 */         if (!filter_value_2) continue;
/* 036 */
/* 037 */         boolean filter_value_3 = false;
/* 038 */         filter_value_3 = inputadapter_value_6.equals(((UTF8String) references[1] /* literal */));
/* 039 */         if (!filter_value_3) continue;
/* 040 */
/* 041 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 042 */
/* 043 */         // common sub-expressions
/* 044 */
/* 045 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 046 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 047 */         null : (inputadapter_row_0.getUTF8String(0));
/* 048 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 049 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 050 */         null : (inputadapter_row_0.getUTF8String(1));
/* 051 */         boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 052 */         UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 053 */         null : (inputadapter_row_0.getUTF8String(2));
/* 054 */         boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 055 */         UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 056 */         null : (inputadapter_row_0.getUTF8String(3));
/* 057 */         boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 058 */         UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 059 */         null : (inputadapter_row_0.getUTF8String(4));
/* 060 */         boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 061 */         UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 062 */         null : (inputadapter_row_0.getUTF8String(5));
/* 063 */         filter_mutableStateArray_0[1].reset();
/* 064 */
/* 065 */         filter_mutableStateArray_0[1].zeroOutNullBytes();
/* 066 */
/* 067 */         if (inputadapter_isNull_0) {
/* 068 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 069 */         } else {
/* 070 */           filter_mutableStateArray_0[1].write(0, inputadapter_value_0);
/* 071 */         }
/* 072 */
/* 073 */         if (inputadapter_isNull_1) {
/* 074 */           filter_mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           filter_mutableStateArray_0[1].write(1, inputadapter_value_1);
/* 077 */         }
/* 078 */
/* 079 */         if (inputadapter_isNull_2) {
/* 080 */           filter_mutableStateArray_0[1].setNullAt(2);
/* 081 */         } else {
/* 082 */           filter_mutableStateArray_0[1].write(2, inputadapter_value_2);
/* 083 */         }
/* 084 */
/* 085 */         if (inputadapter_isNull_3) {
/* 086 */           filter_mutableStateArray_0[1].setNullAt(3);
/* 087 */         } else {
/* 088 */           filter_mutableStateArray_0[1].write(3, inputadapter_value_3);
/* 089 */         }
/* 090 */
/* 091 */         if (inputadapter_isNull_4) {
/* 092 */           filter_mutableStateArray_0[1].setNullAt(4);
/* 093 */         } else {
/* 094 */           filter_mutableStateArray_0[1].write(4, inputadapter_value_4);
/* 095 */         }
/* 096 */
/* 097 */         if (inputadapter_isNull_5) {
/* 098 */           filter_mutableStateArray_0[1].setNullAt(5);
/* 099 */         } else {
/* 100 */           filter_mutableStateArray_0[1].write(5, inputadapter_value_5);
/* 101 */         }
/* 102 */         append((filter_mutableStateArray_0[1].getRow()));
/* 103 */
/* 104 */       } while(false);
/* 105 */       if (shouldStop()) return;
/* 106 */     }
/* 107 */   }
/* 108 */
/* 109 */ }

[INFO] 2021-07-19 16:59:29,317 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.785308 ms
[INFO] 2021-07-19 16:59:29,321 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 174.1 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:59:29,322 org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 3 ms
[DEBUG] 2021-07-19 16:59:29,322 org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 4 ms
[INFO] 2021-07-19 16:59:29,350 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:59:29,350 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.1.29, 35439, None)
[INFO] 2021-07-19 16:59:29,351 org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.29:35439 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:59:29,351 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
[DEBUG] 2021-07-19 16:59:29,352 org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
[DEBUG] 2021-07-19 16:59:29,352 org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 2 ms
[DEBUG] 2021-07-19 16:59:29,352 org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 2 ms
[INFO] 2021-07-19 16:59:29,353 org.apache.spark.SparkContext - Created broadcast 3 from foreach at Simulator.scala:40
[INFO] 2021-07-19 16:59:29,356 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 16:59:29,361 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 16:59:29,372 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:59:29,378 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 16:59:29,385 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 16:59:29,395 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$start$1$adapted
[DEBUG] 2021-07-19 16:59:29,397 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$start$1$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:59:29,400 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$foreach$2$adapted
[DEBUG] 2021-07-19 16:59:29,417 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$foreach$2$adapted) is now cleaned +++
[DEBUG] 2021-07-19 16:59:29,433 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 16:59:29,440 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 16:59:29,441 org.apache.spark.SparkContext - Starting job: foreach at Simulator.scala:40
[DEBUG] 2021-07-19 16:59:29,442 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 16:59:29,443 org.apache.spark.scheduler.DAGScheduler - Got job 1 (foreach at Simulator.scala:40) with 1 output partitions
[INFO] 2021-07-19 16:59:29,443 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (foreach at Simulator.scala:40)
[INFO] 2021-07-19 16:59:29,443 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 16:59:29,444 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 16:59:29,458 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 1 (name=foreach at Simulator.scala:40;jobs=1))
[DEBUG] 2021-07-19 16:59:29,459 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 16:59:29,460 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[15] at foreach at Simulator.scala:40), which has no missing parents
[DEBUG] 2021-07-19 16:59:29,460 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 1)
[INFO] 2021-07-19 16:59:29,554 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 20.4 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:59:29,555 org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 1 ms
[DEBUG] 2021-07-19 16:59:29,555 org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 2 ms
[INFO] 2021-07-19 16:59:29,558 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.5 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 16:59:29,558 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.1.29, 35439, None)
[INFO] 2021-07-19 16:59:29,559 org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.29:35439 (size: 9.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 16:59:29,559 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
[DEBUG] 2021-07-19 16:59:29,559 org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
[DEBUG] 2021-07-19 16:59:29,560 org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 2 ms
[DEBUG] 2021-07-19 16:59:29,560 org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 3 ms
[INFO] 2021-07-19 16:59:29,560 org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 16:59:29,561 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at foreach at Simulator.scala:40) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 16:59:29,561 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 16:59:29,562 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
[DEBUG] 2021-07-19 16:59:29,562 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
[DEBUG] 2021-07-19 16:59:29,562 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
[DEBUG] 2021-07-19 16:59:29,563 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
[INFO] 2021-07-19 16:59:29,565 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4941 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 16:59:29,566 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 16:59:29,567 org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[DEBUG] 2021-07-19 16:59:29,569 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
[DEBUG] 2021-07-19 16:59:29,573 org.apache.spark.storage.BlockManager - Getting local block broadcast_4
[DEBUG] 2021-07-19 16:59:29,573 org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 16:59:29,661 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, input[4, string, true].toString, input[5, string, true].toString, StructField(date,StringType,true), StructField(open,StringType,true), StructField(high,StringType,true), StructField(low,StringType,true), StructField(close,StringType,true), StructField(volume,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[6];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 027 */     if (false) {
/* 028 */       mutableRow.setNullAt(0);
/* 029 */     } else {
/* 030 */
/* 031 */       mutableRow.update(0, value_0);
/* 032 */     }
/* 033 */
/* 034 */     return mutableRow;
/* 035 */   }
/* 036 */
/* 037 */
/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 039 */
/* 040 */     boolean isNull_8 = i.isNullAt(3);
/* 041 */     UTF8String value_8 = isNull_8 ?
/* 042 */     null : (i.getUTF8String(3));
/* 043 */     boolean isNull_7 = true;
/* 044 */     java.lang.String value_7 = null;
/* 045 */     if (!isNull_8) {
/* 046 */
/* 047 */       isNull_7 = false;
/* 048 */       if (!isNull_7) {
/* 049 */
/* 050 */         Object funcResult_3 = null;
/* 051 */         funcResult_3 = value_8.toString();
/* 052 */         value_7 = (java.lang.String) funcResult_3;
/* 053 */
/* 054 */       }
/* 055 */     }
/* 056 */     if (isNull_7) {
/* 057 */       values_0[3] = null;
/* 058 */     } else {
/* 059 */       values_0[3] = value_7;
/* 060 */     }
/* 061 */
/* 062 */     boolean isNull_10 = i.isNullAt(4);
/* 063 */     UTF8String value_10 = isNull_10 ?
/* 064 */     null : (i.getUTF8String(4));
/* 065 */     boolean isNull_9 = true;
/* 066 */     java.lang.String value_9 = null;
/* 067 */     if (!isNull_10) {
/* 068 */
/* 069 */       isNull_9 = false;
/* 070 */       if (!isNull_9) {
/* 071 */
/* 072 */         Object funcResult_4 = null;
/* 073 */         funcResult_4 = value_10.toString();
/* 074 */         value_9 = (java.lang.String) funcResult_4;
/* 075 */
/* 076 */       }
/* 077 */     }
/* 078 */     if (isNull_9) {
/* 079 */       values_0[4] = null;
/* 080 */     } else {
/* 081 */       values_0[4] = value_9;
/* 082 */     }
/* 083 */
/* 084 */     boolean isNull_12 = i.isNullAt(5);
/* 085 */     UTF8String value_12 = isNull_12 ?
/* 086 */     null : (i.getUTF8String(5));
/* 087 */     boolean isNull_11 = true;
/* 088 */     java.lang.String value_11 = null;
/* 089 */     if (!isNull_12) {
/* 090 */
/* 091 */       isNull_11 = false;
/* 092 */       if (!isNull_11) {
/* 093 */
/* 094 */         Object funcResult_5 = null;
/* 095 */         funcResult_5 = value_12.toString();
/* 096 */         value_11 = (java.lang.String) funcResult_5;
/* 097 */
/* 098 */       }
/* 099 */     }
/* 100 */     if (isNull_11) {
/* 101 */       values_0[5] = null;
/* 102 */     } else {
/* 103 */       values_0[5] = value_11;
/* 104 */     }
/* 105 */
/* 106 */   }
/* 107 */
/* 108 */
/* 109 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 110 */
/* 111 */     boolean isNull_2 = i.isNullAt(0);
/* 112 */     UTF8String value_2 = isNull_2 ?
/* 113 */     null : (i.getUTF8String(0));
/* 114 */     boolean isNull_1 = true;
/* 115 */     java.lang.String value_1 = null;
/* 116 */     if (!isNull_2) {
/* 117 */
/* 118 */       isNull_1 = false;
/* 119 */       if (!isNull_1) {
/* 120 */
/* 121 */         Object funcResult_0 = null;
/* 122 */         funcResult_0 = value_2.toString();
/* 123 */         value_1 = (java.lang.String) funcResult_0;
/* 124 */
/* 125 */       }
/* 126 */     }
/* 127 */     if (isNull_1) {
/* 128 */       values_0[0] = null;
/* 129 */     } else {
/* 130 */       values_0[0] = value_1;
/* 131 */     }
/* 132 */
/* 133 */     boolean isNull_4 = i.isNullAt(1);
/* 134 */     UTF8String value_4 = isNull_4 ?
/* 135 */     null : (i.getUTF8String(1));
/* 136 */     boolean isNull_3 = true;
/* 137 */     java.lang.String value_3 = null;
/* 138 */     if (!isNull_4) {
/* 139 */
/* 140 */       isNull_3 = false;
/* 141 */       if (!isNull_3) {
/* 142 */
/* 143 */         Object funcResult_1 = null;
/* 144 */         funcResult_1 = value_4.toString();
/* 145 */         value_3 = (java.lang.String) funcResult_1;
/* 146 */
/* 147 */       }
/* 148 */     }
/* 149 */     if (isNull_3) {
/* 150 */       values_0[1] = null;
/* 151 */     } else {
/* 152 */       values_0[1] = value_3;
/* 153 */     }
/* 154 */
/* 155 */     boolean isNull_6 = i.isNullAt(2);
/* 156 */     UTF8String value_6 = isNull_6 ?
/* 157 */     null : (i.getUTF8String(2));
/* 158 */     boolean isNull_5 = true;
/* 159 */     java.lang.String value_5 = null;
/* 160 */     if (!isNull_6) {
/* 161 */
/* 162 */       isNull_5 = false;
/* 163 */       if (!isNull_5) {
/* 164 */
/* 165 */         Object funcResult_2 = null;
/* 166 */         funcResult_2 = value_6.toString();
/* 167 */         value_5 = (java.lang.String) funcResult_2;
/* 168 */
/* 169 */       }
/* 170 */     }
/* 171 */     if (isNull_5) {
/* 172 */       values_0[2] = null;
/* 173 */     } else {
/* 174 */       values_0[2] = value_5;
/* 175 */     }
/* 176 */
/* 177 */   }
/* 178 */
/* 179 */ }

[DEBUG] 2021-07-19 16:59:29,673 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[6];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 027 */     if (false) {
/* 028 */       mutableRow.setNullAt(0);
/* 029 */     } else {
/* 030 */
/* 031 */       mutableRow.update(0, value_0);
/* 032 */     }
/* 033 */
/* 034 */     return mutableRow;
/* 035 */   }
/* 036 */
/* 037 */
/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 039 */
/* 040 */     boolean isNull_8 = i.isNullAt(3);
/* 041 */     UTF8String value_8 = isNull_8 ?
/* 042 */     null : (i.getUTF8String(3));
/* 043 */     boolean isNull_7 = true;
/* 044 */     java.lang.String value_7 = null;
/* 045 */     if (!isNull_8) {
/* 046 */
/* 047 */       isNull_7 = false;
/* 048 */       if (!isNull_7) {
/* 049 */
/* 050 */         Object funcResult_3 = null;
/* 051 */         funcResult_3 = value_8.toString();
/* 052 */         value_7 = (java.lang.String) funcResult_3;
/* 053 */
/* 054 */       }
/* 055 */     }
/* 056 */     if (isNull_7) {
/* 057 */       values_0[3] = null;
/* 058 */     } else {
/* 059 */       values_0[3] = value_7;
/* 060 */     }
/* 061 */
/* 062 */     boolean isNull_10 = i.isNullAt(4);
/* 063 */     UTF8String value_10 = isNull_10 ?
/* 064 */     null : (i.getUTF8String(4));
/* 065 */     boolean isNull_9 = true;
/* 066 */     java.lang.String value_9 = null;
/* 067 */     if (!isNull_10) {
/* 068 */
/* 069 */       isNull_9 = false;
/* 070 */       if (!isNull_9) {
/* 071 */
/* 072 */         Object funcResult_4 = null;
/* 073 */         funcResult_4 = value_10.toString();
/* 074 */         value_9 = (java.lang.String) funcResult_4;
/* 075 */
/* 076 */       }
/* 077 */     }
/* 078 */     if (isNull_9) {
/* 079 */       values_0[4] = null;
/* 080 */     } else {
/* 081 */       values_0[4] = value_9;
/* 082 */     }
/* 083 */
/* 084 */     boolean isNull_12 = i.isNullAt(5);
/* 085 */     UTF8String value_12 = isNull_12 ?
/* 086 */     null : (i.getUTF8String(5));
/* 087 */     boolean isNull_11 = true;
/* 088 */     java.lang.String value_11 = null;
/* 089 */     if (!isNull_12) {
/* 090 */
/* 091 */       isNull_11 = false;
/* 092 */       if (!isNull_11) {
/* 093 */
/* 094 */         Object funcResult_5 = null;
/* 095 */         funcResult_5 = value_12.toString();
/* 096 */         value_11 = (java.lang.String) funcResult_5;
/* 097 */
/* 098 */       }
/* 099 */     }
/* 100 */     if (isNull_11) {
/* 101 */       values_0[5] = null;
/* 102 */     } else {
/* 103 */       values_0[5] = value_11;
/* 104 */     }
/* 105 */
/* 106 */   }
/* 107 */
/* 108 */
/* 109 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 110 */
/* 111 */     boolean isNull_2 = i.isNullAt(0);
/* 112 */     UTF8String value_2 = isNull_2 ?
/* 113 */     null : (i.getUTF8String(0));
/* 114 */     boolean isNull_1 = true;
/* 115 */     java.lang.String value_1 = null;
/* 116 */     if (!isNull_2) {
/* 117 */
/* 118 */       isNull_1 = false;
/* 119 */       if (!isNull_1) {
/* 120 */
/* 121 */         Object funcResult_0 = null;
/* 122 */         funcResult_0 = value_2.toString();
/* 123 */         value_1 = (java.lang.String) funcResult_0;
/* 124 */
/* 125 */       }
/* 126 */     }
/* 127 */     if (isNull_1) {
/* 128 */       values_0[0] = null;
/* 129 */     } else {
/* 130 */       values_0[0] = value_1;
/* 131 */     }
/* 132 */
/* 133 */     boolean isNull_4 = i.isNullAt(1);
/* 134 */     UTF8String value_4 = isNull_4 ?
/* 135 */     null : (i.getUTF8String(1));
/* 136 */     boolean isNull_3 = true;
/* 137 */     java.lang.String value_3 = null;
/* 138 */     if (!isNull_4) {
/* 139 */
/* 140 */       isNull_3 = false;
/* 141 */       if (!isNull_3) {
/* 142 */
/* 143 */         Object funcResult_1 = null;
/* 144 */         funcResult_1 = value_4.toString();
/* 145 */         value_3 = (java.lang.String) funcResult_1;
/* 146 */
/* 147 */       }
/* 148 */     }
/* 149 */     if (isNull_3) {
/* 150 */       values_0[1] = null;
/* 151 */     } else {
/* 152 */       values_0[1] = value_3;
/* 153 */     }
/* 154 */
/* 155 */     boolean isNull_6 = i.isNullAt(2);
/* 156 */     UTF8String value_6 = isNull_6 ?
/* 157 */     null : (i.getUTF8String(2));
/* 158 */     boolean isNull_5 = true;
/* 159 */     java.lang.String value_5 = null;
/* 160 */     if (!isNull_6) {
/* 161 */
/* 162 */       isNull_5 = false;
/* 163 */       if (!isNull_5) {
/* 164 */
/* 165 */         Object funcResult_2 = null;
/* 166 */         funcResult_2 = value_6.toString();
/* 167 */         value_5 = (java.lang.String) funcResult_2;
/* 168 */
/* 169 */       }
/* 170 */     }
/* 171 */     if (isNull_5) {
/* 172 */       values_0[2] = null;
/* 173 */     } else {
/* 174 */       values_0[2] = value_5;
/* 175 */     }
/* 176 */
/* 177 */   }
/* 178 */
/* 179 */ }

[INFO] 2021-07-19 16:59:29,709 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 46.92155 ms
[INFO] 2021-07-19 16:59:29,711 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data1/aapl.txt/part-00000-4f441212-b8c4-4edf-b2fe-f60ce24a157f-c000.csv, range: 0-445012, partition values: [empty row]
[DEBUG] 2021-07-19 16:59:29,722 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */   }
/* 057 */
/* 058 */
/* 059 */   private void writeFields_0_0(InternalRow i) {
/* 060 */
/* 061 */     boolean isNull_0 = i.isNullAt(0);
/* 062 */     UTF8String value_0 = isNull_0 ?
/* 063 */     null : (i.getUTF8String(0));
/* 064 */     if (isNull_0) {
/* 065 */       mutableStateArray_0[0].setNullAt(0);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(0, value_0);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_1 = i.isNullAt(1);
/* 071 */     UTF8String value_1 = isNull_1 ?
/* 072 */     null : (i.getUTF8String(1));
/* 073 */     if (isNull_1) {
/* 074 */       mutableStateArray_0[0].setNullAt(1);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(1, value_1);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_2 = i.isNullAt(2);
/* 080 */     UTF8String value_2 = isNull_2 ?
/* 081 */     null : (i.getUTF8String(2));
/* 082 */     if (isNull_2) {
/* 083 */       mutableStateArray_0[0].setNullAt(2);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(2, value_2);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_3 = i.isNullAt(3);
/* 089 */     UTF8String value_3 = isNull_3 ?
/* 090 */     null : (i.getUTF8String(3));
/* 091 */     if (isNull_3) {
/* 092 */       mutableStateArray_0[0].setNullAt(3);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(3, value_3);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_4 = i.isNullAt(4);
/* 098 */     UTF8String value_4 = isNull_4 ?
/* 099 */     null : (i.getUTF8String(4));
/* 100 */     if (isNull_4) {
/* 101 */       mutableStateArray_0[0].setNullAt(4);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(4, value_4);
/* 104 */     }
/* 105 */
/* 106 */   }
/* 107 */
/* 108 */ }

[DEBUG] 2021-07-19 16:59:29,742 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */   }
/* 057 */
/* 058 */
/* 059 */   private void writeFields_0_0(InternalRow i) {
/* 060 */
/* 061 */     boolean isNull_0 = i.isNullAt(0);
/* 062 */     UTF8String value_0 = isNull_0 ?
/* 063 */     null : (i.getUTF8String(0));
/* 064 */     if (isNull_0) {
/* 065 */       mutableStateArray_0[0].setNullAt(0);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(0, value_0);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_1 = i.isNullAt(1);
/* 071 */     UTF8String value_1 = isNull_1 ?
/* 072 */     null : (i.getUTF8String(1));
/* 073 */     if (isNull_1) {
/* 074 */       mutableStateArray_0[0].setNullAt(1);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(1, value_1);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_2 = i.isNullAt(2);
/* 080 */     UTF8String value_2 = isNull_2 ?
/* 081 */     null : (i.getUTF8String(2));
/* 082 */     if (isNull_2) {
/* 083 */       mutableStateArray_0[0].setNullAt(2);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(2, value_2);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_3 = i.isNullAt(3);
/* 089 */     UTF8String value_3 = isNull_3 ?
/* 090 */     null : (i.getUTF8String(3));
/* 091 */     if (isNull_3) {
/* 092 */       mutableStateArray_0[0].setNullAt(3);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(3, value_3);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_4 = i.isNullAt(4);
/* 098 */     UTF8String value_4 = isNull_4 ?
/* 099 */     null : (i.getUTF8String(4));
/* 100 */     if (isNull_4) {
/* 101 */       mutableStateArray_0[0].setNullAt(4);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(4, value_4);
/* 104 */     }
/* 105 */
/* 106 */   }
/* 107 */
/* 108 */ }

[INFO] 2021-07-19 16:59:29,788 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 65.539851 ms
[DEBUG] 2021-07-19 16:59:29,790 org.apache.spark.storage.BlockManager - Getting local block broadcast_3
[DEBUG] 2021-07-19 16:59:29,790 org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 16:59:29,821 org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate - Generated predicate '(isnotnull(input[6, string, true]) AND (input[6, string, true] = aapl))':
/* 001 */ public SpecificPredicate generate(Object[] references) {
/* 002 */   return new SpecificPredicate(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {
/* 006 */   private final Object[] references;
/* 007 */
/* 008 */
/* 009 */   public SpecificPredicate(Object[] references) {
/* 010 */     this.references = references;
/* 011 */
/* 012 */   }
/* 013 */
/* 014 */   public void initialize(int partitionIndex) {
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public boolean eval(InternalRow i) {
/* 019 */
/* 020 */     boolean isNull_2 = i.isNullAt(6);
/* 021 */     UTF8String value_2 = isNull_2 ?
/* 022 */     null : (i.getUTF8String(6));
/* 023 */     boolean value_3 = !isNull_2;
/* 024 */     boolean isNull_0 = false;
/* 025 */     boolean value_0 = false;
/* 026 */
/* 027 */     if (!false && !value_3) {
/* 028 */     } else {
/* 029 */       boolean isNull_3 = true;
/* 030 */       boolean value_4 = false;
/* 031 */       boolean isNull_4 = i.isNullAt(6);
/* 032 */       UTF8String value_5 = isNull_4 ?
/* 033 */       null : (i.getUTF8String(6));
/* 034 */       if (!isNull_4) {
/* 035 */
/* 036 */
/* 037 */         isNull_3 = false; // resultCode could change nullability.
/* 038 */         value_4 = value_5.equals(((UTF8String) references[0] /* literal */));
/* 039 */
/* 040 */       }
/* 041 */       if (!isNull_3 && !value_4) {
/* 042 */       } else if (!false && !isNull_3) {
/* 043 */         value_0 = true;
/* 044 */       } else {
/* 045 */         isNull_0 = true;
/* 046 */       }
/* 047 */     }
/* 048 */     return !isNull_0 && value_0;
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

[DEBUG] 2021-07-19 16:59:29,825 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public SpecificPredicate generate(Object[] references) {
/* 002 */   return new SpecificPredicate(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {
/* 006 */   private final Object[] references;
/* 007 */
/* 008 */
/* 009 */   public SpecificPredicate(Object[] references) {
/* 010 */     this.references = references;
/* 011 */
/* 012 */   }
/* 013 */
/* 014 */   public void initialize(int partitionIndex) {
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public boolean eval(InternalRow i) {
/* 019 */
/* 020 */     boolean isNull_2 = i.isNullAt(6);
/* 021 */     UTF8String value_2 = isNull_2 ?
/* 022 */     null : (i.getUTF8String(6));
/* 023 */     boolean value_3 = !isNull_2;
/* 024 */     boolean isNull_0 = false;
/* 025 */     boolean value_0 = false;
/* 026 */
/* 027 */     if (!false && !value_3) {
/* 028 */     } else {
/* 029 */       boolean isNull_3 = true;
/* 030 */       boolean value_4 = false;
/* 031 */       boolean isNull_4 = i.isNullAt(6);
/* 032 */       UTF8String value_5 = isNull_4 ?
/* 033 */       null : (i.getUTF8String(6));
/* 034 */       if (!isNull_4) {
/* 035 */
/* 036 */
/* 037 */         isNull_3 = false; // resultCode could change nullability.
/* 038 */         value_4 = value_5.equals(((UTF8String) references[0] /* literal */));
/* 039 */
/* 040 */       }
/* 041 */       if (!isNull_3 && !value_4) {
/* 042 */       } else if (!false && !isNull_3) {
/* 043 */         value_0 = true;
/* 044 */       } else {
/* 045 */         isNull_0 = true;
/* 046 */       }
/* 047 */     }
/* 048 */     return !isNull_0 && value_0;
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

[INFO] 2021-07-19 16:59:29,837 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.661443 ms
[INFO] 2021-07-19 16:59:58,672 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 16:59:58,686 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 16:59:58,692 org.apache.spark.scheduler.DAGScheduler - Job 1 failed: foreach at Simulator.scala:40, took 29.250522 s
[INFO] 2021-07-19 16:59:58,692 org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (foreach at Simulator.scala:40) failed in 29.229 s due to Stage cancelled because SparkContext was shut down
[INFO] 2021-07-19 16:59:58,718 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 16:59:58,743 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 16:59:58,743 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 16:59:58,751 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 16:59:58,754 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 16:59:58,761 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 16:59:58,761 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 16:59:58,762 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-69dcf24a-6fa7-4dd0-ac9b-a5a9f0a006e4
[WARN] 2021-07-19 17:00:48,810 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:00:48,812 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 17:00:49,103 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 17:00:49,894 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 17:00:49,896 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 17:00:49,905 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 17:00:49,906 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 17:00:49,968 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 17:00:50,012 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 17:00:50,013 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 17:00:50,171 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 17:00:50,173 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 17:00:50,177 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 17:00:50,184 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 17:00:50,186 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[DEBUG] 2021-07-19 17:00:50,893 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 34407
[INFO] 2021-07-19 17:00:50,973 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34407.
[DEBUG] 2021-07-19 17:00:50,975 org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
[INFO] 2021-07-19 17:00:51,153 org.apache.spark.SparkEnv - Registering MapOutputTracker
[DEBUG] 2021-07-19 17:00:51,154 org.apache.spark.MapOutputTrackerMasterEndpoint - init
[INFO] 2021-07-19 17:00:51,313 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 17:00:51,378 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 17:00:51,396 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 17:00:51,408 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 17:00:51,487 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b9669a5b-266a-4663-a3ae-697c8ead906a
[DEBUG] 2021-07-19 17:00:51,489 org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
[DEBUG] 2021-07-19 17:00:51,492 org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
[INFO] 2021-07-19 17:00:51,648 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 17:00:51,699 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[DEBUG] 2021-07-19 17:00:51,704 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
[DEBUG] 2021-07-19 17:00:51,773 org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[DEBUG] 2021-07-19 17:00:52,026 org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
[INFO] 2021-07-19 17:00:52,067 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 17:00:52,146 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 17:00:52,444 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[DEBUG] 2021-07-19 17:00:52,482 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 46767
[INFO] 2021-07-19 17:00:52,482 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46767.
[INFO] 2021-07-19 17:00:52,483 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:46767
[INFO] 2021-07-19 17:00:52,485 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 17:00:52,496 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 46767, None)
[DEBUG] 2021-07-19 17:00:52,500 org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.1.29
[INFO] 2021-07-19 17:00:52,502 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:46767 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:00:52,507 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:00:52,509 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 46767, None)
[DEBUG] 2021-07-19 17:00:52,890 org.apache.spark.SparkContext - Adding shutdown hook
[INFO] 2021-07-19 17:00:53,121 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 17:00:53,122 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[DEBUG] 2021-07-19 17:00:53,135 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> test
[DEBUG] 2021-07-19 17:00:53,138 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local[*]
[DEBUG] 2021-07-19 17:00:54,181 org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:
  
[INFO] 2021-07-19 17:00:54,257 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 60 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 17:00:54,390 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 5 ms to list leaf files for 1 paths.
[DEBUG] 2021-07-19 17:00:56,534 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#0
[DEBUG] 2021-07-19 17:00:56,826 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#6
[INFO] 2021-07-19 17:00:57,301 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 17:00:57,307 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
[INFO] 2021-07-19 17:00:57,312 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[DEBUG] 2021-07-19 17:00:57,918 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[DEBUG] 2021-07-19 17:00:57,983 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[INFO] 2021-07-19 17:00:58,358 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 422.095954 ms
[INFO] 2021-07-19 17:00:58,435 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 174.2 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 17:00:58,437 org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 53 ms
[DEBUG] 2021-07-19 17:00:58,452 org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 67 ms
[INFO] 2021-07-19 17:00:58,523 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 17:00:58,525 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:00:58,527 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.29:46767 (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 17:00:58,528 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 17:00:58,529 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 17:00:58,530 org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 9 ms
[DEBUG] 2021-07-19 17:00:58,530 org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 9 ms
[INFO] 2021-07-19 17:00:58,533 org.apache.spark.SparkContext - Created broadcast 0 from csv at Simulator.scala:34
[INFO] 2021-07-19 17:00:58,544 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 17:00:58,632 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 17:00:58,657 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 17:00:58,716 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
[DEBUG] 2021-07-19 17:00:58,720 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
[DEBUG] 2021-07-19 17:00:58,724 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 17:00:58,760 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 17:00:58,764 org.apache.spark.SparkContext - Starting job: csv at Simulator.scala:34
[DEBUG] 2021-07-19 17:00:58,775 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 17:00:58,798 org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at Simulator.scala:34) with 1 output partitions
[INFO] 2021-07-19 17:00:58,799 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at Simulator.scala:34)
[INFO] 2021-07-19 17:00:58,802 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 17:00:58,804 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 17:00:58,807 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=csv at Simulator.scala:34;jobs=0))
[DEBUG] 2021-07-19 17:00:58,809 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 17:00:58,812 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:34), which has no missing parents
[DEBUG] 2021-07-19 17:00:58,814 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
[INFO] 2021-07-19 17:00:58,946 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 17:00:58,946 org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 2 ms
[DEBUG] 2021-07-19 17:00:58,947 org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 2 ms
[INFO] 2021-07-19 17:00:58,950 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 17:00:58,950 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:00:58,952 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.29:46767 (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 17:00:58,952 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 17:00:58,952 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 17:00:58,953 org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 3 ms
[DEBUG] 2021-07-19 17:00:58,953 org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 4 ms
[INFO] 2021-07-19 17:00:58,953 org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 17:00:58,976 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:34) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 17:00:58,977 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 17:00:58,996 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
[DEBUG] 2021-07-19 17:00:59,000 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 3 ms
[DEBUG] 2021-07-19 17:00:59,002 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[DEBUG] 2021-07-19 17:00:59,021 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
[DEBUG] 2021-07-19 17:00:59,022 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[INFO] 2021-07-19 17:00:59,049 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4941 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 17:00:59,064 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 17:00:59,077 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[DEBUG] 2021-07-19 17:00:59,090 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
[DEBUG] 2021-07-19 17:00:59,123 org.apache.spark.storage.BlockManager - Getting local block broadcast_1
[DEBUG] 2021-07-19 17:00:59,125 org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 17:00:59,262 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data1/aapl.txt/part-00000-4f441212-b8c4-4edf-b2fe-f60ce24a157f-c000.csv, range: 0-445012, partition values: [empty row]
[DEBUG] 2021-07-19 17:00:59,275 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[DEBUG] 2021-07-19 17:00:59,282 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[INFO] 2021-07-19 17:00:59,307 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.068827 ms
[DEBUG] 2021-07-19 17:00:59,308 org.apache.spark.storage.BlockManager - Getting local block broadcast_0
[DEBUG] 2021-07-19 17:00:59,309 org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 17:00:59,354 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1556 bytes result sent to driver
[DEBUG] 2021-07-19 17:00:59,355 org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
[INFO] 2021-07-19 17:00:59,362 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 328 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 17:00:59,365 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 17:00:59,386 org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at Simulator.scala:34) finished in 0.550 s
[DEBUG] 2021-07-19 17:00:59,392 org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
[INFO] 2021-07-19 17:00:59,396 org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 17:00:59,399 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
[INFO] 2021-07-19 17:00:59,403 org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at Simulator.scala:34, took 0.637998 s
[DEBUG] 2021-07-19 17:00:59,442 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[DEBUG] 2021-07-19 17:00:59,458 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[DEBUG] 2021-07-19 17:00:59,556 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(33)
[INFO] 2021-07-19 17:00:59,560 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 117.248757 ms
[DEBUG] 2021-07-19 17:00:59,561 org.apache.spark.ContextCleaner - Cleaning accumulator 33
[DEBUG] 2021-07-19 17:00:59,574 org.apache.spark.ContextCleaner - Cleaned accumulator 33
[DEBUG] 2021-07-19 17:00:59,574 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(17)
[DEBUG] 2021-07-19 17:00:59,574 org.apache.spark.ContextCleaner - Cleaning accumulator 17
[DEBUG] 2021-07-19 17:00:59,574 org.apache.spark.ContextCleaner - Cleaned accumulator 17
[DEBUG] 2021-07-19 17:00:59,574 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(25)
[DEBUG] 2021-07-19 17:00:59,575 org.apache.spark.ContextCleaner - Cleaning accumulator 25
[DEBUG] 2021-07-19 17:00:59,575 org.apache.spark.ContextCleaner - Cleaned accumulator 25
[DEBUG] 2021-07-19 17:00:59,575 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(16)
[DEBUG] 2021-07-19 17:00:59,575 org.apache.spark.ContextCleaner - Cleaning accumulator 16
[DEBUG] 2021-07-19 17:00:59,575 org.apache.spark.ContextCleaner - Cleaned accumulator 16
[DEBUG] 2021-07-19 17:00:59,575 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(30)
[DEBUG] 2021-07-19 17:00:59,575 org.apache.spark.ContextCleaner - Cleaning accumulator 30
[DEBUG] 2021-07-19 17:00:59,576 org.apache.spark.ContextCleaner - Cleaned accumulator 30
[DEBUG] 2021-07-19 17:00:59,576 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(21)
[DEBUG] 2021-07-19 17:00:59,576 org.apache.spark.ContextCleaner - Cleaning accumulator 21
[DEBUG] 2021-07-19 17:00:59,576 org.apache.spark.ContextCleaner - Cleaned accumulator 21
[DEBUG] 2021-07-19 17:00:59,576 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(35)
[DEBUG] 2021-07-19 17:00:59,576 org.apache.spark.ContextCleaner - Cleaning accumulator 35
[DEBUG] 2021-07-19 17:00:59,576 org.apache.spark.ContextCleaner - Cleaned accumulator 35
[DEBUG] 2021-07-19 17:00:59,576 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(22)
[DEBUG] 2021-07-19 17:00:59,577 org.apache.spark.ContextCleaner - Cleaning accumulator 22
[DEBUG] 2021-07-19 17:00:59,577 org.apache.spark.ContextCleaner - Cleaned accumulator 22
[DEBUG] 2021-07-19 17:00:59,577 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(36)
[DEBUG] 2021-07-19 17:00:59,577 org.apache.spark.ContextCleaner - Cleaning accumulator 36
[DEBUG] 2021-07-19 17:00:59,578 org.apache.spark.ContextCleaner - Cleaned accumulator 36
[DEBUG] 2021-07-19 17:00:59,578 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(1)
[DEBUG] 2021-07-19 17:00:59,579 org.apache.spark.ContextCleaner - Cleaning broadcast 1
[DEBUG] 2021-07-19 17:00:59,580 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
[DEBUG] 2021-07-19 17:00:59,606 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
[DEBUG] 2021-07-19 17:00:59,607 org.apache.spark.storage.BlockManager - Removing broadcast 1
[DEBUG] 2021-07-19 17:00:59,609 org.apache.spark.storage.BlockManager - Removing block broadcast_1
[DEBUG] 2021-07-19 17:00:59,610 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 11088 dropped from memory (free 1044169693)
[DEBUG] 2021-07-19 17:00:59,611 org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
[DEBUG] 2021-07-19 17:00:59,612 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 5515 dropped from memory (free 1044175208)
[DEBUG] 2021-07-19 17:00:59,613 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:00:59,615 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.1.29:46767 in memory (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 17:00:59,616 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 17:00:59,616 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 17:00:59,624 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
[DEBUG] 2021-07-19 17:00:59,626 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:34407
[DEBUG] 2021-07-19 17:00:59,628 org.apache.spark.ContextCleaner - Cleaned broadcast 1
[DEBUG] 2021-07-19 17:00:59,628 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(29)
[DEBUG] 2021-07-19 17:00:59,628 org.apache.spark.ContextCleaner - Cleaning accumulator 29
[DEBUG] 2021-07-19 17:00:59,628 org.apache.spark.ContextCleaner - Cleaned accumulator 29
[DEBUG] 2021-07-19 17:00:59,628 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(34)
[DEBUG] 2021-07-19 17:00:59,629 org.apache.spark.ContextCleaner - Cleaning accumulator 34
[DEBUG] 2021-07-19 17:00:59,629 org.apache.spark.ContextCleaner - Cleaned accumulator 34
[DEBUG] 2021-07-19 17:00:59,630 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(38)
[DEBUG] 2021-07-19 17:00:59,631 org.apache.spark.ContextCleaner - Cleaning accumulator 38
[DEBUG] 2021-07-19 17:00:59,631 org.apache.spark.ContextCleaner - Cleaned accumulator 38
[DEBUG] 2021-07-19 17:00:59,631 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(28)
[DEBUG] 2021-07-19 17:00:59,631 org.apache.spark.ContextCleaner - Cleaning accumulator 28
[DEBUG] 2021-07-19 17:00:59,631 org.apache.spark.ContextCleaner - Cleaned accumulator 28
[DEBUG] 2021-07-19 17:00:59,631 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(31)
[DEBUG] 2021-07-19 17:00:59,631 org.apache.spark.ContextCleaner - Cleaning accumulator 31
[DEBUG] 2021-07-19 17:00:59,631 org.apache.spark.ContextCleaner - Cleaned accumulator 31
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(32)
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Cleaning accumulator 32
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Cleaned accumulator 32
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(40)
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Cleaning accumulator 40
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Cleaned accumulator 40
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(18)
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Cleaning accumulator 18
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Cleaned accumulator 18
[DEBUG] 2021-07-19 17:00:59,632 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(23)
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Cleaning accumulator 23
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Cleaned accumulator 23
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(24)
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Cleaning accumulator 24
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Cleaned accumulator 24
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(37)
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Cleaning accumulator 37
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Cleaned accumulator 37
[DEBUG] 2021-07-19 17:00:59,633 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(39)
[DEBUG] 2021-07-19 17:00:59,634 org.apache.spark.ContextCleaner - Cleaning accumulator 39
[DEBUG] 2021-07-19 17:00:59,634 org.apache.spark.ContextCleaner - Cleaned accumulator 39
[DEBUG] 2021-07-19 17:00:59,634 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(20)
[DEBUG] 2021-07-19 17:00:59,634 org.apache.spark.ContextCleaner - Cleaning accumulator 20
[DEBUG] 2021-07-19 17:00:59,634 org.apache.spark.ContextCleaner - Cleaned accumulator 20
[DEBUG] 2021-07-19 17:00:59,634 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(19)
[DEBUG] 2021-07-19 17:00:59,634 org.apache.spark.ContextCleaner - Cleaning accumulator 19
[DEBUG] 2021-07-19 17:00:59,635 org.apache.spark.ContextCleaner - Cleaned accumulator 19
[DEBUG] 2021-07-19 17:00:59,635 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(27)
[DEBUG] 2021-07-19 17:00:59,635 org.apache.spark.ContextCleaner - Cleaning accumulator 27
[DEBUG] 2021-07-19 17:00:59,635 org.apache.spark.ContextCleaner - Cleaned accumulator 27
[DEBUG] 2021-07-19 17:00:59,635 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(26)
[DEBUG] 2021-07-19 17:00:59,635 org.apache.spark.ContextCleaner - Cleaning accumulator 26
[DEBUG] 2021-07-19 17:00:59,635 org.apache.spark.ContextCleaner - Cleaned accumulator 26
[INFO] 2021-07-19 17:00:59,699 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 17:00:59,700 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 17:00:59,700 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 17:00:59,715 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 174.2 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 17:00:59,716 org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 9 ms
[DEBUG] 2021-07-19 17:00:59,716 org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 9 ms
[INFO] 2021-07-19 17:00:59,743 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 17:00:59,744 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:00:59,744 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.29:46767 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 17:00:59,745 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 17:00:59,745 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[DEBUG] 2021-07-19 17:00:59,745 org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 3 ms
[DEBUG] 2021-07-19 17:00:59,745 org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 4 ms
[INFO] 2021-07-19 17:00:59,746 org.apache.spark.SparkContext - Created broadcast 2 from csv at Simulator.scala:34
[INFO] 2021-07-19 17:00:59,748 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 17:00:59,755 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 17:00:59,770 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 17:00:59,796 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inferFromDataset$2
[DEBUG] 2021-07-19 17:00:59,797 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
[DEBUG] 2021-07-19 17:00:59,887 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'ticket to ticket#36
[INFO] 2021-07-19 17:00:59,991 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(Ticket),EqualTo(Ticket,aapl)
[INFO] 2021-07-19 17:00:59,991 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(Ticket#22),(Ticket#22 = aapl)
[INFO] 2021-07-19 17:00:59,992 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<date: string, open: string, high: string, low: string, close: string ... 5 more fields>
[DEBUG] 2021-07-19 17:01:00,095 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while ( inputadapter_input_0.hasNext()) {
/* 027 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 028 */
/* 029 */       do {
/* 030 */         boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);
/* 031 */         UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?
/* 032 */         null : (inputadapter_row_0.getUTF8String(6));
/* 033 */
/* 034 */         boolean filter_value_2 = !inputadapter_isNull_6;
/* 035 */         if (!filter_value_2) continue;
/* 036 */
/* 037 */         boolean filter_value_3 = false;
/* 038 */         filter_value_3 = inputadapter_value_6.equals(((UTF8String) references[1] /* literal */));
/* 039 */         if (!filter_value_3) continue;
/* 040 */
/* 041 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 042 */
/* 043 */         // common sub-expressions
/* 044 */
/* 045 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 046 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 047 */         null : (inputadapter_row_0.getUTF8String(0));
/* 048 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 049 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 050 */         null : (inputadapter_row_0.getUTF8String(1));
/* 051 */         boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 052 */         UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 053 */         null : (inputadapter_row_0.getUTF8String(2));
/* 054 */         boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 055 */         UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 056 */         null : (inputadapter_row_0.getUTF8String(3));
/* 057 */         boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 058 */         UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 059 */         null : (inputadapter_row_0.getUTF8String(4));
/* 060 */         boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 061 */         UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 062 */         null : (inputadapter_row_0.getUTF8String(5));
/* 063 */         filter_mutableStateArray_0[1].reset();
/* 064 */
/* 065 */         filter_mutableStateArray_0[1].zeroOutNullBytes();
/* 066 */
/* 067 */         if (inputadapter_isNull_0) {
/* 068 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 069 */         } else {
/* 070 */           filter_mutableStateArray_0[1].write(0, inputadapter_value_0);
/* 071 */         }
/* 072 */
/* 073 */         if (inputadapter_isNull_1) {
/* 074 */           filter_mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           filter_mutableStateArray_0[1].write(1, inputadapter_value_1);
/* 077 */         }
/* 078 */
/* 079 */         if (inputadapter_isNull_2) {
/* 080 */           filter_mutableStateArray_0[1].setNullAt(2);
/* 081 */         } else {
/* 082 */           filter_mutableStateArray_0[1].write(2, inputadapter_value_2);
/* 083 */         }
/* 084 */
/* 085 */         if (inputadapter_isNull_3) {
/* 086 */           filter_mutableStateArray_0[1].setNullAt(3);
/* 087 */         } else {
/* 088 */           filter_mutableStateArray_0[1].write(3, inputadapter_value_3);
/* 089 */         }
/* 090 */
/* 091 */         if (inputadapter_isNull_4) {
/* 092 */           filter_mutableStateArray_0[1].setNullAt(4);
/* 093 */         } else {
/* 094 */           filter_mutableStateArray_0[1].write(4, inputadapter_value_4);
/* 095 */         }
/* 096 */
/* 097 */         if (inputadapter_isNull_5) {
/* 098 */           filter_mutableStateArray_0[1].setNullAt(5);
/* 099 */         } else {
/* 100 */           filter_mutableStateArray_0[1].write(5, inputadapter_value_5);
/* 101 */         }
/* 102 */
/* 103 */         if (false) {
/* 104 */           filter_mutableStateArray_0[1].setNullAt(6);
/* 105 */         } else {
/* 106 */           filter_mutableStateArray_0[1].write(6, inputadapter_value_6);
/* 107 */         }
/* 108 */         append((filter_mutableStateArray_0[1].getRow()));
/* 109 */
/* 110 */       } while(false);
/* 111 */       if (shouldStop()) return;
/* 112 */     }
/* 113 */   }
/* 114 */
/* 115 */ }

[DEBUG] 2021-07-19 17:01:00,110 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 022 */
/* 023 */   }
/* 024 */
/* 025 */   protected void processNext() throws java.io.IOException {
/* 026 */     while ( inputadapter_input_0.hasNext()) {
/* 027 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 028 */
/* 029 */       do {
/* 030 */         boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);
/* 031 */         UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?
/* 032 */         null : (inputadapter_row_0.getUTF8String(6));
/* 033 */
/* 034 */         boolean filter_value_2 = !inputadapter_isNull_6;
/* 035 */         if (!filter_value_2) continue;
/* 036 */
/* 037 */         boolean filter_value_3 = false;
/* 038 */         filter_value_3 = inputadapter_value_6.equals(((UTF8String) references[1] /* literal */));
/* 039 */         if (!filter_value_3) continue;
/* 040 */
/* 041 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 042 */
/* 043 */         // common sub-expressions
/* 044 */
/* 045 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 046 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 047 */         null : (inputadapter_row_0.getUTF8String(0));
/* 048 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 049 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 050 */         null : (inputadapter_row_0.getUTF8String(1));
/* 051 */         boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 052 */         UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 053 */         null : (inputadapter_row_0.getUTF8String(2));
/* 054 */         boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 055 */         UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 056 */         null : (inputadapter_row_0.getUTF8String(3));
/* 057 */         boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 058 */         UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 059 */         null : (inputadapter_row_0.getUTF8String(4));
/* 060 */         boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 061 */         UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 062 */         null : (inputadapter_row_0.getUTF8String(5));
/* 063 */         filter_mutableStateArray_0[1].reset();
/* 064 */
/* 065 */         filter_mutableStateArray_0[1].zeroOutNullBytes();
/* 066 */
/* 067 */         if (inputadapter_isNull_0) {
/* 068 */           filter_mutableStateArray_0[1].setNullAt(0);
/* 069 */         } else {
/* 070 */           filter_mutableStateArray_0[1].write(0, inputadapter_value_0);
/* 071 */         }
/* 072 */
/* 073 */         if (inputadapter_isNull_1) {
/* 074 */           filter_mutableStateArray_0[1].setNullAt(1);
/* 075 */         } else {
/* 076 */           filter_mutableStateArray_0[1].write(1, inputadapter_value_1);
/* 077 */         }
/* 078 */
/* 079 */         if (inputadapter_isNull_2) {
/* 080 */           filter_mutableStateArray_0[1].setNullAt(2);
/* 081 */         } else {
/* 082 */           filter_mutableStateArray_0[1].write(2, inputadapter_value_2);
/* 083 */         }
/* 084 */
/* 085 */         if (inputadapter_isNull_3) {
/* 086 */           filter_mutableStateArray_0[1].setNullAt(3);
/* 087 */         } else {
/* 088 */           filter_mutableStateArray_0[1].write(3, inputadapter_value_3);
/* 089 */         }
/* 090 */
/* 091 */         if (inputadapter_isNull_4) {
/* 092 */           filter_mutableStateArray_0[1].setNullAt(4);
/* 093 */         } else {
/* 094 */           filter_mutableStateArray_0[1].write(4, inputadapter_value_4);
/* 095 */         }
/* 096 */
/* 097 */         if (inputadapter_isNull_5) {
/* 098 */           filter_mutableStateArray_0[1].setNullAt(5);
/* 099 */         } else {
/* 100 */           filter_mutableStateArray_0[1].write(5, inputadapter_value_5);
/* 101 */         }
/* 102 */
/* 103 */         if (false) {
/* 104 */           filter_mutableStateArray_0[1].setNullAt(6);
/* 105 */         } else {
/* 106 */           filter_mutableStateArray_0[1].write(6, inputadapter_value_6);
/* 107 */         }
/* 108 */         append((filter_mutableStateArray_0[1].getRow()));
/* 109 */
/* 110 */       } while(false);
/* 111 */       if (shouldStop()) return;
/* 112 */     }
/* 113 */   }
/* 114 */
/* 115 */ }

[INFO] 2021-07-19 17:01:00,141 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 45.297951 ms
[INFO] 2021-07-19 17:01:00,148 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 174.1 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 17:01:00,149 org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 5 ms
[DEBUG] 2021-07-19 17:01:00,149 org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 5 ms
[INFO] 2021-07-19 17:01:00,189 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 17:01:00,189 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:01:00,190 org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.29:46767 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 17:01:00,190 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
[DEBUG] 2021-07-19 17:01:00,190 org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
[DEBUG] 2021-07-19 17:01:00,190 org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 2 ms
[DEBUG] 2021-07-19 17:01:00,190 org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 2 ms
[INFO] 2021-07-19 17:01:00,191 org.apache.spark.SparkContext - Created broadcast 3 from foreach at Simulator.scala:38
[INFO] 2021-07-19 17:01:00,195 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 17:01:00,200 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 17:01:00,202 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 17:01:00,213 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 17:01:00,223 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 17:01:00,228 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$start$1$adapted
[DEBUG] 2021-07-19 17:01:00,228 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$start$1$adapted) is now cleaned +++
[DEBUG] 2021-07-19 17:01:00,230 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$foreach$2$adapted
[DEBUG] 2021-07-19 17:01:00,256 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$foreach$2$adapted) is now cleaned +++
[DEBUG] 2021-07-19 17:01:00,258 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 17:01:00,271 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 17:01:00,273 org.apache.spark.SparkContext - Starting job: foreach at Simulator.scala:38
[DEBUG] 2021-07-19 17:01:00,274 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 17:01:00,275 org.apache.spark.scheduler.DAGScheduler - Got job 1 (foreach at Simulator.scala:38) with 1 output partitions
[INFO] 2021-07-19 17:01:00,275 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (foreach at Simulator.scala:38)
[INFO] 2021-07-19 17:01:00,276 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 17:01:00,276 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 17:01:00,277 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 1 (name=foreach at Simulator.scala:38;jobs=1))
[DEBUG] 2021-07-19 17:01:00,277 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 17:01:00,278 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[15] at foreach at Simulator.scala:38), which has no missing parents
[DEBUG] 2021-07-19 17:01:00,282 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 1)
[INFO] 2021-07-19 17:01:00,376 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 20.8 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 17:01:00,376 org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 1 ms
[DEBUG] 2021-07-19 17:01:00,377 org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 2 ms
[DEBUG] 2021-07-19 17:01:00,392 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(44)
[DEBUG] 2021-07-19 17:01:00,392 org.apache.spark.ContextCleaner - Cleaning accumulator 44
[DEBUG] 2021-07-19 17:01:00,392 org.apache.spark.ContextCleaner - Cleaned accumulator 44
[DEBUG] 2021-07-19 17:01:00,392 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(2)
[DEBUG] 2021-07-19 17:01:00,392 org.apache.spark.ContextCleaner - Cleaning broadcast 2
[DEBUG] 2021-07-19 17:01:00,392 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
[INFO] 2021-07-19 17:01:00,393 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 17:01:00,394 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.1.29, 46767, None)
[DEBUG] 2021-07-19 17:01:00,394 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 2
[DEBUG] 2021-07-19 17:01:00,394 org.apache.spark.storage.BlockManager - Removing broadcast 2
[DEBUG] 2021-07-19 17:01:00,394 org.apache.spark.storage.BlockManager - Removing block broadcast_2
[DEBUG] 2021-07-19 17:01:00,394 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 178344 dropped from memory (free 1043909376)
[INFO] 2021-07-19 17:01:00,394 org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.29:46767 (size: 9.7 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 17:01:00,395 org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
[DEBUG] 2021-07-19 17:01:00,395 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 28144 dropped from memory (free 1043937520)
[DEBUG] 2021-07-19 17:01:00,395 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
[DEBUG] 2021-07-19 17:01:00,395 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:01:00,396 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.1.29:46767 in memory (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 17:01:00,397 org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
[DEBUG] 2021-07-19 17:01:00,397 org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 5 ms
[DEBUG] 2021-07-19 17:01:00,398 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 17:01:00,398 org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 5 ms
[DEBUG] 2021-07-19 17:01:00,398 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[INFO] 2021-07-19 17:01:00,398 org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1388
[DEBUG] 2021-07-19 17:01:00,411 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 2, response is 0
[DEBUG] 2021-07-19 17:01:00,422 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:34407
[INFO] 2021-07-19 17:01:00,424 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at foreach at Simulator.scala:38) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 17:01:00,424 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 17:01:00,426 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
[DEBUG] 2021-07-19 17:01:00,427 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Cleaned broadcast 2
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(42)
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Cleaning accumulator 42
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Cleaned accumulator 42
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(41)
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Cleaning accumulator 41
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Cleaned accumulator 41
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(43)
[DEBUG] 2021-07-19 17:01:00,430 org.apache.spark.ContextCleaner - Cleaning accumulator 43
[DEBUG] 2021-07-19 17:01:00,431 org.apache.spark.ContextCleaner - Cleaned accumulator 43
[DEBUG] 2021-07-19 17:01:00,440 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
[INFO] 2021-07-19 17:01:00,441 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4941 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 17:01:00,442 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 17:01:00,443 org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[DEBUG] 2021-07-19 17:01:00,450 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
[DEBUG] 2021-07-19 17:01:00,453 org.apache.spark.storage.BlockManager - Getting local block broadcast_4
[DEBUG] 2021-07-19 17:01:00,453 org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 17:01:00,504 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(11)
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Cleaning accumulator 11
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Cleaned accumulator 11
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(6)
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Cleaning accumulator 6
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Cleaned accumulator 6
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(5)
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Cleaning accumulator 5
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Cleaned accumulator 5
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(4)
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Cleaning accumulator 4
[DEBUG] 2021-07-19 17:01:00,505 org.apache.spark.ContextCleaner - Cleaned accumulator 4
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(3)
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Cleaning accumulator 3
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Cleaned accumulator 3
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(8)
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Cleaning accumulator 8
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Cleaned accumulator 8
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(13)
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Cleaning accumulator 13
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Cleaned accumulator 13
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(7)
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Cleaning accumulator 7
[DEBUG] 2021-07-19 17:01:00,506 org.apache.spark.ContextCleaner - Cleaned accumulator 7
[DEBUG] 2021-07-19 17:01:00,508 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(15)
[DEBUG] 2021-07-19 17:01:00,508 org.apache.spark.ContextCleaner - Cleaning accumulator 15
[DEBUG] 2021-07-19 17:01:00,508 org.apache.spark.ContextCleaner - Cleaned accumulator 15
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(0)
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Cleaning accumulator 0
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Cleaned accumulator 0
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(1)
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Cleaning accumulator 1
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Cleaned accumulator 1
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(10)
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Cleaning accumulator 10
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Cleaned accumulator 10
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(12)
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Cleaning accumulator 12
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Cleaned accumulator 12
[DEBUG] 2021-07-19 17:01:00,509 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(14)
[DEBUG] 2021-07-19 17:01:00,510 org.apache.spark.ContextCleaner - Cleaning accumulator 14
[DEBUG] 2021-07-19 17:01:00,510 org.apache.spark.ContextCleaner - Cleaned accumulator 14
[DEBUG] 2021-07-19 17:01:00,510 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(2)
[DEBUG] 2021-07-19 17:01:00,510 org.apache.spark.ContextCleaner - Cleaning accumulator 2
[DEBUG] 2021-07-19 17:01:00,510 org.apache.spark.ContextCleaner - Cleaned accumulator 2
[DEBUG] 2021-07-19 17:01:00,510 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(9)
[DEBUG] 2021-07-19 17:01:00,510 org.apache.spark.ContextCleaner - Cleaning accumulator 9
[DEBUG] 2021-07-19 17:01:00,510 org.apache.spark.ContextCleaner - Cleaned accumulator 9
[DEBUG] 2021-07-19 17:01:00,558 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, input[4, string, true].toString, input[5, string, true].toString, input[6, string, true].toString, StructField(date,StringType,true), StructField(open,StringType,true), StructField(high,StringType,true), StructField(low,StringType,true), StructField(close,StringType,true), StructField(volume,StringType,true), StructField(ticket,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[7];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     boolean isNull_14 = i.isNullAt(6);
/* 042 */     UTF8String value_14 = isNull_14 ?
/* 043 */     null : (i.getUTF8String(6));
/* 044 */     boolean isNull_13 = true;
/* 045 */     java.lang.String value_13 = null;
/* 046 */     if (!isNull_14) {
/* 047 */
/* 048 */       isNull_13 = false;
/* 049 */       if (!isNull_13) {
/* 050 */
/* 051 */         Object funcResult_6 = null;
/* 052 */         funcResult_6 = value_14.toString();
/* 053 */         value_13 = (java.lang.String) funcResult_6;
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     if (isNull_13) {
/* 058 */       values_0[6] = null;
/* 059 */     } else {
/* 060 */       values_0[6] = value_13;
/* 061 */     }
/* 062 */
/* 063 */   }
/* 064 */
/* 065 */
/* 066 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 067 */
/* 068 */     boolean isNull_8 = i.isNullAt(3);
/* 069 */     UTF8String value_8 = isNull_8 ?
/* 070 */     null : (i.getUTF8String(3));
/* 071 */     boolean isNull_7 = true;
/* 072 */     java.lang.String value_7 = null;
/* 073 */     if (!isNull_8) {
/* 074 */
/* 075 */       isNull_7 = false;
/* 076 */       if (!isNull_7) {
/* 077 */
/* 078 */         Object funcResult_3 = null;
/* 079 */         funcResult_3 = value_8.toString();
/* 080 */         value_7 = (java.lang.String) funcResult_3;
/* 081 */
/* 082 */       }
/* 083 */     }
/* 084 */     if (isNull_7) {
/* 085 */       values_0[3] = null;
/* 086 */     } else {
/* 087 */       values_0[3] = value_7;
/* 088 */     }
/* 089 */
/* 090 */     boolean isNull_10 = i.isNullAt(4);
/* 091 */     UTF8String value_10 = isNull_10 ?
/* 092 */     null : (i.getUTF8String(4));
/* 093 */     boolean isNull_9 = true;
/* 094 */     java.lang.String value_9 = null;
/* 095 */     if (!isNull_10) {
/* 096 */
/* 097 */       isNull_9 = false;
/* 098 */       if (!isNull_9) {
/* 099 */
/* 100 */         Object funcResult_4 = null;
/* 101 */         funcResult_4 = value_10.toString();
/* 102 */         value_9 = (java.lang.String) funcResult_4;
/* 103 */
/* 104 */       }
/* 105 */     }
/* 106 */     if (isNull_9) {
/* 107 */       values_0[4] = null;
/* 108 */     } else {
/* 109 */       values_0[4] = value_9;
/* 110 */     }
/* 111 */
/* 112 */     boolean isNull_12 = i.isNullAt(5);
/* 113 */     UTF8String value_12 = isNull_12 ?
/* 114 */     null : (i.getUTF8String(5));
/* 115 */     boolean isNull_11 = true;
/* 116 */     java.lang.String value_11 = null;
/* 117 */     if (!isNull_12) {
/* 118 */
/* 119 */       isNull_11 = false;
/* 120 */       if (!isNull_11) {
/* 121 */
/* 122 */         Object funcResult_5 = null;
/* 123 */         funcResult_5 = value_12.toString();
/* 124 */         value_11 = (java.lang.String) funcResult_5;
/* 125 */
/* 126 */       }
/* 127 */     }
/* 128 */     if (isNull_11) {
/* 129 */       values_0[5] = null;
/* 130 */     } else {
/* 131 */       values_0[5] = value_11;
/* 132 */     }
/* 133 */
/* 134 */   }
/* 135 */
/* 136 */
/* 137 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 138 */
/* 139 */     boolean isNull_2 = i.isNullAt(0);
/* 140 */     UTF8String value_2 = isNull_2 ?
/* 141 */     null : (i.getUTF8String(0));
/* 142 */     boolean isNull_1 = true;
/* 143 */     java.lang.String value_1 = null;
/* 144 */     if (!isNull_2) {
/* 145 */
/* 146 */       isNull_1 = false;
/* 147 */       if (!isNull_1) {
/* 148 */
/* 149 */         Object funcResult_0 = null;
/* 150 */         funcResult_0 = value_2.toString();
/* 151 */         value_1 = (java.lang.String) funcResult_0;
/* 152 */
/* 153 */       }
/* 154 */     }
/* 155 */     if (isNull_1) {
/* 156 */       values_0[0] = null;
/* 157 */     } else {
/* 158 */       values_0[0] = value_1;
/* 159 */     }
/* 160 */
/* 161 */     boolean isNull_4 = i.isNullAt(1);
/* 162 */     UTF8String value_4 = isNull_4 ?
/* 163 */     null : (i.getUTF8String(1));
/* 164 */     boolean isNull_3 = true;
/* 165 */     java.lang.String value_3 = null;
/* 166 */     if (!isNull_4) {
/* 167 */
/* 168 */       isNull_3 = false;
/* 169 */       if (!isNull_3) {
/* 170 */
/* 171 */         Object funcResult_1 = null;
/* 172 */         funcResult_1 = value_4.toString();
/* 173 */         value_3 = (java.lang.String) funcResult_1;
/* 174 */
/* 175 */       }
/* 176 */     }
/* 177 */     if (isNull_3) {
/* 178 */       values_0[1] = null;
/* 179 */     } else {
/* 180 */       values_0[1] = value_3;
/* 181 */     }
/* 182 */
/* 183 */     boolean isNull_6 = i.isNullAt(2);
/* 184 */     UTF8String value_6 = isNull_6 ?
/* 185 */     null : (i.getUTF8String(2));
/* 186 */     boolean isNull_5 = true;
/* 187 */     java.lang.String value_5 = null;
/* 188 */     if (!isNull_6) {
/* 189 */
/* 190 */       isNull_5 = false;
/* 191 */       if (!isNull_5) {
/* 192 */
/* 193 */         Object funcResult_2 = null;
/* 194 */         funcResult_2 = value_6.toString();
/* 195 */         value_5 = (java.lang.String) funcResult_2;
/* 196 */
/* 197 */       }
/* 198 */     }
/* 199 */     if (isNull_5) {
/* 200 */       values_0[2] = null;
/* 201 */     } else {
/* 202 */       values_0[2] = value_5;
/* 203 */     }
/* 204 */
/* 205 */   }
/* 206 */
/* 207 */ }

[DEBUG] 2021-07-19 17:01:00,566 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[7];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     boolean isNull_14 = i.isNullAt(6);
/* 042 */     UTF8String value_14 = isNull_14 ?
/* 043 */     null : (i.getUTF8String(6));
/* 044 */     boolean isNull_13 = true;
/* 045 */     java.lang.String value_13 = null;
/* 046 */     if (!isNull_14) {
/* 047 */
/* 048 */       isNull_13 = false;
/* 049 */       if (!isNull_13) {
/* 050 */
/* 051 */         Object funcResult_6 = null;
/* 052 */         funcResult_6 = value_14.toString();
/* 053 */         value_13 = (java.lang.String) funcResult_6;
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     if (isNull_13) {
/* 058 */       values_0[6] = null;
/* 059 */     } else {
/* 060 */       values_0[6] = value_13;
/* 061 */     }
/* 062 */
/* 063 */   }
/* 064 */
/* 065 */
/* 066 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 067 */
/* 068 */     boolean isNull_8 = i.isNullAt(3);
/* 069 */     UTF8String value_8 = isNull_8 ?
/* 070 */     null : (i.getUTF8String(3));
/* 071 */     boolean isNull_7 = true;
/* 072 */     java.lang.String value_7 = null;
/* 073 */     if (!isNull_8) {
/* 074 */
/* 075 */       isNull_7 = false;
/* 076 */       if (!isNull_7) {
/* 077 */
/* 078 */         Object funcResult_3 = null;
/* 079 */         funcResult_3 = value_8.toString();
/* 080 */         value_7 = (java.lang.String) funcResult_3;
/* 081 */
/* 082 */       }
/* 083 */     }
/* 084 */     if (isNull_7) {
/* 085 */       values_0[3] = null;
/* 086 */     } else {
/* 087 */       values_0[3] = value_7;
/* 088 */     }
/* 089 */
/* 090 */     boolean isNull_10 = i.isNullAt(4);
/* 091 */     UTF8String value_10 = isNull_10 ?
/* 092 */     null : (i.getUTF8String(4));
/* 093 */     boolean isNull_9 = true;
/* 094 */     java.lang.String value_9 = null;
/* 095 */     if (!isNull_10) {
/* 096 */
/* 097 */       isNull_9 = false;
/* 098 */       if (!isNull_9) {
/* 099 */
/* 100 */         Object funcResult_4 = null;
/* 101 */         funcResult_4 = value_10.toString();
/* 102 */         value_9 = (java.lang.String) funcResult_4;
/* 103 */
/* 104 */       }
/* 105 */     }
/* 106 */     if (isNull_9) {
/* 107 */       values_0[4] = null;
/* 108 */     } else {
/* 109 */       values_0[4] = value_9;
/* 110 */     }
/* 111 */
/* 112 */     boolean isNull_12 = i.isNullAt(5);
/* 113 */     UTF8String value_12 = isNull_12 ?
/* 114 */     null : (i.getUTF8String(5));
/* 115 */     boolean isNull_11 = true;
/* 116 */     java.lang.String value_11 = null;
/* 117 */     if (!isNull_12) {
/* 118 */
/* 119 */       isNull_11 = false;
/* 120 */       if (!isNull_11) {
/* 121 */
/* 122 */         Object funcResult_5 = null;
/* 123 */         funcResult_5 = value_12.toString();
/* 124 */         value_11 = (java.lang.String) funcResult_5;
/* 125 */
/* 126 */       }
/* 127 */     }
/* 128 */     if (isNull_11) {
/* 129 */       values_0[5] = null;
/* 130 */     } else {
/* 131 */       values_0[5] = value_11;
/* 132 */     }
/* 133 */
/* 134 */   }
/* 135 */
/* 136 */
/* 137 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 138 */
/* 139 */     boolean isNull_2 = i.isNullAt(0);
/* 140 */     UTF8String value_2 = isNull_2 ?
/* 141 */     null : (i.getUTF8String(0));
/* 142 */     boolean isNull_1 = true;
/* 143 */     java.lang.String value_1 = null;
/* 144 */     if (!isNull_2) {
/* 145 */
/* 146 */       isNull_1 = false;
/* 147 */       if (!isNull_1) {
/* 148 */
/* 149 */         Object funcResult_0 = null;
/* 150 */         funcResult_0 = value_2.toString();
/* 151 */         value_1 = (java.lang.String) funcResult_0;
/* 152 */
/* 153 */       }
/* 154 */     }
/* 155 */     if (isNull_1) {
/* 156 */       values_0[0] = null;
/* 157 */     } else {
/* 158 */       values_0[0] = value_1;
/* 159 */     }
/* 160 */
/* 161 */     boolean isNull_4 = i.isNullAt(1);
/* 162 */     UTF8String value_4 = isNull_4 ?
/* 163 */     null : (i.getUTF8String(1));
/* 164 */     boolean isNull_3 = true;
/* 165 */     java.lang.String value_3 = null;
/* 166 */     if (!isNull_4) {
/* 167 */
/* 168 */       isNull_3 = false;
/* 169 */       if (!isNull_3) {
/* 170 */
/* 171 */         Object funcResult_1 = null;
/* 172 */         funcResult_1 = value_4.toString();
/* 173 */         value_3 = (java.lang.String) funcResult_1;
/* 174 */
/* 175 */       }
/* 176 */     }
/* 177 */     if (isNull_3) {
/* 178 */       values_0[1] = null;
/* 179 */     } else {
/* 180 */       values_0[1] = value_3;
/* 181 */     }
/* 182 */
/* 183 */     boolean isNull_6 = i.isNullAt(2);
/* 184 */     UTF8String value_6 = isNull_6 ?
/* 185 */     null : (i.getUTF8String(2));
/* 186 */     boolean isNull_5 = true;
/* 187 */     java.lang.String value_5 = null;
/* 188 */     if (!isNull_6) {
/* 189 */
/* 190 */       isNull_5 = false;
/* 191 */       if (!isNull_5) {
/* 192 */
/* 193 */         Object funcResult_2 = null;
/* 194 */         funcResult_2 = value_6.toString();
/* 195 */         value_5 = (java.lang.String) funcResult_2;
/* 196 */
/* 197 */       }
/* 198 */     }
/* 199 */     if (isNull_5) {
/* 200 */       values_0[2] = null;
/* 201 */     } else {
/* 202 */       values_0[2] = value_5;
/* 203 */     }
/* 204 */
/* 205 */   }
/* 206 */
/* 207 */ }

[INFO] 2021-07-19 17:01:00,602 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 43.479364 ms
[INFO] 2021-07-19 17:01:00,607 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data1/aapl.txt/part-00000-4f441212-b8c4-4edf-b2fe-f60ce24a157f-c000.csv, range: 0-445012, partition values: [empty row]
[DEBUG] 2021-07-19 17:01:00,630 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */   }
/* 057 */
/* 058 */
/* 059 */   private void writeFields_0_0(InternalRow i) {
/* 060 */
/* 061 */     boolean isNull_0 = i.isNullAt(0);
/* 062 */     UTF8String value_0 = isNull_0 ?
/* 063 */     null : (i.getUTF8String(0));
/* 064 */     if (isNull_0) {
/* 065 */       mutableStateArray_0[0].setNullAt(0);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(0, value_0);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_1 = i.isNullAt(1);
/* 071 */     UTF8String value_1 = isNull_1 ?
/* 072 */     null : (i.getUTF8String(1));
/* 073 */     if (isNull_1) {
/* 074 */       mutableStateArray_0[0].setNullAt(1);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(1, value_1);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_2 = i.isNullAt(2);
/* 080 */     UTF8String value_2 = isNull_2 ?
/* 081 */     null : (i.getUTF8String(2));
/* 082 */     if (isNull_2) {
/* 083 */       mutableStateArray_0[0].setNullAt(2);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(2, value_2);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_3 = i.isNullAt(3);
/* 089 */     UTF8String value_3 = isNull_3 ?
/* 090 */     null : (i.getUTF8String(3));
/* 091 */     if (isNull_3) {
/* 092 */       mutableStateArray_0[0].setNullAt(3);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(3, value_3);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_4 = i.isNullAt(4);
/* 098 */     UTF8String value_4 = isNull_4 ?
/* 099 */     null : (i.getUTF8String(4));
/* 100 */     if (isNull_4) {
/* 101 */       mutableStateArray_0[0].setNullAt(4);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(4, value_4);
/* 104 */     }
/* 105 */
/* 106 */   }
/* 107 */
/* 108 */ }

[DEBUG] 2021-07-19 17:01:00,638 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */   }
/* 057 */
/* 058 */
/* 059 */   private void writeFields_0_0(InternalRow i) {
/* 060 */
/* 061 */     boolean isNull_0 = i.isNullAt(0);
/* 062 */     UTF8String value_0 = isNull_0 ?
/* 063 */     null : (i.getUTF8String(0));
/* 064 */     if (isNull_0) {
/* 065 */       mutableStateArray_0[0].setNullAt(0);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(0, value_0);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_1 = i.isNullAt(1);
/* 071 */     UTF8String value_1 = isNull_1 ?
/* 072 */     null : (i.getUTF8String(1));
/* 073 */     if (isNull_1) {
/* 074 */       mutableStateArray_0[0].setNullAt(1);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(1, value_1);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_2 = i.isNullAt(2);
/* 080 */     UTF8String value_2 = isNull_2 ?
/* 081 */     null : (i.getUTF8String(2));
/* 082 */     if (isNull_2) {
/* 083 */       mutableStateArray_0[0].setNullAt(2);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(2, value_2);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_3 = i.isNullAt(3);
/* 089 */     UTF8String value_3 = isNull_3 ?
/* 090 */     null : (i.getUTF8String(3));
/* 091 */     if (isNull_3) {
/* 092 */       mutableStateArray_0[0].setNullAt(3);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(3, value_3);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_4 = i.isNullAt(4);
/* 098 */     UTF8String value_4 = isNull_4 ?
/* 099 */     null : (i.getUTF8String(4));
/* 100 */     if (isNull_4) {
/* 101 */       mutableStateArray_0[0].setNullAt(4);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(4, value_4);
/* 104 */     }
/* 105 */
/* 106 */   }
/* 107 */
/* 108 */ }

[INFO] 2021-07-19 17:01:00,707 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 74.571648 ms
[DEBUG] 2021-07-19 17:01:00,709 org.apache.spark.storage.BlockManager - Getting local block broadcast_3
[DEBUG] 2021-07-19 17:01:00,709 org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 17:01:00,751 org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate - Generated predicate '(isnotnull(input[6, string, true]) AND (input[6, string, true] = aapl))':
/* 001 */ public SpecificPredicate generate(Object[] references) {
/* 002 */   return new SpecificPredicate(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {
/* 006 */   private final Object[] references;
/* 007 */
/* 008 */
/* 009 */   public SpecificPredicate(Object[] references) {
/* 010 */     this.references = references;
/* 011 */
/* 012 */   }
/* 013 */
/* 014 */   public void initialize(int partitionIndex) {
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public boolean eval(InternalRow i) {
/* 019 */
/* 020 */     boolean isNull_2 = i.isNullAt(6);
/* 021 */     UTF8String value_2 = isNull_2 ?
/* 022 */     null : (i.getUTF8String(6));
/* 023 */     boolean value_3 = !isNull_2;
/* 024 */     boolean isNull_0 = false;
/* 025 */     boolean value_0 = false;
/* 026 */
/* 027 */     if (!false && !value_3) {
/* 028 */     } else {
/* 029 */       boolean isNull_3 = true;
/* 030 */       boolean value_4 = false;
/* 031 */       boolean isNull_4 = i.isNullAt(6);
/* 032 */       UTF8String value_5 = isNull_4 ?
/* 033 */       null : (i.getUTF8String(6));
/* 034 */       if (!isNull_4) {
/* 035 */
/* 036 */
/* 037 */         isNull_3 = false; // resultCode could change nullability.
/* 038 */         value_4 = value_5.equals(((UTF8String) references[0] /* literal */));
/* 039 */
/* 040 */       }
/* 041 */       if (!isNull_3 && !value_4) {
/* 042 */       } else if (!false && !isNull_3) {
/* 043 */         value_0 = true;
/* 044 */       } else {
/* 045 */         isNull_0 = true;
/* 046 */       }
/* 047 */     }
/* 048 */     return !isNull_0 && value_0;
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

[DEBUG] 2021-07-19 17:01:00,754 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public SpecificPredicate generate(Object[] references) {
/* 002 */   return new SpecificPredicate(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {
/* 006 */   private final Object[] references;
/* 007 */
/* 008 */
/* 009 */   public SpecificPredicate(Object[] references) {
/* 010 */     this.references = references;
/* 011 */
/* 012 */   }
/* 013 */
/* 014 */   public void initialize(int partitionIndex) {
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public boolean eval(InternalRow i) {
/* 019 */
/* 020 */     boolean isNull_2 = i.isNullAt(6);
/* 021 */     UTF8String value_2 = isNull_2 ?
/* 022 */     null : (i.getUTF8String(6));
/* 023 */     boolean value_3 = !isNull_2;
/* 024 */     boolean isNull_0 = false;
/* 025 */     boolean value_0 = false;
/* 026 */
/* 027 */     if (!false && !value_3) {
/* 028 */     } else {
/* 029 */       boolean isNull_3 = true;
/* 030 */       boolean value_4 = false;
/* 031 */       boolean isNull_4 = i.isNullAt(6);
/* 032 */       UTF8String value_5 = isNull_4 ?
/* 033 */       null : (i.getUTF8String(6));
/* 034 */       if (!isNull_4) {
/* 035 */
/* 036 */
/* 037 */         isNull_3 = false; // resultCode could change nullability.
/* 038 */         value_4 = value_5.equals(((UTF8String) references[0] /* literal */));
/* 039 */
/* 040 */       }
/* 041 */       if (!isNull_3 && !value_4) {
/* 042 */       } else if (!false && !isNull_3) {
/* 043 */         value_0 = true;
/* 044 */       } else {
/* 045 */         isNull_0 = true;
/* 046 */       }
/* 047 */     }
/* 048 */     return !isNull_0 && value_0;
/* 049 */   }
/* 050 */
/* 051 */
/* 052 */ }

[INFO] 2021-07-19 17:01:00,762 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.807812 ms
[DEBUG] 2021-07-19 17:01:00,876 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(0)
[DEBUG] 2021-07-19 17:01:00,876 org.apache.spark.ContextCleaner - Cleaning broadcast 0
[DEBUG] 2021-07-19 17:01:00,876 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 0
[DEBUG] 2021-07-19 17:01:00,878 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 0
[DEBUG] 2021-07-19 17:01:00,878 org.apache.spark.storage.BlockManager - Removing broadcast 0
[DEBUG] 2021-07-19 17:01:00,878 org.apache.spark.storage.BlockManager - Removing block broadcast_0_piece0
[DEBUG] 2021-07-19 17:01:00,881 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 of size 28144 dropped from memory (free 1043965664)
[DEBUG] 2021-07-19 17:01:00,889 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 46767, None)
[INFO] 2021-07-19 17:01:00,889 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.1.29:46767 in memory (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 17:01:00,891 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 17:01:00,892 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 17:01:00,892 org.apache.spark.storage.BlockManager - Removing block broadcast_0
[DEBUG] 2021-07-19 17:01:00,892 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 of size 178344 dropped from memory (free 1044144008)
[DEBUG] 2021-07-19 17:01:00,893 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 0, response is 0
[DEBUG] 2021-07-19 17:01:00,894 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:34407
[DEBUG] 2021-07-19 17:01:00,895 org.apache.spark.ContextCleaner - Cleaned broadcast 0
[INFO] 2021-07-19 17:11:45,639 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 17:11:45,659 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 17:11:45,664 org.apache.spark.scheduler.DAGScheduler - Job 1 failed: foreach at Simulator.scala:38, took 645.389927 s
[INFO] 2021-07-19 17:11:45,667 org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (foreach at Simulator.scala:38) failed in 645.371 s due to Stage cancelled because SparkContext was shut down
[INFO] 2021-07-19 17:11:45,690 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 17:11:45,745 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 17:11:45,745 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 17:11:45,748 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 17:11:45,752 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 17:11:45,795 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 17:11:45,796 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 17:11:45,797 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-88aaff04-cce6-42a3-872b-0cbb85d1c2c8
[WARN] 2021-07-19 17:12:13,411 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:12:13,413 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[INFO] 2021-07-19 17:12:13,682 org.apache.spark.SparkContext - Running Spark version 3.1.2
[INFO] 2021-07-19 17:12:14,492 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 17:12:14,493 org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
[INFO] 2021-07-19 17:12:14,495 org.apache.spark.resource.ResourceUtils - ==============================================================
[INFO] 2021-07-19 17:12:14,498 org.apache.spark.SparkContext - Submitted application: test
[INFO] 2021-07-19 17:12:14,564 org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[INFO] 2021-07-19 17:12:14,584 org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
[INFO] 2021-07-19 17:12:14,586 org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
[INFO] 2021-07-19 17:12:14,711 org.apache.spark.SecurityManager - Changing view acls to: dev
[INFO] 2021-07-19 17:12:14,714 org.apache.spark.SecurityManager - Changing modify acls to: dev
[INFO] 2021-07-19 17:12:14,721 org.apache.spark.SecurityManager - Changing view acls groups to: 
[INFO] 2021-07-19 17:12:14,726 org.apache.spark.SecurityManager - Changing modify acls groups to: 
[INFO] 2021-07-19 17:12:14,741 org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dev); groups with view permissions: Set(); users  with modify permissions: Set(dev); groups with modify permissions: Set()
[DEBUG] 2021-07-19 17:12:15,610 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 32937
[INFO] 2021-07-19 17:12:15,625 org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 32937.
[DEBUG] 2021-07-19 17:12:15,631 org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
[INFO] 2021-07-19 17:12:15,731 org.apache.spark.SparkEnv - Registering MapOutputTracker
[DEBUG] 2021-07-19 17:12:15,732 org.apache.spark.MapOutputTrackerMasterEndpoint - init
[INFO] 2021-07-19 17:12:15,786 org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO] 2021-07-19 17:12:15,822 org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO] 2021-07-19 17:12:15,824 org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
[INFO] 2021-07-19 17:12:15,838 org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
[INFO] 2021-07-19 17:12:15,887 org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f02c7ed5-3e91-4abd-82c2-37e62211fdce
[DEBUG] 2021-07-19 17:12:15,890 org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
[DEBUG] 2021-07-19 17:12:15,892 org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
[INFO] 2021-07-19 17:12:15,951 org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 996.0 MiB
[INFO] 2021-07-19 17:12:15,999 org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[DEBUG] 2021-07-19 17:12:16,000 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
[DEBUG] 2021-07-19 17:12:16,024 org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
[DEBUG] 2021-07-19 17:12:16,355 org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
[INFO] 2021-07-19 17:12:16,384 org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO] 2021-07-19 17:12:16,482 org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.29:4040
[INFO] 2021-07-19 17:12:16,951 org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.1.29
[DEBUG] 2021-07-19 17:12:17,012 org.apache.spark.network.server.TransportServer - Shuffle server started on port: 35121
[INFO] 2021-07-19 17:12:17,012 org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35121.
[INFO] 2021-07-19 17:12:17,012 org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.1.29:35121
[INFO] 2021-07-19 17:12:17,015 org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO] 2021-07-19 17:12:17,025 org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.1.29, 35121, None)
[DEBUG] 2021-07-19 17:12:17,028 org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.1.29
[INFO] 2021-07-19 17:12:17,031 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.1.29:35121 with 996.0 MiB RAM, BlockManagerId(driver, 192.168.1.29, 35121, None)
[INFO] 2021-07-19 17:12:17,034 org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.1.29, 35121, None)
[INFO] 2021-07-19 17:12:17,036 org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.1.29, 35121, None)
[DEBUG] 2021-07-19 17:12:17,341 org.apache.spark.SparkContext - Adding shutdown hook
[INFO] 2021-07-19 17:12:17,588 org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse').
[INFO] 2021-07-19 17:12:17,589 org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/dev/IdeaProjects/StockSimulator/spark-warehouse'.
[DEBUG] 2021-07-19 17:12:17,594 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> test
[DEBUG] 2021-07-19 17:12:17,595 org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local[*]
[DEBUG] 2021-07-19 17:12:19,023 org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:
  
[INFO] 2021-07-19 17:12:19,093 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 57 ms to list leaf files for 1 paths.
[INFO] 2021-07-19 17:12:19,213 org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 7 ms to list leaf files for 1 paths.
[DEBUG] 2021-07-19 17:12:21,225 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#0
[DEBUG] 2021-07-19 17:12:21,477 org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#6
[INFO] 2021-07-19 17:12:21,969 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 17:12:21,973 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
[INFO] 2021-07-19 17:12:21,977 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[DEBUG] 2021-07-19 17:12:22,544 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[DEBUG] 2021-07-19 17:12:22,581 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

[INFO] 2021-07-19 17:12:22,852 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 306.277525 ms
[INFO] 2021-07-19 17:12:22,941 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 174.2 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 17:12:22,943 org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 65 ms
[DEBUG] 2021-07-19 17:12:22,945 org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 66 ms
[INFO] 2021-07-19 17:12:23,064 org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 17:12:23,067 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.1.29, 35121, None)
[INFO] 2021-07-19 17:12:23,069 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.1.29:35121 (size: 27.5 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 17:12:23,075 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
[DEBUG] 2021-07-19 17:12:23,076 org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
[DEBUG] 2021-07-19 17:12:23,077 org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 15 ms
[DEBUG] 2021-07-19 17:12:23,077 org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 15 ms
[INFO] 2021-07-19 17:12:23,082 org.apache.spark.SparkContext - Created broadcast 0 from csv at Simulator.scala:21
[INFO] 2021-07-19 17:12:23,099 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 17:12:23,214 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 17:12:23,238 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 17:12:23,316 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
[DEBUG] 2021-07-19 17:12:23,335 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
[DEBUG] 2021-07-19 17:12:23,342 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
[DEBUG] 2021-07-19 17:12:23,399 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
[INFO] 2021-07-19 17:12:23,412 org.apache.spark.SparkContext - Starting job: csv at Simulator.scala:21
[DEBUG] 2021-07-19 17:12:23,433 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 17:12:23,475 org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at Simulator.scala:21) with 1 output partitions
[INFO] 2021-07-19 17:12:23,477 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at Simulator.scala:21)
[INFO] 2021-07-19 17:12:23,478 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 17:12:23,485 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 17:12:23,517 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=csv at Simulator.scala:21;jobs=0))
[DEBUG] 2021-07-19 17:12:23,527 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 17:12:23,540 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:21), which has no missing parents
[DEBUG] 2021-07-19 17:12:23,546 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
[INFO] 2021-07-19 17:12:23,933 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 17:12:23,934 org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 3 ms
[DEBUG] 2021-07-19 17:12:23,935 org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 4 ms
[INFO] 2021-07-19 17:12:23,938 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 995.8 MiB)
[DEBUG] 2021-07-19 17:12:23,939 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 35121, None)
[INFO] 2021-07-19 17:12:23,940 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.1.29:35121 (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 17:12:23,942 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 17:12:23,943 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 17:12:23,944 org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 7 ms
[DEBUG] 2021-07-19 17:12:23,945 org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 8 ms
[INFO] 2021-07-19 17:12:23,946 org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[INFO] 2021-07-19 17:12:23,973 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at Simulator.scala:21) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 17:12:23,975 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 17:12:24,004 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
[DEBUG] 2021-07-19 17:12:24,011 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 6 ms
[DEBUG] 2021-07-19 17:12:24,015 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[DEBUG] 2021-07-19 17:12:24,027 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
[DEBUG] 2021-07-19 17:12:24,027 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
[INFO] 2021-07-19 17:12:24,048 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 4885 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 17:12:24,053 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[INFO] 2021-07-19 17:12:24,071 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[DEBUG] 2021-07-19 17:12:24,086 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
[DEBUG] 2021-07-19 17:12:24,113 org.apache.spark.storage.BlockManager - Getting local block broadcast_1
[DEBUG] 2021-07-19 17:12:24,115 org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 17:12:24,231 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data/dis.us.txt, range: 0-600954, partition values: [empty row]
[DEBUG] 2021-07-19 17:12:24,243 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[DEBUG] 2021-07-19 17:12:24,258 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

[INFO] 2021-07-19 17:12:24,273 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 29.052849 ms
[DEBUG] 2021-07-19 17:12:24,274 org.apache.spark.storage.BlockManager - Getting local block broadcast_0
[DEBUG] 2021-07-19 17:12:24,274 org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
[INFO] 2021-07-19 17:12:24,343 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1596 bytes result sent to driver
[DEBUG] 2021-07-19 17:12:24,349 org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
[INFO] 2021-07-19 17:12:24,364 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 327 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 17:12:24,366 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 17:12:24,375 org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at Simulator.scala:21) finished in 0.732 s
[DEBUG] 2021-07-19 17:12:24,380 org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
[INFO] 2021-07-19 17:12:24,381 org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 17:12:24,382 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
[INFO] 2021-07-19 17:12:24,384 org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at Simulator.scala:21, took 0.971048 s
[DEBUG] 2021-07-19 17:12:24,409 org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[DEBUG] 2021-07-19 17:12:24,416 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

[INFO] 2021-07-19 17:12:24,434 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.792421 ms
[DEBUG] 2021-07-19 17:12:24,517 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(40)
[DEBUG] 2021-07-19 17:12:24,518 org.apache.spark.ContextCleaner - Cleaning accumulator 40
[DEBUG] 2021-07-19 17:12:24,519 org.apache.spark.ContextCleaner - Cleaned accumulator 40
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(36)
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Cleaning accumulator 36
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Cleaned accumulator 36
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(21)
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Cleaning accumulator 21
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Cleaned accumulator 21
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(16)
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Cleaning accumulator 16
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Cleaned accumulator 16
[DEBUG] 2021-07-19 17:12:24,520 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(27)
[DEBUG] 2021-07-19 17:12:24,521 org.apache.spark.ContextCleaner - Cleaning accumulator 27
[DEBUG] 2021-07-19 17:12:24,521 org.apache.spark.ContextCleaner - Cleaned accumulator 27
[DEBUG] 2021-07-19 17:12:24,521 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(34)
[DEBUG] 2021-07-19 17:12:24,521 org.apache.spark.ContextCleaner - Cleaning accumulator 34
[DEBUG] 2021-07-19 17:12:24,522 org.apache.spark.ContextCleaner - Cleaned accumulator 34
[DEBUG] 2021-07-19 17:12:24,522 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(24)
[DEBUG] 2021-07-19 17:12:24,522 org.apache.spark.ContextCleaner - Cleaning accumulator 24
[DEBUG] 2021-07-19 17:12:24,522 org.apache.spark.ContextCleaner - Cleaned accumulator 24
[DEBUG] 2021-07-19 17:12:24,522 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(26)
[DEBUG] 2021-07-19 17:12:24,522 org.apache.spark.ContextCleaner - Cleaning accumulator 26
[DEBUG] 2021-07-19 17:12:24,523 org.apache.spark.ContextCleaner - Cleaned accumulator 26
[DEBUG] 2021-07-19 17:12:24,523 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(31)
[DEBUG] 2021-07-19 17:12:24,523 org.apache.spark.ContextCleaner - Cleaning accumulator 31
[DEBUG] 2021-07-19 17:12:24,523 org.apache.spark.ContextCleaner - Cleaned accumulator 31
[DEBUG] 2021-07-19 17:12:24,523 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(32)
[DEBUG] 2021-07-19 17:12:24,523 org.apache.spark.ContextCleaner - Cleaning accumulator 32
[DEBUG] 2021-07-19 17:12:24,523 org.apache.spark.ContextCleaner - Cleaned accumulator 32
[DEBUG] 2021-07-19 17:12:24,524 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(29)
[DEBUG] 2021-07-19 17:12:24,524 org.apache.spark.ContextCleaner - Cleaning accumulator 29
[DEBUG] 2021-07-19 17:12:24,524 org.apache.spark.ContextCleaner - Cleaned accumulator 29
[DEBUG] 2021-07-19 17:12:24,524 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(18)
[DEBUG] 2021-07-19 17:12:24,524 org.apache.spark.ContextCleaner - Cleaning accumulator 18
[DEBUG] 2021-07-19 17:12:24,524 org.apache.spark.ContextCleaner - Cleaned accumulator 18
[DEBUG] 2021-07-19 17:12:24,525 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(20)
[DEBUG] 2021-07-19 17:12:24,525 org.apache.spark.ContextCleaner - Cleaning accumulator 20
[DEBUG] 2021-07-19 17:12:24,526 org.apache.spark.ContextCleaner - Cleaned accumulator 20
[DEBUG] 2021-07-19 17:12:24,526 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(37)
[DEBUG] 2021-07-19 17:12:24,526 org.apache.spark.ContextCleaner - Cleaning accumulator 37
[DEBUG] 2021-07-19 17:12:24,526 org.apache.spark.ContextCleaner - Cleaned accumulator 37
[DEBUG] 2021-07-19 17:12:24,527 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(25)
[DEBUG] 2021-07-19 17:12:24,529 org.apache.spark.ContextCleaner - Cleaning accumulator 25
[DEBUG] 2021-07-19 17:12:24,529 org.apache.spark.ContextCleaner - Cleaned accumulator 25
[DEBUG] 2021-07-19 17:12:24,529 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(17)
[DEBUG] 2021-07-19 17:12:24,530 org.apache.spark.ContextCleaner - Cleaning accumulator 17
[DEBUG] 2021-07-19 17:12:24,530 org.apache.spark.ContextCleaner - Cleaned accumulator 17
[DEBUG] 2021-07-19 17:12:24,530 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(30)
[DEBUG] 2021-07-19 17:12:24,530 org.apache.spark.ContextCleaner - Cleaning accumulator 30
[DEBUG] 2021-07-19 17:12:24,530 org.apache.spark.ContextCleaner - Cleaned accumulator 30
[DEBUG] 2021-07-19 17:12:24,530 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(39)
[DEBUG] 2021-07-19 17:12:24,530 org.apache.spark.ContextCleaner - Cleaning accumulator 39
[DEBUG] 2021-07-19 17:12:24,531 org.apache.spark.ContextCleaner - Cleaned accumulator 39
[DEBUG] 2021-07-19 17:12:24,532 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(38)
[DEBUG] 2021-07-19 17:12:24,532 org.apache.spark.ContextCleaner - Cleaning accumulator 38
[DEBUG] 2021-07-19 17:12:24,533 org.apache.spark.ContextCleaner - Cleaned accumulator 38
[DEBUG] 2021-07-19 17:12:24,533 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(19)
[DEBUG] 2021-07-19 17:12:24,533 org.apache.spark.ContextCleaner - Cleaning accumulator 19
[DEBUG] 2021-07-19 17:12:24,533 org.apache.spark.ContextCleaner - Cleaned accumulator 19
[DEBUG] 2021-07-19 17:12:24,533 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(23)
[DEBUG] 2021-07-19 17:12:24,533 org.apache.spark.ContextCleaner - Cleaning accumulator 23
[DEBUG] 2021-07-19 17:12:24,533 org.apache.spark.ContextCleaner - Cleaned accumulator 23
[DEBUG] 2021-07-19 17:12:24,542 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(1)
[DEBUG] 2021-07-19 17:12:24,543 org.apache.spark.ContextCleaner - Cleaning broadcast 1
[DEBUG] 2021-07-19 17:12:24,544 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
[DEBUG] 2021-07-19 17:12:24,573 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
[DEBUG] 2021-07-19 17:12:24,574 org.apache.spark.storage.BlockManager - Removing broadcast 1
[DEBUG] 2021-07-19 17:12:24,576 org.apache.spark.storage.BlockManager - Removing block broadcast_1
[DEBUG] 2021-07-19 17:12:24,577 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 11088 dropped from memory (free 1044169695)
[DEBUG] 2021-07-19 17:12:24,578 org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
[DEBUG] 2021-07-19 17:12:24,579 org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 5515 dropped from memory (free 1044175210)
[DEBUG] 2021-07-19 17:12:24,579 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.1.29, 35121, None)
[INFO] 2021-07-19 17:12:24,582 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.1.29:35121 in memory (size: 5.4 KiB, free: 996.0 MiB)
[DEBUG] 2021-07-19 17:12:24,584 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
[DEBUG] 2021-07-19 17:12:24,584 org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
[DEBUG] 2021-07-19 17:12:24,586 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
[DEBUG] 2021-07-19 17:12:24,587 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:32937
[DEBUG] 2021-07-19 17:12:24,589 org.apache.spark.ContextCleaner - Cleaned broadcast 1
[DEBUG] 2021-07-19 17:12:24,589 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(33)
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Cleaning accumulator 33
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Cleaned accumulator 33
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(35)
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Cleaning accumulator 35
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Cleaned accumulator 35
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(22)
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Cleaning accumulator 22
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Cleaned accumulator 22
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(28)
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Cleaning accumulator 28
[DEBUG] 2021-07-19 17:12:24,590 org.apache.spark.ContextCleaner - Cleaned accumulator 28
[INFO] 2021-07-19 17:12:24,604 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 17:12:24,604 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 17:12:24,604 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
[INFO] 2021-07-19 17:12:24,618 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 174.2 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 17:12:24,619 org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 9 ms
[DEBUG] 2021-07-19 17:12:24,619 org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 9 ms
[INFO] 2021-07-19 17:12:24,635 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.6 MiB)
[DEBUG] 2021-07-19 17:12:24,635 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 35121, None)
[INFO] 2021-07-19 17:12:24,636 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.1.29:35121 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 17:12:24,642 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 17:12:24,643 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[DEBUG] 2021-07-19 17:12:24,643 org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 8 ms
[DEBUG] 2021-07-19 17:12:24,644 org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 9 ms
[INFO] 2021-07-19 17:12:24,650 org.apache.spark.SparkContext - Created broadcast 2 from csv at Simulator.scala:21
[INFO] 2021-07-19 17:12:24,652 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 17:12:24,661 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
[DEBUG] 2021-07-19 17:12:24,684 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
[DEBUG] 2021-07-19 17:12:24,713 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inferFromDataset$2
[DEBUG] 2021-07-19 17:12:24,717 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
[INFO] 2021-07-19 17:12:24,988 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
[INFO] 2021-07-19 17:12:24,988 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
[INFO] 2021-07-19 17:12:24,989 org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<Date: string, Open: string, High: string, Low: string, Close: string ... 1 more field>
[DEBUG] 2021-07-19 17:12:25,018 org.apache.spark.internal.io.FileCommitProtocol - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job 16cc4e65-995f-433d-aef4-75a99d7e759d; output=file:/home/dev/IdeaProjects/StockSimulator/data1/dis.txt; dynamic=false
[DEBUG] 2021-07-19 17:12:25,021 org.apache.spark.internal.io.FileCommitProtocol - Using (String, String, Boolean) constructor
[INFO] 2021-07-19 17:12:25,044 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[DEBUG] 2021-07-19 17:12:25,100 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       // common sub-expressions
/* 029 */
/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(0));
/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 035 */       null : (inputadapter_row_0.getUTF8String(1));
/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 037 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 038 */       null : (inputadapter_row_0.getUTF8String(2));
/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 041 */       null : (inputadapter_row_0.getUTF8String(3));
/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 044 */       null : (inputadapter_row_0.getUTF8String(4));
/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 046 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 047 */       null : (inputadapter_row_0.getUTF8String(5));
/* 048 */       project_mutableStateArray_0[0].reset();
/* 049 */
/* 050 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 051 */
/* 052 */       if (inputadapter_isNull_0) {
/* 053 */         project_mutableStateArray_0[0].setNullAt(0);
/* 054 */       } else {
/* 055 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 056 */       }
/* 057 */
/* 058 */       if (inputadapter_isNull_1) {
/* 059 */         project_mutableStateArray_0[0].setNullAt(1);
/* 060 */       } else {
/* 061 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 062 */       }
/* 063 */
/* 064 */       if (inputadapter_isNull_2) {
/* 065 */         project_mutableStateArray_0[0].setNullAt(2);
/* 066 */       } else {
/* 067 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);
/* 068 */       }
/* 069 */
/* 070 */       if (inputadapter_isNull_3) {
/* 071 */         project_mutableStateArray_0[0].setNullAt(3);
/* 072 */       } else {
/* 073 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);
/* 074 */       }
/* 075 */
/* 076 */       if (inputadapter_isNull_4) {
/* 077 */         project_mutableStateArray_0[0].setNullAt(4);
/* 078 */       } else {
/* 079 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);
/* 080 */       }
/* 081 */
/* 082 */       if (inputadapter_isNull_5) {
/* 083 */         project_mutableStateArray_0[0].setNullAt(5);
/* 084 */       } else {
/* 085 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);
/* 086 */       }
/* 087 */
/* 088 */       project_mutableStateArray_0[0].write(6, ((UTF8String) references[0] /* literal */));
/* 089 */       append((project_mutableStateArray_0[0].getRow()));
/* 090 */       if (shouldStop()) return;
/* 091 */     }
/* 092 */   }
/* 093 */
/* 094 */ }

[DEBUG] 2021-07-19 17:12:25,108 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       // common sub-expressions
/* 029 */
/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(0));
/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 035 */       null : (inputadapter_row_0.getUTF8String(1));
/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 037 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 038 */       null : (inputadapter_row_0.getUTF8String(2));
/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 041 */       null : (inputadapter_row_0.getUTF8String(3));
/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 044 */       null : (inputadapter_row_0.getUTF8String(4));
/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 046 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 047 */       null : (inputadapter_row_0.getUTF8String(5));
/* 048 */       project_mutableStateArray_0[0].reset();
/* 049 */
/* 050 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 051 */
/* 052 */       if (inputadapter_isNull_0) {
/* 053 */         project_mutableStateArray_0[0].setNullAt(0);
/* 054 */       } else {
/* 055 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 056 */       }
/* 057 */
/* 058 */       if (inputadapter_isNull_1) {
/* 059 */         project_mutableStateArray_0[0].setNullAt(1);
/* 060 */       } else {
/* 061 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 062 */       }
/* 063 */
/* 064 */       if (inputadapter_isNull_2) {
/* 065 */         project_mutableStateArray_0[0].setNullAt(2);
/* 066 */       } else {
/* 067 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);
/* 068 */       }
/* 069 */
/* 070 */       if (inputadapter_isNull_3) {
/* 071 */         project_mutableStateArray_0[0].setNullAt(3);
/* 072 */       } else {
/* 073 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);
/* 074 */       }
/* 075 */
/* 076 */       if (inputadapter_isNull_4) {
/* 077 */         project_mutableStateArray_0[0].setNullAt(4);
/* 078 */       } else {
/* 079 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);
/* 080 */       }
/* 081 */
/* 082 */       if (inputadapter_isNull_5) {
/* 083 */         project_mutableStateArray_0[0].setNullAt(5);
/* 084 */       } else {
/* 085 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);
/* 086 */       }
/* 087 */
/* 088 */       project_mutableStateArray_0[0].write(6, ((UTF8String) references[0] /* literal */));
/* 089 */       append((project_mutableStateArray_0[0].getRow()));
/* 090 */       if (shouldStop()) return;
/* 091 */     }
/* 092 */   }
/* 093 */
/* 094 */ }

[INFO] 2021-07-19 17:12:25,140 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 38.731669 ms
[INFO] 2021-07-19 17:12:25,146 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 174.1 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 17:12:25,147 org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 5 ms
[DEBUG] 2021-07-19 17:12:25,147 org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 5 ms
[INFO] 2021-07-19 17:12:25,182 org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 995.4 MiB)
[DEBUG] 2021-07-19 17:12:25,182 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.1.29, 35121, None)
[INFO] 2021-07-19 17:12:25,183 org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.1.29:35121 (size: 27.5 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 17:12:25,184 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
[DEBUG] 2021-07-19 17:12:25,184 org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
[DEBUG] 2021-07-19 17:12:25,184 org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 3 ms
[DEBUG] 2021-07-19 17:12:25,184 org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 3 ms
[INFO] 2021-07-19 17:12:25,186 org.apache.spark.SparkContext - Created broadcast 3 from save at Simulator.scala:30
[INFO] 2021-07-19 17:12:25,195 org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[DEBUG] 2021-07-19 17:12:25,219 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 17:12:25,224 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 17:12:25,264 org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 224);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       // common sub-expressions
/* 029 */
/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(0));
/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 035 */       null : (inputadapter_row_0.getUTF8String(1));
/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 037 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 038 */       null : (inputadapter_row_0.getUTF8String(2));
/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 041 */       null : (inputadapter_row_0.getUTF8String(3));
/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 044 */       null : (inputadapter_row_0.getUTF8String(4));
/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 046 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 047 */       null : (inputadapter_row_0.getUTF8String(5));
/* 048 */       project_mutableStateArray_0[0].reset();
/* 049 */
/* 050 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 051 */
/* 052 */       if (inputadapter_isNull_0) {
/* 053 */         project_mutableStateArray_0[0].setNullAt(0);
/* 054 */       } else {
/* 055 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 056 */       }
/* 057 */
/* 058 */       if (inputadapter_isNull_1) {
/* 059 */         project_mutableStateArray_0[0].setNullAt(1);
/* 060 */       } else {
/* 061 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 062 */       }
/* 063 */
/* 064 */       if (inputadapter_isNull_2) {
/* 065 */         project_mutableStateArray_0[0].setNullAt(2);
/* 066 */       } else {
/* 067 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);
/* 068 */       }
/* 069 */
/* 070 */       if (inputadapter_isNull_3) {
/* 071 */         project_mutableStateArray_0[0].setNullAt(3);
/* 072 */       } else {
/* 073 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);
/* 074 */       }
/* 075 */
/* 076 */       if (inputadapter_isNull_4) {
/* 077 */         project_mutableStateArray_0[0].setNullAt(4);
/* 078 */       } else {
/* 079 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);
/* 080 */       }
/* 081 */
/* 082 */       if (inputadapter_isNull_5) {
/* 083 */         project_mutableStateArray_0[0].setNullAt(5);
/* 084 */       } else {
/* 085 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);
/* 086 */       }
/* 087 */
/* 088 */       project_mutableStateArray_0[0].write(6, ((UTF8String) references[0] /* literal */));
/* 089 */       append((project_mutableStateArray_0[0].getRow()));
/* 090 */       if (shouldStop()) return;
/* 091 */     }
/* 092 */   }
/* 093 */
/* 094 */ }

[DEBUG] 2021-07-19 17:12:25,266 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
[DEBUG] 2021-07-19 17:12:25,268 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
[DEBUG] 2021-07-19 17:12:25,313 org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$write$15
[DEBUG] 2021-07-19 17:12:25,317 org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$write$15) is now cleaned +++
[INFO] 2021-07-19 17:12:25,375 org.apache.spark.SparkContext - Starting job: save at Simulator.scala:30
[DEBUG] 2021-07-19 17:12:25,376 org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
[INFO] 2021-07-19 17:12:25,378 org.apache.spark.scheduler.DAGScheduler - Got job 1 (save at Simulator.scala:30) with 1 output partitions
[INFO] 2021-07-19 17:12:25,378 org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (save at Simulator.scala:30)
[INFO] 2021-07-19 17:12:25,378 org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO] 2021-07-19 17:12:25,379 org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[DEBUG] 2021-07-19 17:12:25,383 org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 1 (name=save at Simulator.scala:30;jobs=1))
[DEBUG] 2021-07-19 17:12:25,383 org.apache.spark.scheduler.DAGScheduler - missing: List()
[INFO] 2021-07-19 17:12:25,384 org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (CoalescedRDD[15] at save at Simulator.scala:30), which has no missing parents
[DEBUG] 2021-07-19 17:12:25,384 org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 1)
[INFO] 2021-07-19 17:12:25,443 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 176.1 KiB, free 995.2 MiB)
[DEBUG] 2021-07-19 17:12:25,444 org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 5 ms
[DEBUG] 2021-07-19 17:12:25,445 org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 6 ms
[DEBUG] 2021-07-19 17:12:25,487 org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(2)
[DEBUG] 2021-07-19 17:12:25,487 org.apache.spark.ContextCleaner - Cleaning broadcast 2
[DEBUG] 2021-07-19 17:12:25,487 org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
[DEBUG] 2021-07-19 17:12:25,490 org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 2
[DEBUG] 2021-07-19 17:12:25,490 org.apache.spark.storage.BlockManager - Removing broadcast 2
[DEBUG] 2021-07-19 17:12:25,490 org.apache.spark.storage.BlockManager - Removing block broadcast_2
[INFO] 2021-07-19 17:12:25,491 org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 63.1 KiB, free 995.2 MiB)
[DEBUG] 2021-07-19 17:12:25,491 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 178344 dropped from memory (free 1043695681)
[DEBUG] 2021-07-19 17:12:25,491 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.1.29, 35121, None)
[DEBUG] 2021-07-19 17:12:25,492 org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
[INFO] 2021-07-19 17:12:25,492 org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.1.29:35121 (size: 63.1 KiB, free: 995.9 MiB)
[DEBUG] 2021-07-19 17:12:25,492 org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 28142 dropped from memory (free 1043723823)
[DEBUG] 2021-07-19 17:12:25,493 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
[DEBUG] 2021-07-19 17:12:25,494 org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
[DEBUG] 2021-07-19 17:12:25,494 org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 4 ms
[DEBUG] 2021-07-19 17:12:25,494 org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 4 ms
[INFO] 2021-07-19 17:12:25,494 org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1388
[DEBUG] 2021-07-19 17:12:25,494 org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.1.29, 35121, None)
[INFO] 2021-07-19 17:12:25,495 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.1.29:35121 in memory (size: 27.5 KiB, free: 995.9 MiB)
[INFO] 2021-07-19 17:12:25,497 org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[15] at save at Simulator.scala:30) (first 15 tasks are for partitions Vector(0))
[INFO] 2021-07-19 17:12:25,498 org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
[DEBUG] 2021-07-19 17:12:25,499 org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
[DEBUG] 2021-07-19 17:12:25,499 org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
[DEBUG] 2021-07-19 17:12:25,499 org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
[DEBUG] 2021-07-19 17:12:25,500 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
[DEBUG] 2021-07-19 17:12:25,507 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
[DEBUG] 2021-07-19 17:12:25,507 org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
[INFO] 2021-07-19 17:12:25,517 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.29, executor driver, partition 0, PROCESS_LOCAL, 5114 bytes) taskResourceAssignments Map()
[DEBUG] 2021-07-19 17:12:25,519 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
[DEBUG] 2021-07-19 17:12:25,519 org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 2, response is 0
[INFO] 2021-07-19 17:12:25,520 org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[DEBUG] 2021-07-19 17:12:25,523 org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
[DEBUG] 2021-07-19 17:12:25,534 org.apache.spark.storage.BlockManager - Getting local block broadcast_4
[DEBUG] 2021-07-19 17:12:25,534 org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 17:12:25,557 org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.1.29:32937
[DEBUG] 2021-07-19 17:12:25,565 org.apache.spark.ContextCleaner - Cleaned broadcast 2
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(43)
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Cleaning accumulator 43
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Cleaned accumulator 43
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(42)
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Cleaning accumulator 42
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Cleaned accumulator 42
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(44)
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Cleaning accumulator 44
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Cleaned accumulator 44
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(41)
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Cleaning accumulator 41
[DEBUG] 2021-07-19 17:12:25,566 org.apache.spark.ContextCleaner - Cleaned accumulator 41
[DEBUG] 2021-07-19 17:12:25,612 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(9)
[DEBUG] 2021-07-19 17:12:25,612 org.apache.spark.ContextCleaner - Cleaning accumulator 9
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Cleaned accumulator 9
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(8)
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Cleaning accumulator 8
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Cleaned accumulator 8
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(11)
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Cleaning accumulator 11
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Cleaned accumulator 11
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(13)
[DEBUG] 2021-07-19 17:12:25,613 org.apache.spark.ContextCleaner - Cleaning accumulator 13
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Cleaned accumulator 13
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(6)
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Cleaning accumulator 6
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Cleaned accumulator 6
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(5)
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Cleaning accumulator 5
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Cleaned accumulator 5
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(15)
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Cleaning accumulator 15
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Cleaned accumulator 15
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(2)
[DEBUG] 2021-07-19 17:12:25,614 org.apache.spark.ContextCleaner - Cleaning accumulator 2
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaned accumulator 2
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(1)
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaning accumulator 1
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaned accumulator 1
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(3)
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaning accumulator 3
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaned accumulator 3
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(12)
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaning accumulator 12
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaned accumulator 12
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(10)
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaning accumulator 10
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaned accumulator 10
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(14)
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaning accumulator 14
[DEBUG] 2021-07-19 17:12:25,615 org.apache.spark.ContextCleaner - Cleaned accumulator 14
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(4)
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Cleaning accumulator 4
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Cleaned accumulator 4
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(0)
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Cleaning accumulator 0
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Cleaned accumulator 0
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(7)
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Cleaning accumulator 7
[DEBUG] 2021-07-19 17:12:25,616 org.apache.spark.ContextCleaner - Cleaned accumulator 7
[INFO] 2021-07-19 17:12:25,716 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[INFO] 2021-07-19 17:12:25,776 org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///home/dev/IdeaProjects/StockSimulator/data/dis.us.txt, range: 0-600954, partition values: [empty row]
[DEBUG] 2021-07-19 17:12:25,793 org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */   private void writeFields_0_0(InternalRow i) {
/* 051 */
/* 052 */     boolean isNull_0 = i.isNullAt(0);
/* 053 */     UTF8String value_0 = isNull_0 ?
/* 054 */     null : (i.getUTF8String(0));
/* 055 */     if (isNull_0) {
/* 056 */       mutableStateArray_0[0].setNullAt(0);
/* 057 */     } else {
/* 058 */       mutableStateArray_0[0].write(0, value_0);
/* 059 */     }
/* 060 */
/* 061 */     boolean isNull_1 = i.isNullAt(1);
/* 062 */     UTF8String value_1 = isNull_1 ?
/* 063 */     null : (i.getUTF8String(1));
/* 064 */     if (isNull_1) {
/* 065 */       mutableStateArray_0[0].setNullAt(1);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(1, value_1);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_2 = i.isNullAt(2);
/* 071 */     UTF8String value_2 = isNull_2 ?
/* 072 */     null : (i.getUTF8String(2));
/* 073 */     if (isNull_2) {
/* 074 */       mutableStateArray_0[0].setNullAt(2);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(2, value_2);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_3 = i.isNullAt(3);
/* 080 */     UTF8String value_3 = isNull_3 ?
/* 081 */     null : (i.getUTF8String(3));
/* 082 */     if (isNull_3) {
/* 083 */       mutableStateArray_0[0].setNullAt(3);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(3, value_3);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_4 = i.isNullAt(4);
/* 089 */     UTF8String value_4 = isNull_4 ?
/* 090 */     null : (i.getUTF8String(4));
/* 091 */     if (isNull_4) {
/* 092 */       mutableStateArray_0[0].setNullAt(4);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(4, value_4);
/* 095 */     }
/* 096 */
/* 097 */   }
/* 098 */
/* 099 */ }

[DEBUG] 2021-07-19 17:12:25,799 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */   private void writeFields_0_0(InternalRow i) {
/* 051 */
/* 052 */     boolean isNull_0 = i.isNullAt(0);
/* 053 */     UTF8String value_0 = isNull_0 ?
/* 054 */     null : (i.getUTF8String(0));
/* 055 */     if (isNull_0) {
/* 056 */       mutableStateArray_0[0].setNullAt(0);
/* 057 */     } else {
/* 058 */       mutableStateArray_0[0].write(0, value_0);
/* 059 */     }
/* 060 */
/* 061 */     boolean isNull_1 = i.isNullAt(1);
/* 062 */     UTF8String value_1 = isNull_1 ?
/* 063 */     null : (i.getUTF8String(1));
/* 064 */     if (isNull_1) {
/* 065 */       mutableStateArray_0[0].setNullAt(1);
/* 066 */     } else {
/* 067 */       mutableStateArray_0[0].write(1, value_1);
/* 068 */     }
/* 069 */
/* 070 */     boolean isNull_2 = i.isNullAt(2);
/* 071 */     UTF8String value_2 = isNull_2 ?
/* 072 */     null : (i.getUTF8String(2));
/* 073 */     if (isNull_2) {
/* 074 */       mutableStateArray_0[0].setNullAt(2);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(2, value_2);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_3 = i.isNullAt(3);
/* 080 */     UTF8String value_3 = isNull_3 ?
/* 081 */     null : (i.getUTF8String(3));
/* 082 */     if (isNull_3) {
/* 083 */       mutableStateArray_0[0].setNullAt(3);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(3, value_3);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_4 = i.isNullAt(4);
/* 089 */     UTF8String value_4 = isNull_4 ?
/* 090 */     null : (i.getUTF8String(4));
/* 091 */     if (isNull_4) {
/* 092 */       mutableStateArray_0[0].setNullAt(4);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(4, value_4);
/* 095 */     }
/* 096 */
/* 097 */   }
/* 098 */
/* 099 */ }

[INFO] 2021-07-19 17:12:25,829 org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.819523 ms
[DEBUG] 2021-07-19 17:12:25,831 org.apache.spark.storage.BlockManager - Getting local block broadcast_3
[DEBUG] 2021-07-19 17:12:25,831 org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
[DEBUG] 2021-07-19 17:12:26,281 org.apache.spark.scheduler.OutputCommitCoordinator - Commit allowed for stage=1.0, partition=0, task attempt 0
[INFO] 2021-07-19 17:12:26,285 org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202107191712255973707319973359761_0001_m_000000_1: Committed
[INFO] 2021-07-19 17:12:26,293 org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2482 bytes result sent to driver
[DEBUG] 2021-07-19 17:12:26,294 org.apache.spark.executor.ExecutorMetricsPoller - removing (1, 0) from stageTCMP
[INFO] 2021-07-19 17:12:26,299 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 799 ms on 192.168.1.29 (executor driver) (1/1)
[INFO] 2021-07-19 17:12:26,299 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO] 2021-07-19 17:12:26,300 org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (save at Simulator.scala:30) finished in 0.914 s
[DEBUG] 2021-07-19 17:12:26,300 org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
[INFO] 2021-07-19 17:12:26,301 org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[INFO] 2021-07-19 17:12:26,301 org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 1: Stage finished
[DEBUG] 2021-07-19 17:12:26,301 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@7e006dd0)
[INFO] 2021-07-19 17:12:26,302 org.apache.spark.scheduler.DAGScheduler - Job 1 finished: save at Simulator.scala:30, took 0.926375 s
[DEBUG] 2021-07-19 17:12:26,315 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Committing files staged for absolute locations Map()
[DEBUG] 2021-07-19 17:12:26,316 org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Create absolute parent directories: Set()
[INFO] 2021-07-19 17:12:26,317 org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job f7b2da49-a616-4930-8d7c-3c823eb6d08f committed.
[INFO] 2021-07-19 17:12:26,322 org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job f7b2da49-a616-4930-8d7c-3c823eb6d08f.
[INFO] 2021-07-19 17:12:26,334 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO] 2021-07-19 17:12:26,342 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.1.29:4040
[INFO] 2021-07-19 17:12:26,353 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO] 2021-07-19 17:12:26,365 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
[INFO] 2021-07-19 17:12:26,366 org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO] 2021-07-19 17:12:26,374 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO] 2021-07-19 17:12:26,377 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO] 2021-07-19 17:12:26,390 org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] 2021-07-19 17:12:26,391 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO] 2021-07-19 17:12:26,391 org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-5d3b6936-5c97-444f-bb4a-8762d583a9be
[WARN] 2021-07-19 17:13:21,832 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:13:21,837 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:15:46,459 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:15:46,470 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:32:41,040 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:32:41,042 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:34:09,018 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:34:09,020 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:34:44,259 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:34:44,267 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:35:07,534 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:35:07,536 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:35:32,592 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:35:32,609 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:35:54,930 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:35:54,931 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:36:20,657 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:36:20,663 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:36:46,290 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:36:46,291 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:37:59,231 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:37:59,235 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:38:37,814 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:38:37,817 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 17:43:38,172 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 17:43:38,179 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 18:03:07,208 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:03:07,210 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 18:04:12,850 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:04:12,852 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-19 18:04:22,544 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.IllegalArgumentException: Too many pattern letters: a
	at java.base/java.time.format.DateTimeFormatterBuilder.parseField(DateTimeFormatterBuilder.java:1918)
	at java.base/java.time.format.DateTimeFormatterBuilder.parsePattern(DateTimeFormatterBuilder.java:1742)
	at java.base/java.time.format.DateTimeFormatterBuilder.appendPattern(DateTimeFormatterBuilder.java:1710)
	at java.base/java.time.format.DateTimeFormatter.ofPattern(DateTimeFormatter.java:565)
	at com.naya.simulator.Simulator.$anonfun$start$1(Simulator.scala:30)
	at com.naya.simulator.Simulator.$anonfun$start$1$adapted(Simulator.scala:27)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-19 18:04:22,615 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.IllegalArgumentException: Too many pattern letters: a
	at java.base/java.time.format.DateTimeFormatterBuilder.parseField(DateTimeFormatterBuilder.java:1918)
	at java.base/java.time.format.DateTimeFormatterBuilder.parsePattern(DateTimeFormatterBuilder.java:1742)
	at java.base/java.time.format.DateTimeFormatterBuilder.appendPattern(DateTimeFormatterBuilder.java:1710)
	at java.base/java.time.format.DateTimeFormatter.ofPattern(DateTimeFormatter.java:565)
	at com.naya.simulator.Simulator.$anonfun$start$1(Simulator.scala:30)
	at com.naya.simulator.Simulator.$anonfun$start$1$adapted(Simulator.scala:27)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-19 18:04:22,617 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-19 18:04:46,878 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:04:46,882 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-19 18:04:57,360 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.IllegalArgumentException: Too many pattern letters: a
	at java.base/java.time.format.DateTimeFormatterBuilder.parseField(DateTimeFormatterBuilder.java:1918)
	at java.base/java.time.format.DateTimeFormatterBuilder.parsePattern(DateTimeFormatterBuilder.java:1742)
	at java.base/java.time.format.DateTimeFormatterBuilder.appendPattern(DateTimeFormatterBuilder.java:1710)
	at java.base/java.time.format.DateTimeFormatter.ofPattern(DateTimeFormatter.java:565)
	at com.naya.simulator.Simulator.$anonfun$start$1(Simulator.scala:30)
	at com.naya.simulator.Simulator.$anonfun$start$1$adapted(Simulator.scala:27)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-19 18:04:57,458 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.IllegalArgumentException: Too many pattern letters: a
	at java.base/java.time.format.DateTimeFormatterBuilder.parseField(DateTimeFormatterBuilder.java:1918)
	at java.base/java.time.format.DateTimeFormatterBuilder.parsePattern(DateTimeFormatterBuilder.java:1742)
	at java.base/java.time.format.DateTimeFormatterBuilder.appendPattern(DateTimeFormatterBuilder.java:1710)
	at java.base/java.time.format.DateTimeFormatter.ofPattern(DateTimeFormatter.java:565)
	at com.naya.simulator.Simulator.$anonfun$start$1(Simulator.scala:30)
	at com.naya.simulator.Simulator.$anonfun$start$1$adapted(Simulator.scala:27)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-19 18:04:57,465 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-19 18:07:47,768 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:07:47,772 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-19 18:08:05,702 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.IllegalArgumentException: Too many pattern letters: a
	at java.base/java.time.format.DateTimeFormatterBuilder.parseField(DateTimeFormatterBuilder.java:1918)
	at java.base/java.time.format.DateTimeFormatterBuilder.parsePattern(DateTimeFormatterBuilder.java:1742)
	at java.base/java.time.format.DateTimeFormatterBuilder.appendPattern(DateTimeFormatterBuilder.java:1710)
	at java.base/java.time.format.DateTimeFormatter.ofPattern(DateTimeFormatter.java:565)
	at com.naya.simulator.Simulator.$anonfun$start$1(Simulator.scala:30)
	at com.naya.simulator.Simulator.$anonfun$start$1$adapted(Simulator.scala:27)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-19 18:08:05,766 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.IllegalArgumentException: Too many pattern letters: a
	at java.base/java.time.format.DateTimeFormatterBuilder.parseField(DateTimeFormatterBuilder.java:1918)
	at java.base/java.time.format.DateTimeFormatterBuilder.parsePattern(DateTimeFormatterBuilder.java:1742)
	at java.base/java.time.format.DateTimeFormatterBuilder.appendPattern(DateTimeFormatterBuilder.java:1710)
	at java.base/java.time.format.DateTimeFormatter.ofPattern(DateTimeFormatter.java:565)
	at com.naya.simulator.Simulator.$anonfun$start$1(Simulator.scala:30)
	at com.naya.simulator.Simulator.$anonfun$start$1$adapted(Simulator.scala:27)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-19 18:08:05,772 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-19 18:08:15,085 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:08:15,087 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-19 18:08:27,050 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.time.temporal.UnsupportedTemporalTypeException: Unsupported field: HourOfDay
	at java.base/java.time.LocalDate.get0(LocalDate.java:709)
	at java.base/java.time.LocalDate.getLong(LocalDate.java:688)
	at java.base/java.time.format.DateTimePrintContext.getValue(DateTimePrintContext.java:308)
	at java.base/java.time.format.DateTimeFormatterBuilder$NumberPrinterParser.format(DateTimeFormatterBuilder.java:2704)
	at java.base/java.time.format.DateTimeFormatterBuilder$CompositePrinterParser.format(DateTimeFormatterBuilder.java:2343)
	at java.base/java.time.format.DateTimeFormatter.formatTo(DateTimeFormatter.java:1848)
	at java.base/java.time.format.DateTimeFormatter.format(DateTimeFormatter.java:1822)
	at com.naya.simulator.Simulator.$anonfun$start$1(Simulator.scala:30)
	at com.naya.simulator.Simulator.$anonfun$start$1$adapted(Simulator.scala:27)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-19 18:08:27,140 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.time.temporal.UnsupportedTemporalTypeException: Unsupported field: HourOfDay
	at java.base/java.time.LocalDate.get0(LocalDate.java:709)
	at java.base/java.time.LocalDate.getLong(LocalDate.java:688)
	at java.base/java.time.format.DateTimePrintContext.getValue(DateTimePrintContext.java:308)
	at java.base/java.time.format.DateTimeFormatterBuilder$NumberPrinterParser.format(DateTimeFormatterBuilder.java:2704)
	at java.base/java.time.format.DateTimeFormatterBuilder$CompositePrinterParser.format(DateTimeFormatterBuilder.java:2343)
	at java.base/java.time.format.DateTimeFormatter.formatTo(DateTimeFormatter.java:1848)
	at java.base/java.time.format.DateTimeFormatter.format(DateTimeFormatter.java:1822)
	at com.naya.simulator.Simulator.$anonfun$start$1(Simulator.scala:30)
	at com.naya.simulator.Simulator.$anonfun$start$1$adapted(Simulator.scala:27)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-19 18:08:27,142 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-19 18:11:09,644 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:11:09,645 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 18:15:55,175 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:15:55,179 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 18:16:27,830 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:16:27,832 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 18:50:17,905 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:50:17,907 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 18:54:23,119 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 18:54:23,122 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 19:11:54,270 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 19:11:54,272 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 19:12:32,058 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 19:12:32,061 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 19:34:06,048 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 19:34:06,051 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-19 19:36:53,979 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-19 19:36:53,980 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 09:29:09,005 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 09:29:09,018 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 10:05:10,446 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 10:05:10,455 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 10:08:30,729 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 10:08:30,735 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 10:30:16,098 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 10:30:16,100 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 10:30:48,157 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 10:30:48,159 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 10:33:00,317 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 10:33:00,322 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 10:43:47,570 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 10:43:47,573 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 10:44:30,093 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 10:44:30,095 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 10:46:56,449 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 10:46:56,450 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 21:12:18,372 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 21:12:18,379 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 21:18:10,646 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 21:18:10,648 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 21:35:43,374 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 21:35:43,376 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 21:36:42,371 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 21:36:42,373 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 21:38:30,720 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 21:38:30,724 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 21:43:13,598 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 21:43:13,604 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 22:00:26,163 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 22:00:26,165 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 22:01:28,310 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 22:01:28,312 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 22:01:59,268 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 22:01:59,270 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 22:14:48,692 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 22:14:48,693 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 22:39:28,304 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 22:39:28,308 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 22:41:10,172 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 22:41:10,174 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 22:42:57,167 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 22:42:57,169 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 22:49:29,638 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 22:49:29,640 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:17:00,390 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:17:00,392 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:17:11,646 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:22)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:17:11,715 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:22)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:17:11,717 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:17:49,942 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:17:49,944 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:18:01,037 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:22)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:18:01,119 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:22)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:18:01,121 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:18:32,578 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:18:32,580 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:18:43,597 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:22)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:18:43,663 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:22)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:18:43,665 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:20:00,497 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:20:00,499 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:20:10,962 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:24)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:20:11,038 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:24)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:20:11,041 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:21:22,301 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:21:22,302 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:21:33,240 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:21:33,323 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:21:33,327 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:21:58,706 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:21:58,707 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:22:09,533 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:22:09,595 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:22:09,597 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:25:58,142 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:25:58,144 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:26:09,100 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:27)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:26:09,193 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:27)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:26:09,200 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:28:04,151 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:28:04,153 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:28:18,641 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:27)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:28:18,707 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:27)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:28:18,709 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:29:56,143 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:29:56,144 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:30:07,749 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:29)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:30:07,835 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:29)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:30:07,838 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:31:34,007 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:31:34,008 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:31:37,973 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[ERROR] 2021-07-20 23:31:44,711 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:29)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:31:44,799 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:29)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:31:44,801 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:32:33,969 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:32:33,972 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:32:44,861 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:32:44,957 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:32:44,961 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:34:15,388 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:34:15,390 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:34:25,861 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:34:25,913 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:34:25,917 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:35:42,744 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:35:42,746 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:35:53,089 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:35:53,145 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:35:53,147 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:36:48,774 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:36:48,781 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[ERROR] 2021-07-20 23:37:03,731 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:37:03,774 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:37:03,776 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:45:58,580 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:45:58,590 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:46:03,493 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[ERROR] 2021-07-20 23:46:12,278 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:46:12,323 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:28)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:46:12,324 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:47:51,055 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:47:51,058 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:49:09,140 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:49:09,147 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:49:54,941 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:49:54,943 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:49:58,784 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[ERROR] 2021-07-20 23:50:05,483 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:27)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:50:05,538 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:27)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:50:05,540 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:51:08,004 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:51:08,006 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:52:50,207 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:52:50,209 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:53:40,904 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:53:40,905 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:53:45,338 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[ERROR] 2021-07-20 23:53:51,800 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:27)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:53:51,844 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:27)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:53:51,846 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:55:35,565 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:55:35,566 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:55:39,665 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[ERROR] 2021-07-20 23:55:46,275 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:55:46,311 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:55:46,313 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-20 23:57:40,859 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-20 23:57:40,860 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-20 23:57:44,657 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[ERROR] 2021-07-20 23:57:50,760 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-20 23:57:50,835 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-20 23:57:50,838 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-21 00:01:01,454 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 00:01:01,457 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 00:01:06,387 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[ERROR] 2021-07-21 00:01:14,620 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 1.0 (TID 2)
java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
[WARN] 2021-07-21 00:01:14,695 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 1.0 (TID 2) (192.168.1.29 executor driver): java.lang.NullPointerException
	at java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at java.base/java.util.Properties.put(Properties.java:1334)
	at com.naya.simulator.demo.ProducerDemo.mainT(ProducerDemo.java:26)
	at com.naya.simulator.service.Simulator.$anonfun$start$1(Simulator.scala:32)
	at com.naya.simulator.service.Simulator.$anonfun$start$1$adapted(Simulator.scala:29)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:1012)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:1012)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)

[ERROR] 2021-07-21 00:01:14,700 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 1.0 failed 1 times; aborting job
[WARN] 2021-07-21 09:15:58,011 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:15:58,014 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 09:19:21,598 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:19:21,600 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 09:22:20,431 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:22:20,432 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 09:27:12,195 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:27:12,197 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 09:32:49,916 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:32:49,918 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 09:40:42,511 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:40:42,513 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 09:45:00,765 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:45:00,766 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 09:53:23,234 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:53:23,235 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 09:53:54,809 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 09:53:54,812 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 10:20:08,355 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 10:20:08,356 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 11:17:50,505 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 11:17:50,507 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 11:17:54,672 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 11:42:56,202 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 11:42:56,204 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 11:43:00,620 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 11:52:20,442 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 11:52:20,444 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 11:52:25,352 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 11:52:25,352 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:03:50,746 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 12:03:50,752 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 12:03:55,056 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:03:55,057 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:03:55,057 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:03:55,058 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:06:00,473 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 12:06:00,475 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 12:06:04,971 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:06:04,971 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:06:04,973 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:06:04,974 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:09:02,313 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 12:09:02,314 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 12:09:06,703 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:09:06,703 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:09:06,704 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:09:06,704 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:11:27,018 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 12:11:27,019 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 12:11:31,424 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:11:31,424 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:11:31,433 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:11:31,433 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:34,603 org.apache.spark.util.Utils - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface wlp2s0)
[WARN] 2021-07-21 12:14:34,604 org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
[WARN] 2021-07-21 12:14:39,535 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:39,544 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:39,544 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:39,545 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:39,545 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:39,546 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:39,547 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:39,547 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
[WARN] 2021-07-21 12:14:39,548 org.apache.spark.sql.SparkSession$Builder - Using an existing SparkSession; some spark core configurations may not take effect.
